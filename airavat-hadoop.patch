Index: src/core/org/apache/hadoop/security/LaminarCalls.java
===================================================================
--- src/core/org/apache/hadoop/security/LaminarCalls.java	(revision 0)
+++ src/core/org/apache/hadoop/security/LaminarCalls.java	(revision 290)
@@ -0,0 +1,60 @@
+/* Class that holds functions that make jni calls.
+ * Feb, 2009(indrajit04@gmail.com)
+ * Specially all laminar system calls related to labeling.
+ * To run set 'java -Djava.library.path='
+ */
+package org.apache.hadoop.security;
+public class LaminarCalls{
+	 /*Used to specify the type of capability*/
+	  public static final int PLUS_CAPABILITY=0, MINUS_CAPABILITY=1, BOTH_CAPABILITY=2;
+	  /*Used to specify the set where the capability should be added*/
+	  public static final int REGION_SELF=1, REGION_GROUP=2, REGION_NONE=0;
+	  /*Label type used in object-label map*/
+	  public static final int SECRECY=0, INTEGRITY=1;
+	  /*Drop capabilities permanent or temporarily*/
+	  public static final int DROP_PERMANENT=0, DROP_TEMPORARY=1;
+
+    /*DIFC: pass the labels to the OS*/
+    public static native int sysPassLabels(long secrecySet[], long integritySet[]);
+
+    /*DIFC: ask the OS to replace the labels without making checks*/
+    public static native int sysReplaceLabelsTCB(long secrecySet[], long integritySet[]);
+
+    /*DIFC: Drop capability of the thread
+     *flag=0->permanent drop;flag=1->temporary drop
+     *type=0->plus capability;type=1->minus capability 
+     */
+    public static native int sysDropCapability(long labels[], int type, int flag);
+
+    /*DIFC: Create a new capability and add it to the thread
+     *or the program's capability list
+     *region=0: none; region=1: add only to the thread ; region=2: add to the program
+     *In JAVA: PLUS_CAPABILITY=0, MINUS_CAPABILITY=1, BOTH_CAPABILITY=2
+     */
+    public static native int sysCreateAndAddLabel(int type, int region);
+
+    /*DIFC: create a labeled directory*/
+    public static native int sysCreateLabeledDirectory(String pname, int mode, long secrecySet[], long integritySet[]);
+
+    /*DIFC: create a labeled file*/
+    public static native int sysCreateLabeledFile(String pname, int mode, long secrecySet[], long integritySet[]);
+    
+    /*DIFC: allow arbitrary command*/
+    public static native int sysExecuteCommand(String cmd);
+
+    static {
+	System.loadLibrary("LaminarCalls");
+    }
+
+    public static void main(String a[]){
+	LaminarCalls.sysCreateAndAddLabel(1,2);
+	long[] s={2,5,18,1};
+	long[] in={3,5};
+	LaminarCalls.sysPassLabels(s,in);
+	LaminarCalls.sysReplaceLabelsTCB(s,in);
+	LaminarCalls.sysDropCapability(s,1,2);
+	LaminarCalls.sysCreateLabeledDirectory("mydir",0,s,in);
+	LaminarCalls.sysCreateLabeledFile("myfile",0,s,in);
+    }
+}
+ 
\ No newline at end of file
Index: src/core/org/apache/hadoop/security/DIFCException.java
===================================================================
--- src/core/org/apache/hadoop/security/DIFCException.java	(revision 0)
+++ src/core/org/apache/hadoop/security/DIFCException.java	(revision 290)
@@ -0,0 +1,14 @@
+package org.apache.hadoop.security;
+
+/* DIFC
+ * This exception is thrown when ever the runtime detects
+ * a difc security violation 
+ */
+public class DIFCException extends SecurityException{
+
+  public DIFCException(String msg) {
+    super(msg);
+  }
+  /*Lets put the golden ratio. Is there a convention? */
+  private static final long serialVersionUID = 16180339887L;
+}
Index: src/core/org/apache/hadoop/security/DIFC.java
===================================================================
--- src/core/org/apache/hadoop/security/DIFC.java	(revision 0)
+++ src/core/org/apache/hadoop/security/DIFC.java	(revision 290)
@@ -0,0 +1,43 @@
+/*
+ * Class to hold major functions related to DIFC checks, file creation etc.
+ *  Feb., 2009 (indrajit04@gmail.com)
+ */
+package org.apache.hadoop.security;
+
+public class DIFC {
+
+	/*Set this bit to ON only when running on Laminar OS. It is used
+	 * during labeled file creation etc. If its ON when not running on Laminar OS
+	 * file creation etc. may fail.
+	 */
+	static final boolean USE_LAMINAR=false;
+	static final int SECRECY=0,INTEGRITY=1; 
+	/*Check whether a file can be created within the specified directory
+	 * Secrecy increases as we go deeper in dir. path, and integrity dec.
+	 * */
+	public static boolean checkFileCreation(LabelSet dirSecrecy, LabelSet dirIntegrity, LabelSet fileSecrecy, LabelSet fileIntegrity){
+		if(!dirSecrecy.isSubsetOf(fileSecrecy))
+			return false;
+		if(!fileIntegrity.isSubsetOf(dirIntegrity))
+			return false;
+		return true;
+	}
+	
+	/*Check whether a file can be read by a principal*/
+	public static boolean checkFileReadPermission(LabelSet principalSec, LabelSet principalInt, LabelSet fileSecrecy, LabelSet fileIntegrity){
+		if(!fileSecrecy.isSubsetOf(principalSec))
+			return false;
+		if(!principalInt.isSubsetOf(fileIntegrity))
+			return false;
+		return true;
+	}
+	
+	/*TODO:Check whether a file can be renamed by a principal.
+	 * One may need to check that the renamed file follows the label
+	 * hierarchy as per its position in the path
+	 */
+	public static boolean checkRename(String src, String dest, LabelSet userSecrecy, LabelSet userIntegrity){
+		return true;
+	}
+	
+}
Index: src/core/org/apache/hadoop/security/LabelSet.java
===================================================================
--- src/core/org/apache/hadoop/security/LabelSet.java	(revision 0)
+++ src/core/org/apache/hadoop/security/LabelSet.java	(revision 290)
@@ -0,0 +1,354 @@
+/*
+ * Labels in our system. Same as those used in Laminar.
+ * Feb, 2009 (indrajit04@gmail.com)
+ */
+package org.apache.hadoop.security;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.util.StringTokenizer;
+
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.WritableFactories;
+import org.apache.hadoop.io.WritableFactory;
+
+public class LabelSet implements Writable{
+  
+	static {                                      // register a ctor
+	    WritableFactories.setFactory
+	      (LabelSet.class,
+	       new WritableFactory() {
+	         public Writable newInstance() { return new LabelSet(); }
+	       });
+	  }
+	
+  public static LabelSet EMPTY = new LabelSet(new long[0], 0);
+  //public static LabelSet EMPTY = new LabelSet(1);
+  private long[] labels; // guaranteed sorted
+  // package-level access since we don't want to expose to programmers
+  int len; // use this instead of labels.length!!  (since we may want to allocate arrays bigger than the space used)
+  
+  public LabelSet() {
+	    this.labels = new long[0];
+	    this.len = 0;
+ }
+  
+ public LabelSet(String v) {
+	 	long val=Long.parseLong(v);
+	 	if(val<=0){
+	 		this.len=0;
+	 		this.labels=new long[0];
+	 	 }else{
+	 		this.labels = new long[1];
+	 		this.len = 1;
+	 		labels[0]=val;
+	 	}
+}
+  
+  public LabelSet(long[] labels, int len) {
+    this.labels = labels;
+    this.len = len;
+  }
+
+  /////////////////////////////////////
+  // Writable
+  /////////////////////////////////////
+  public void write(DataOutput out) throws IOException {
+	out.writeInt(len);
+    for(int i=0;i<len;i++)
+    	out.writeLong(labels[i]);
+  }
+
+  public void readFields(DataInput in) throws IOException {
+	this.len = in.readInt();
+    this.labels=new long[this.len];
+    for(int i=0;i<this.len;i++)
+    	this.labels[i]=in.readLong();
+  }
+  
+  /* Writing/reading the labelset in a stream*/
+  public static void writeToStream(DataOutputStream out, LabelSet label) throws IOException {
+	  out.writeInt(label.len);
+	  for(int i=0;i<label.len;i++)
+		  out.writeLong(label.labels[i]);
+  }
+
+  public static LabelSet readFields(DataInputStream in) throws IOException {
+	  int length = in.readInt();
+	  long array[]=new long[length];
+	  for(int i=0;i<length;i++)
+		  array[i]=in.readLong();
+	  return new LabelSet(array,length);
+  }
+  // standard ways to create a new label set (the multi-label sets use
+  // union for simplicity -- consider the cases where the program
+  // passes in unsorted parameters or duplicate parameters) 
+
+  public static LabelSet getLabelSet(long label) {
+    return new LabelSet(new long[] { label }, 1);
+  }
+
+  public static LabelSet getLabelSet(long label1, long label2) {
+    return union(getLabelSet(label1), label2);
+  }
+  
+  public static LabelSet getLabelSet(long label1, long label2, long label3) {
+    return union(getLabelSet(label1, label2), label3);
+  }
+  
+  // allow only package-level access since we don't want to expose to programmers
+  // use (final) len field instead
+  int size(){ return len; }
+  
+  // EMPTY is final, so should access it directly
+  @Deprecated
+  public static LabelSet getEmptyLabel(){return EMPTY;}
+  
+  // restrict accesses to package since programmers shouldn't be able to get this
+  long[] getLongLabels(){return labels;}
+  
+  // DIFC: O(n) test for subset
+  // Checks if THIS is a subset of OTHER
+  public boolean isSubsetOf(LabelSet other) {
+    // short-circuit check
+    if (this == EMPTY || this.len==0) {
+      return true;
+    } else if (this == other) {
+      return true;
+    } else if (other == EMPTY || other.len==0) {
+      return false; // since we know this != EMPTY
+    }
+    return isSubsetOfSlowPath(other);
+  }
+  
+  boolean isSubsetOfSlowPath(LabelSet other) { 
+    long[] labels1 = this.labels;
+    long[] labels2 = other.labels;
+    int pos1, pos2;
+    // the check here just checks if we've reached the end of this
+    for (pos1 = 0, pos2 = 0; pos1 < this.len; ) {
+      if (pos2 == other.len) {
+        return false;
+      } else if (labels2[pos2] < labels1[pos1]) {
+        pos2++;
+      } else  if (labels2[pos2] > labels1[pos1]) {
+        return false;
+      } else {
+        // don't include the elements that are in both
+        pos1++;
+        pos2++;
+      }
+    }
+    return true;
+  }
+
+  /*Since l1 and l2 can have common elements, we will need to remove duplicates
+   * This function is slow if length of labels is large, hence should not be used often.
+   */
+  
+  public static LabelSet union(LabelSet l1, LabelSet l2) {
+    // this probably happens a lot; also it catches the case where
+    // someone unions EMPTY and EMPTY, in which case we want the
+    // result to be EMPTY, not a duplicate empty label set 
+    if (l1 == l2) {
+      return l1;
+    } else if (l2==EMPTY) {
+      return l1;
+    } else if (l1==EMPTY) {
+      return l2;
+    }
+    return unionSlowPath(l1, l2);
+  }
+  
+  static LabelSet unionSlowPath(LabelSet l1, LabelSet l2) {
+    long[] labels1 = l1.labels;
+    long[] labels2 = l2.labels;
+    long[] newLabels = new long[l1.len + l2.len];
+    int newPos, pos1, pos2;
+    for (newPos = 0, pos1 = 0, pos2 = 0; pos1 < l1.len || pos2 < l2.len; newPos++) {
+      // pick the lesser (or take both if equal)
+      // if one array is exhausted, pick the other
+      if (pos1 == l1.len) {
+        newLabels[newPos] = labels2[pos2];
+        pos2++;
+      } else if (pos2 == l2.len) {
+        newLabels[newPos] = labels1[pos1];
+        pos1++;
+      } else if (labels1[pos1] < labels2[pos2]) {
+        newLabels[newPos] = labels1[pos1];
+        pos1++;
+      } else if (labels1[pos1] > labels2[pos2]) {
+        newLabels[newPos] = labels2[pos2];
+        pos2++;
+      } else {
+        newLabels[newPos] = labels1[pos1];
+        pos1++;
+        pos2++;
+      }
+    }
+    
+    // skip if an existing label will do
+    if (newPos == l1.len) {
+      return l1;
+    } else if (newPos == l2.len) {
+      return l2;
+    }
+    
+    return new LabelSet(newLabels, newPos);
+  }
+
+  public static LabelSet union(LabelSet l1, long l2){
+    long[] labels1 = l1.labels;
+    long[] newLabels = new long[l1.len + 1];
+    int newPos, pos1, pos2;
+    for (newPos = 0, pos1 = 0, pos2 = 0; pos1 < l1.len || pos2 < 1; newPos++) {
+      // pick the lesser (or take both if equal)
+      // if one array is exhausted, pick the other
+      if (pos1 == l1.len) {
+        newLabels[newPos] = l2;
+        pos2++;
+      } else if (pos2 == 1) {
+        newLabels[newPos] = labels1[pos1];
+        pos1++;
+      } else if (labels1[pos1] < l2) {
+        newLabels[newPos] = labels1[pos1];
+        pos1++;
+      } else if (labels1[pos1] > l2) {
+        newLabels[newPos] = l2;
+        pos2++;
+      } else {
+        newLabels[newPos] = labels1[pos1];
+        pos1++;
+        pos2++;
+      }
+    }
+    // short-circuit checks
+    if (newPos == l1.len) {
+      return l1;
+    }
+    return new LabelSet(newLabels, newPos);
+  }
+  
+  public static LabelSet minus(LabelSet l1, LabelSet l2){
+    // short-circuit checks
+    if (l1 == EMPTY) {
+      return l1;
+    } else if (l2 == EMPTY) {
+      return l1;
+    } else if (l1 == l2) {
+      return EMPTY;
+    }
+    return minusSlowPath(l1, l2);
+  }
+  
+  static LabelSet minusSlowPath(LabelSet l1, LabelSet l2) {
+    long[] labels1 = l1.labels;
+    long[] labels2 = l2.labels;
+    long[] newLabels = new long[l1.len]; // be conservative
+    int newPos, pos1, pos2;
+    // only need to wait until we've exhausted the first array
+    for (newPos = 0, pos1 = 0, pos2 = 0; pos1 < l1.len; ) {
+      // pick the lesser (or take both if equal)
+      // if one array is exhausted, pick the other
+      if (pos2 == l2.len || labels1[pos1] < labels2[pos2]) {
+        newLabels[newPos] = labels1[pos1];
+        newPos++;
+        pos1++;
+      } else if (labels1[pos1] > labels2[pos2]) {
+        // don't include the elements from l2
+        pos2++;
+      } else {
+        // don't include the elements that are in both
+        pos1++;
+        pos2++;
+      }
+    }
+    if (newPos == 0) {
+      return EMPTY;
+    } else {
+      return new LabelSet(newLabels, newPos);
+    }
+  }
+  
+  /** Helper function to check if the elements of the caller are part of the first or second sets */
+  public final boolean checkInUnion(LabelSet firstSet, LabelSet secondSet){
+    // short-circuit checks
+    if (this == EMPTY) {
+      return true;
+    } else if (firstSet == EMPTY) {
+      return this.isSubsetOf(secondSet);
+    } else if (secondSet == EMPTY) {
+      return this.isSubsetOf(firstSet);
+    }
+    return checkInUnionSlowPath(firstSet, secondSet);
+  }
+  
+  final boolean checkInUnionSlowPath(LabelSet firstSet, LabelSet secondSet) {
+    int pos=0,pos1=0,pos2=0, incr=0;
+    boolean change=false;
+    while(pos<this.len){
+      change=false;incr=0;
+      if(pos1<firstSet.len){
+        if(this.labels[pos]==firstSet.labels[pos1]) incr=1;
+        if(firstSet.labels[pos1]<=this.labels[pos] ){
+          pos1++; change=true;
+        }
+      }
+      if(pos2<secondSet.len){
+        if(this.labels[pos]==secondSet.labels[pos2]) incr=1;
+        if(secondSet.labels[pos2]<=this.labels[pos] ){
+          pos2++; change=true;
+        }
+      }
+      //The current value is not present and is smaller than those in the 2 sets
+      if(!change) {
+        return false;
+      }
+      pos+=incr;
+    }
+    return true;
+  }
+
+  
+  public void printLabels(String msg){
+	  System.out.print(msg+":(");
+	  for(int i = 0; i < len; i++)
+		  System.out.print(labels[i]+",");
+	  System.out.println(") : len="+len);
+  }
+  
+  @Override
+  public String toString() {
+    StringBuilder sb = new StringBuilder();
+    for(int i = 0; i < len; i++) {
+      sb.append(labels[i]);
+    }
+    return sb.toString();
+  }
+  
+  /*This is used to flatten the label into a string
+   * Useful in storing labels in JobConf
+   */
+  public static String convertToString(LabelSet input){
+	  String ret=String.valueOf(input.len);
+	  for(int i=0;i<input.len;i++){
+		  ret+="\t"+String.valueOf(input.labels[i]);
+	  }
+	  return ret;
+  }
+  
+  /*Read back the label from the string format.
+   * This can throw exceptions if there is a format error.
+   */
+  public static LabelSet readFromString(String val){
+	  StringTokenizer token=new StringTokenizer(val);
+	  int len=Integer.parseInt(token.nextToken());
+	  long[] labels=new long[len];
+	  for(int i=0;i<len;i++)
+		  labels[i]=Long.parseLong(token.nextToken());
+	  return new LabelSet(labels,len);
+  }
+}
Index: src/core/org/apache/hadoop/conf/Configuration.java
===================================================================
--- src/core/org/apache/hadoop/conf/Configuration.java	(revision 142)
+++ src/core/org/apache/hadoop/conf/Configuration.java	(working copy)
@@ -58,6 +58,7 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableUtils;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.StringUtils;
 import org.w3c.dom.DOMException;
 import org.w3c.dom.Document;
@@ -401,7 +402,18 @@
     set(name, Integer.toString(value));
   }
 
+  /*DIFC: added to flatten a label into a string*/
+  public void setLabel(String name, LabelSet value) {
+	    set(name, LabelSet.convertToString(value));
+  }
 
+  /*DIFC: read back the label from the flattened string*/
+  public LabelSet getLabel(String name, LabelSet defaultValue){
+	  String valueString = get(name);
+	    if (valueString == null)
+	      return defaultValue;
+	  return LabelSet.readFromString(valueString);
+  }
   /** 
    * Get the value of the <code>name</code> property as a <code>long</code>.  
    * If no such property is specified, or if the specified value is not a valid
Index: src/core/org/apache/hadoop/io/MapFile.java
===================================================================
--- src/core/org/apache/hadoop/io/MapFile.java	(revision 142)
+++ src/core/org/apache/hadoop/io/MapFile.java	(working copy)
@@ -24,6 +24,7 @@
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.*;
 import org.apache.hadoop.conf.*;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.Progressable;
 import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hadoop.io.SequenceFile.CompressionType;
@@ -84,6 +85,14 @@
            SequenceFile.getCompressionType(conf));
     }
 
+    public Writer(Configuration conf, FileSystem fs, String dirName,
+    		Class<? extends WritableComparable> keyClass, Class valClass, LabelSet secrecy, LabelSet integrity)
+    throws IOException {
+    	this(conf, fs, dirName,
+    			WritableComparator.get(keyClass), valClass,
+    			SequenceFile.getCompressionType(conf), secrecy,integrity);
+    }
+    
     /** Create the named map for keys of the named class. */
     public Writer(Configuration conf, FileSystem fs, String dirName,
                   Class<? extends WritableComparable> keyClass, Class valClass,
@@ -93,6 +102,14 @@
            compress, progress);
     }
 
+    public Writer(Configuration conf, FileSystem fs, String dirName,
+    		Class<? extends WritableComparable> keyClass, Class valClass,
+    		CompressionType compress, Progressable progress, LabelSet secrecy, LabelSet integrity)
+    throws IOException {
+    	this(conf, fs, dirName, WritableComparator.get(keyClass), valClass,
+    			compress, progress, secrecy,integrity);
+    }
+
     /** Create the named map for keys of the named class. */
     public Writer(Configuration conf, FileSystem fs, String dirName,
                   Class<? extends WritableComparable> keyClass, Class valClass,
@@ -103,6 +120,14 @@
            compress, codec, progress);
     }
 
+    public Writer(Configuration conf, FileSystem fs, String dirName,
+    		Class<? extends WritableComparable> keyClass, Class valClass,
+    		CompressionType compress, CompressionCodec codec,
+    		Progressable progress, LabelSet secrecy, LabelSet integrity)
+    throws IOException {
+    	this(conf, fs, dirName, WritableComparator.get(keyClass), valClass,
+    			compress, codec, progress, secrecy,integrity);
+    }
     /** Create the named map for keys of the named class. */
     public Writer(Configuration conf, FileSystem fs, String dirName,
                   Class<? extends WritableComparable> keyClass, Class valClass,
@@ -110,35 +135,69 @@
       throws IOException {
       this(conf, fs, dirName, WritableComparator.get(keyClass), valClass, compress);
     }
+    public Writer(Configuration conf, FileSystem fs, String dirName,
+    		Class<? extends WritableComparable> keyClass, Class valClass,
+    		CompressionType compress, LabelSet secrecy, LabelSet integrity)
+    throws IOException {
+    	this(conf, fs, dirName, WritableComparator.get(keyClass), valClass, compress, secrecy,integrity);
+    }
 
     /** Create the named map using the named key comparator. */
     public Writer(Configuration conf, FileSystem fs, String dirName,
-                  WritableComparator comparator, Class valClass)
+                  WritableComparator comparator, Class valClass, LabelSet secrecy, LabelSet integrity)
       throws IOException {
       this(conf, fs, dirName, comparator, valClass,
-           SequenceFile.getCompressionType(conf));
+           SequenceFile.getCompressionType(conf), secrecy,integrity);
     }
+    public Writer(Configuration conf, FileSystem fs, String dirName,
+    		WritableComparator comparator, Class valClass)
+    throws IOException {
+    	this(conf, fs, dirName, comparator, valClass,
+    			SequenceFile.getCompressionType(conf));
+    }
     /** Create the named map using the named key comparator. */
     public Writer(Configuration conf, FileSystem fs, String dirName,
                   WritableComparator comparator, Class valClass,
-                  SequenceFile.CompressionType compress)
+                  SequenceFile.CompressionType compress, LabelSet secrecy, LabelSet integrity)
       throws IOException {
-      this(conf, fs, dirName, comparator, valClass, compress, null);
+      this(conf, fs, dirName, comparator, valClass, compress, null, secrecy,integrity);
     }
+    public Writer(Configuration conf, FileSystem fs, String dirName,
+    		WritableComparator comparator, Class valClass,
+    		SequenceFile.CompressionType compress)
+    throws IOException {
+    	this(conf, fs, dirName, comparator, valClass, compress, null);
+    }
     /** Create the named map using the named key comparator. */
     public Writer(Configuration conf, FileSystem fs, String dirName,
                   WritableComparator comparator, Class valClass,
                   SequenceFile.CompressionType compress,
-                  Progressable progress)
+                  Progressable progress, LabelSet secrecy, LabelSet integrity)
       throws IOException {
       this(conf, fs, dirName, comparator, valClass, 
-           compress, new DefaultCodec(), progress);
+           compress, new DefaultCodec(), progress, secrecy,integrity);
     }
+    public Writer(Configuration conf, FileSystem fs, String dirName,
+    		WritableComparator comparator, Class valClass,
+    		SequenceFile.CompressionType compress,
+    		Progressable progress)
+    throws IOException {
+    	this(conf, fs, dirName, comparator, valClass, 
+    			compress, new DefaultCodec(), progress);
+    }
+    
+    public Writer(Configuration conf, FileSystem fs, String dirName,
+            WritableComparator comparator, Class valClass,
+            SequenceFile.CompressionType compress, CompressionCodec codec,
+            Progressable progress)
+throws IOException {
+    	this(conf,fs,dirName,comparator,valClass,compress,codec,progress,LabelSet.EMPTY,LabelSet.EMPTY);
+    }
     /** Create the named map using the named key comparator. */
     public Writer(Configuration conf, FileSystem fs, String dirName,
                   WritableComparator comparator, Class valClass,
                   SequenceFile.CompressionType compress, CompressionCodec codec,
-                  Progressable progress)
+                  Progressable progress, LabelSet secrecy, LabelSet integrity)
       throws IOException {
 
       this.indexInterval = conf.getInt(INDEX_INTERVAL, this.indexInterval);
@@ -156,11 +215,11 @@
       Class keyClass = comparator.getKeyClass();
       this.data =
         SequenceFile.createWriter
-        (fs, conf, dataFile, keyClass, valClass, compress, codec, progress);
+        (fs, conf, dataFile, keyClass, valClass, compress, codec, progress, secrecy,integrity);
       this.index =
         SequenceFile.createWriter
         (fs, conf, indexFile, keyClass, LongWritable.class,
-         CompressionType.BLOCK, progress);
+         CompressionType.BLOCK, progress, secrecy,integrity);
     }
     
     /** The number of entries that are added before an index entry is added.*/
Index: src/core/org/apache/hadoop/io/SequenceFile.java
===================================================================
--- src/core/org/apache/hadoop/io/SequenceFile.java	(revision 142)
+++ src/core/org/apache/hadoop/io/SequenceFile.java	(working copy)
@@ -37,6 +37,7 @@
 import org.apache.hadoop.io.serializer.SerializationFactory;
 import org.apache.hadoop.io.serializer.Serializer;
 import org.apache.hadoop.conf.*;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.Progressable;
 import org.apache.hadoop.util.Progress;
 import org.apache.hadoop.util.ReflectionUtils;
@@ -265,7 +266,13 @@
     return createWriter(fs, conf, name, keyClass, valClass,
                         getCompressionType(conf));
   }
-  
+  public static Writer 
+  createWriter(FileSystem fs, Configuration conf, Path name, 
+               Class keyClass, Class valClass, LabelSet secrecy, LabelSet integrity) 
+  throws IOException {
+  return createWriter(fs, conf, name, keyClass, valClass,
+                      getCompressionType(conf), secrecy,integrity);
+}
   /**
    * Construct the preferred type of SequenceFile Writer.
    * @param fs The configured filesystem. 
@@ -286,7 +293,15 @@
             fs.getDefaultReplication(), fs.getDefaultBlockSize(),
             compressionType, new DefaultCodec(), null, new Metadata());
   }
-  
+  public static Writer 
+  createWriter(FileSystem fs, Configuration conf, Path name, 
+               Class keyClass, Class valClass, CompressionType compressionType, LabelSet secrecy, LabelSet integrity) 
+  throws IOException {
+  return createWriter(fs, conf, name, keyClass, valClass,
+          fs.getConf().getInt("io.file.buffer.size", 4096),
+          fs.getDefaultReplication(), fs.getDefaultBlockSize(),
+          compressionType, new DefaultCodec(), null, new Metadata(), secrecy,integrity);
+}
   /**
    * Construct the preferred type of SequenceFile Writer.
    * @param fs The configured filesystem. 
@@ -308,6 +323,16 @@
             fs.getDefaultReplication(), fs.getDefaultBlockSize(),
             compressionType, new DefaultCodec(), progress, new Metadata());
   }
+  
+  public static Writer
+  createWriter(FileSystem fs, Configuration conf, Path name, 
+               Class keyClass, Class valClass, CompressionType compressionType,
+               Progressable progress, LabelSet secrecy, LabelSet integrity) throws IOException {
+  return createWriter(fs, conf, name, keyClass, valClass,
+          fs.getConf().getInt("io.file.buffer.size", 4096),
+          fs.getDefaultReplication(), fs.getDefaultBlockSize(),
+          compressionType, new DefaultCodec(), progress, new Metadata(), secrecy,integrity);
+}
 
   /**
    * Construct the preferred type of SequenceFile Writer.
@@ -332,6 +357,16 @@
             compressionType, codec, null, new Metadata());
   }
   
+  public static Writer 
+  createWriter(FileSystem fs, Configuration conf, Path name, 
+               Class keyClass, Class valClass, 
+               CompressionType compressionType, CompressionCodec codec, LabelSet secrecy, LabelSet integrity) 
+  throws IOException {
+  return createWriter(fs, conf, name, keyClass, valClass,
+          fs.getConf().getInt("io.file.buffer.size", 4096),
+          fs.getDefaultReplication(), fs.getDefaultBlockSize(),
+          compressionType, codec, null, new Metadata(), secrecy,integrity);
+}
   /**
    * Construct the preferred type of SequenceFile Writer.
    * @param fs The configured filesystem. 
@@ -347,14 +382,25 @@
    * @throws IOException
    */
   public static Writer
+  createWriter(FileSystem fs, Configuration conf, Path name, 
+               Class keyClass, Class valClass, 
+               CompressionType compressionType, CompressionCodec codec,
+               Progressable progress, Metadata metadata) throws IOException {
+	  return createWriter(fs, conf, name, keyClass, valClass,
+	            fs.getConf().getInt("io.file.buffer.size", 4096),
+	            fs.getDefaultReplication(), fs.getDefaultBlockSize(),
+	            compressionType, codec, progress, metadata,LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  
+  public static Writer
     createWriter(FileSystem fs, Configuration conf, Path name, 
                  Class keyClass, Class valClass, 
                  CompressionType compressionType, CompressionCodec codec,
-                 Progressable progress, Metadata metadata) throws IOException {
+                 Progressable progress, Metadata metadata, LabelSet secrecy, LabelSet integrity) throws IOException {
     return createWriter(fs, conf, name, keyClass, valClass,
             fs.getConf().getInt("io.file.buffer.size", 4096),
             fs.getDefaultReplication(), fs.getDefaultBlockSize(),
-            compressionType, codec, progress, metadata);
+            compressionType, codec, progress, metadata, secrecy, integrity);
   }
 
   /**
@@ -375,11 +421,21 @@
    * @throws IOException
    */
   public static Writer
+  createWriter(FileSystem fs, Configuration conf, Path name,
+               Class keyClass, Class valClass, int bufferSize,
+               short replication, long blockSize,
+               CompressionType compressionType, CompressionCodec codec,
+               Progressable progress, Metadata metadata) throws IOException {
+	  return createWriter(fs,conf,name,keyClass,valClass,bufferSize,replication,blockSize,compressionType,codec,progress,metadata, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  
+	  
+  public static Writer
     createWriter(FileSystem fs, Configuration conf, Path name,
                  Class keyClass, Class valClass, int bufferSize,
                  short replication, long blockSize,
                  CompressionType compressionType, CompressionCodec codec,
-                 Progressable progress, Metadata metadata) throws IOException {
+                 Progressable progress, Metadata metadata, LabelSet secrecy, LabelSet integrity) throws IOException {
     if ((codec instanceof GzipCodec) &&
         !NativeCodeLoader.isNativeCodeLoaded() &&
         !ZlibFactory.isNativeZlibLoaded(conf)) {
@@ -392,15 +448,15 @@
     if (compressionType == CompressionType.NONE) {
       writer = new Writer(fs, conf, name, keyClass, valClass,
                           bufferSize, replication, blockSize,
-                          progress, metadata);
+                          progress, metadata, secrecy, integrity);
     } else if (compressionType == CompressionType.RECORD) {
       writer = new RecordCompressWriter(fs, conf, name, keyClass, valClass,
                                         bufferSize, replication, blockSize,
-                                        codec, progress, metadata);
+                                        codec, progress, metadata, secrecy, integrity);
     } else if (compressionType == CompressionType.BLOCK){
       writer = new BlockCompressWriter(fs, conf, name, keyClass, valClass,
                                        bufferSize, replication, blockSize,
-                                       codec, progress, metadata);
+                                       codec, progress, metadata, secrecy, integrity);
     }
 
     return writer;
@@ -428,7 +484,15 @@
                                  compressionType, codec, progress, new Metadata());
     return writer;
   }
-
+  public static Writer
+  createWriter(FileSystem fs, Configuration conf, Path name, 
+               Class keyClass, Class valClass, 
+               CompressionType compressionType, CompressionCodec codec,
+               Progressable progress, LabelSet secrecy, LabelSet integrity) throws IOException {
+  Writer writer = createWriter(fs, conf, name, keyClass, valClass, 
+                               compressionType, codec, progress, new Metadata(), secrecy,integrity);
+  return writer;
+}
   /**
    * Construct the preferred type of 'raw' SequenceFile Writer.
    * @param out The stream on top which the writer is to be constructed.
@@ -820,9 +884,15 @@
     public Writer(FileSystem fs, Configuration conf, Path name, 
                   Class keyClass, Class valClass)
       throws IOException {
-      this(fs, conf, name, keyClass, valClass, null, new Metadata());
+      this(fs, conf, name, keyClass, valClass, null, new Metadata(), LabelSet.EMPTY,LabelSet.EMPTY);
     }
     
+    public Writer(FileSystem fs, Configuration conf, Path name, 
+            Class keyClass, Class valClass, LabelSet secrecy, LabelSet integrity)
+    throws IOException {
+    	this(fs, conf, name, keyClass, valClass, null, new Metadata(), secrecy, integrity);
+    }
+    
     /** Create the named file with write-progress reporter. */
     public Writer(FileSystem fs, Configuration conf, Path name, 
                   Class keyClass, Class valClass,
@@ -831,17 +901,34 @@
       this(fs, conf, name, keyClass, valClass,
            fs.getConf().getInt("io.file.buffer.size", 4096),
            fs.getDefaultReplication(), fs.getDefaultBlockSize(),
-           progress, metadata);
+           progress, metadata, LabelSet.EMPTY,LabelSet.EMPTY);
     }
     
+    public Writer(FileSystem fs, Configuration conf, Path name, 
+    		Class keyClass, Class valClass,
+    		Progressable progress, Metadata metadata, LabelSet secrecy, LabelSet integrity)
+    throws IOException {
+    	this(fs, conf, name, keyClass, valClass,
+    			fs.getConf().getInt("io.file.buffer.size", 4096),
+    			fs.getDefaultReplication(), fs.getDefaultBlockSize(),
+    			progress, metadata, secrecy, integrity);
+    }
     /** Create the named file with write-progress reporter. */
     public Writer(FileSystem fs, Configuration conf, Path name,
                   Class keyClass, Class valClass,
                   int bufferSize, short replication, long blockSize,
                   Progressable progress, Metadata metadata)
       throws IOException {
+    	this(fs,conf,name,keyClass,valClass,bufferSize,replication,blockSize,progress,metadata, LabelSet.EMPTY,LabelSet.EMPTY);
+    }
+    /** Create the named file with write-progress reporter. */
+    public Writer(FileSystem fs, Configuration conf, Path name,
+                  Class keyClass, Class valClass,
+                  int bufferSize, short replication, long blockSize,
+                  Progressable progress, Metadata metadata, LabelSet secrecy, LabelSet integrity)
+      throws IOException {
       init(name, conf,
-           fs.create(name, true, bufferSize, replication, blockSize, progress),
+           fs.create(name, true, bufferSize, replication, blockSize, progress, secrecy, integrity),
               keyClass, valClass, false, null, metadata);
       initializeFileHeader();
       writeFileHeader();
@@ -1052,6 +1139,11 @@
       throws IOException {
       this(conf, fs.create(name), keyClass, valClass, codec, new Metadata());
     }
+    public RecordCompressWriter(FileSystem fs, Configuration conf, Path name, 
+    		Class keyClass, Class valClass, CompressionCodec codec,LabelSet secrecy, LabelSet integrity) 
+    throws IOException {
+    	this(conf, fs.create(name, secrecy, integrity), keyClass, valClass, codec, new Metadata());
+    }
     
     /** Create the named file with write-progress reporter. */
     public RecordCompressWriter(FileSystem fs, Configuration conf, Path name, 
@@ -1064,6 +1156,15 @@
            progress, metadata);
     }
 
+    public RecordCompressWriter(FileSystem fs, Configuration conf, Path name, 
+    		Class keyClass, Class valClass, CompressionCodec codec,
+    		Progressable progress, Metadata metadata,LabelSet secrecy, LabelSet integrity) 
+    throws IOException {
+    	this(fs, conf, name, keyClass, valClass,
+    			fs.getConf().getInt("io.file.buffer.size", 4096),
+    			fs.getDefaultReplication(), fs.getDefaultBlockSize(), codec,
+    			progress, metadata, secrecy, integrity);
+    }
     /** Create the named file with write-progress reporter. */
     public RecordCompressWriter(FileSystem fs, Configuration conf, Path name,
                                 Class keyClass, Class valClass,
@@ -1079,7 +1180,21 @@
       writeFileHeader();
       finalizeFileHeader();
     }
+    public RecordCompressWriter(FileSystem fs, Configuration conf, Path name,
+    		Class keyClass, Class valClass,
+    		int bufferSize, short replication, long blockSize,
+    		CompressionCodec codec,
+    		Progressable progress, Metadata metadata,LabelSet secrecy, LabelSet integrity)
+    throws IOException {
+    	super.init(name, conf,
+    			fs.create(name, true, bufferSize, replication, blockSize, progress, secrecy, integrity),
+    			keyClass, valClass, true, codec, metadata);
 
+    	initializeFileHeader();
+    	writeFileHeader();
+    	finalizeFileHeader();
+    }
+
     /** Create the named file with write-progress reporter. */
     public RecordCompressWriter(FileSystem fs, Configuration conf, Path name, 
                                 Class keyClass, Class valClass, CompressionCodec codec,
@@ -1088,6 +1203,14 @@
       this(fs, conf, name, keyClass, valClass, codec, progress, new Metadata());
     }
     
+    /** Create the named file with write-progress reporter. */
+    public RecordCompressWriter(FileSystem fs, Configuration conf, Path name, 
+                                Class keyClass, Class valClass, CompressionCodec codec,
+                                Progressable progress,LabelSet secrecy, LabelSet integrity)
+      throws IOException {
+      this(fs, conf, name, keyClass, valClass, codec, progress, new Metadata(), secrecy, integrity);
+    }
+    
     /** Write to an arbitrary stream using a specified buffer size. */
     private RecordCompressWriter(Configuration conf, FSDataOutputStream out,
                                  Class keyClass, Class valClass, CompressionCodec codec, Metadata metadata)
@@ -1177,6 +1300,15 @@
            null, new Metadata());
     }
     
+    public BlockCompressWriter(FileSystem fs, Configuration conf, Path name, 
+    		Class keyClass, Class valClass, CompressionCodec codec, LabelSet secrecy, LabelSet integrity) 
+    throws IOException {
+    	this(fs, conf, name, keyClass, valClass,
+    			fs.getConf().getInt("io.file.buffer.size", 4096),
+    			fs.getDefaultReplication(), fs.getDefaultBlockSize(), codec,
+    			null, new Metadata(), secrecy, integrity);
+    }
+    
     /** Create the named file with write-progress reporter. */
     public BlockCompressWriter(FileSystem fs, Configuration conf, Path name, 
                                Class keyClass, Class valClass, CompressionCodec codec,
@@ -1187,7 +1319,15 @@
            fs.getDefaultReplication(), fs.getDefaultBlockSize(), codec,
            progress, metadata);
     }
-
+    public BlockCompressWriter(FileSystem fs, Configuration conf, Path name, 
+    		Class keyClass, Class valClass, CompressionCodec codec,
+    		Progressable progress, Metadata metadata, LabelSet secrecy, LabelSet integrity)
+    throws IOException {
+    	this(fs, conf, name, keyClass, valClass,
+    			fs.getConf().getInt("io.file.buffer.size", 4096),
+    			fs.getDefaultReplication(), fs.getDefaultBlockSize(), codec,
+    			progress, metadata, secrecy, integrity);
+    }
     /** Create the named file with write-progress reporter. */
     public BlockCompressWriter(FileSystem fs, Configuration conf, Path name,
                                Class keyClass, Class valClass,
@@ -1205,6 +1345,21 @@
       finalizeFileHeader();
     }
 
+    public BlockCompressWriter(FileSystem fs, Configuration conf, Path name,
+    		Class keyClass, Class valClass,
+    		int bufferSize, short replication, long blockSize,
+    		CompressionCodec codec,
+    		Progressable progress, Metadata metadata, LabelSet secrecy, LabelSet integrity)
+    throws IOException {
+    	super.init(name, conf,
+    			fs.create(name, true, bufferSize, replication, blockSize, progress, secrecy, integrity),
+    			keyClass, valClass, true, codec, metadata);
+    	init(conf.getInt("io.seqfile.compress.blocksize", 1000000));
+
+    	initializeFileHeader();
+    	writeFileHeader();
+    	finalizeFileHeader();
+    }
     /** Create the named file with write-progress reporter. */
     public BlockCompressWriter(FileSystem fs, Configuration conf, Path name, 
                                Class keyClass, Class valClass, CompressionCodec codec,
@@ -1213,6 +1368,12 @@
       this(fs, conf, name, keyClass, valClass, codec, progress, new Metadata());
     }
     
+    public BlockCompressWriter(FileSystem fs, Configuration conf, Path name, 
+    		Class keyClass, Class valClass, CompressionCodec codec,
+    		Progressable progress, LabelSet secrecy, LabelSet integrity)
+    throws IOException {
+    	this(fs, conf, name, keyClass, valClass, codec, progress, new Metadata(), secrecy, integrity);
+    }
     /** Write to an arbitrary stream using a specified buffer size. */
     private BlockCompressWriter(Configuration conf, FSDataOutputStream out,
                                 Class keyClass, Class valClass, CompressionCodec codec, Metadata metadata)
@@ -1409,19 +1570,34 @@
     /** Open the named file. */
     public Reader(FileSystem fs, Path file, Configuration conf)
       throws IOException {
-      this(fs, file, conf.getInt("io.file.buffer.size", 4096), conf, false);
+      this(fs, file, conf.getInt("io.file.buffer.size", 4096), conf, false, LabelSet.EMPTY,LabelSet.EMPTY);
     }
 
+    public Reader(FileSystem fs, Path file, Configuration conf, LabelSet secrecy, LabelSet integrity)
+    throws IOException {
+    	this(fs, file, conf.getInt("io.file.buffer.size", 4096), conf, false, secrecy, integrity);
+    }
     private Reader(FileSystem fs, Path file, int bufferSize,
                    Configuration conf, boolean tempReader) throws IOException {
-      this(fs, file, bufferSize, 0, fs.getLength(file), conf, tempReader);
+      this(fs, file, bufferSize, 0, fs.getLength(file), conf, tempReader, LabelSet.EMPTY,LabelSet.EMPTY);
     }
     
+    private Reader(FileSystem fs, Path file, int bufferSize,
+            Configuration conf, boolean tempReader, LabelSet secrecy, LabelSet integrity) throws IOException {
+    	this(fs, file, bufferSize, 0, fs.getLength(file), conf, tempReader, secrecy, integrity);
+    }
+    
     private Reader(FileSystem fs, Path file, int bufferSize, long start,
-                   long length, Configuration conf, boolean tempReader) 
+            long length, Configuration conf, boolean tempReader) 
     throws IOException {
+    	this(fs,file,bufferSize, start,length,conf,tempReader,LabelSet.EMPTY,LabelSet.EMPTY);
+    }
+
+    private Reader(FileSystem fs, Path file, int bufferSize, long start,
+                   long length, Configuration conf, boolean tempReader, LabelSet secrecy, LabelSet integrity) 
+    throws IOException {
       this.file = file;
-      this.in = openFile(fs, file, bufferSize, length);
+      this.in = openFile(fs, file, bufferSize, length, secrecy, integrity);
       this.conf = conf;
       seek(start);
       this.end = in.getPos() + length;
@@ -1433,8 +1609,13 @@
      * {@link FSDataInputStream} returned.
      */
     protected FSDataInputStream openFile(FileSystem fs, Path file,
-        int bufferSize, long length) throws IOException {
-      return fs.open(file, bufferSize);
+            int bufferSize, long length) throws IOException {
+          return fs.open(file, bufferSize, LabelSet.EMPTY,LabelSet.EMPTY);
+        }
+
+    protected FSDataInputStream openFile(FileSystem fs, Path file,
+        int bufferSize, long length, LabelSet secrecy, LabelSet integrity) throws IOException {
+      return fs.open(file, bufferSize, secrecy, integrity);
     }
     
     /**
Index: src/core/org/apache/hadoop/fs/FsShell.java
===================================================================
--- src/core/org/apache/hadoop/fs/FsShell.java	(revision 142)
+++ src/core/org/apache/hadoop/fs/FsShell.java	(working copy)
@@ -44,7 +44,7 @@
 import org.apache.hadoop.util.Tool;
 import org.apache.hadoop.util.ToolRunner;
 import org.apache.hadoop.hdfs.DistributedFileSystem;
-
+import org.apache.hadoop.security.LabelSet;
 /** Provide command line access to a FileSystem. */
 public class FsShell extends Configured implements Tool {
 
@@ -112,6 +112,22 @@
     }
   }
 
+  private void copyFromStdinLabeled(Path dst, FileSystem dstFs, LabelSet secrecy, LabelSet integrity) throws IOException {
+	    if (dstFs.isDirectory(dst)) {
+	      throw new IOException("When source is stdin, destination must be a file.");
+	    }
+	    if (dstFs.exists(dst)) {
+	      throw new IOException("Target " + dst.toString() + " already exists.");
+	    }
+	    FSDataOutputStream out = dstFs.create(dst, secrecy, integrity); 
+	    try {
+	      IOUtils.copyBytes(System.in, out, getConf(), false);
+	    } 
+	    finally {
+	      out.close();
+	    }
+	  }
+  
   /** 
    * Print from src to stdout.
    */
@@ -136,6 +152,15 @@
       dstFs.copyFromLocalFile(false, false, srcs, dstPath);
   }
   
+  void copyFromLocalLabeled(Path[] srcs, String dstf, LabelSet secrecy, LabelSet integrity) throws IOException {
+	  Path dstPath = new Path(dstf);
+	    FileSystem dstFs = dstPath.getFileSystem(getConf());
+	    if (srcs.length == 1 && srcs[0].toString().equals("-"))
+	      copyFromStdinLabeled(dstPath, dstFs, secrecy, integrity);
+	    else
+	      dstFs.copyFromLocalFileLabeled(false, false, srcs, dstPath, secrecy, integrity);
+  }
+  
   /**
    * Add local files to the indicated FileSystem name. src is removed.
    */
@@ -162,6 +187,52 @@
    * @exception: IOException  
    * @see org.apache.hadoop.fs.FileSystem.globStatus 
    */
+  /*DIFC: labeled version for copying*/
+  void copyToLocalLabeled(String[]argv, int pos) throws IOException {
+	  	CommandFormat cf = new CommandFormat("copyToLocalLabeled", 4,4,"crc","ignoreCrc");
+	   
+	  	String srcstr = null;
+	    String dststr = null;
+	    LabelSet secrecy=LabelSet.EMPTY;
+	    LabelSet integrity=LabelSet.EMPTY;
+	    try {
+	      List<String> parameters = cf.parse(argv, pos);
+	      srcstr = parameters.get(0);
+	      dststr = parameters.get(1);
+	      secrecy=new LabelSet(parameters.get(2));
+	      integrity=new LabelSet(parameters.get(3));
+	    }
+	    catch(IllegalArgumentException iae) {
+	      System.err.println("Usage: java FsShell " + GET_SHORT_USAGE);
+	      throw iae;
+	    }
+	    final boolean copyCrc = cf.getOpt("crc");
+	    final boolean verifyChecksum = !cf.getOpt("ignoreCrc");
+
+	    if (dststr.equals("-")) {
+	      if (copyCrc) {
+	        System.err.println("-crc option is not valid when destination is stdout.");
+	      }
+	      /*DIFC: TODO change this*/
+	      cat(srcstr, verifyChecksum, secrecy, integrity);
+	    } else {
+	      File dst = new File(dststr);      
+	      Path srcpath = new Path(srcstr);
+	      FileSystem srcFS = getSrcFileSystem(srcpath, verifyChecksum);
+	      FileStatus[] srcs = srcFS.globStatus(srcpath);
+	      boolean dstIsDir = dst.isDirectory(); 
+	      if (srcs.length > 1 && !dstIsDir) {
+	        throw new IOException("When copying multiple files, "
+	                              + "destination should be a directory.");
+	      }
+	      for (FileStatus status : srcs) {
+	        Path p = status.getPath();
+	        File f = dstIsDir? new File(dst, p.getName()): dst;
+	        copyToLocal(srcFS, p, f, copyCrc, secrecy, integrity);
+	      }
+	    } 
+  }
+  
   void copyToLocal(String[]argv, int pos) throws IOException {
     CommandFormat cf = new CommandFormat("copyToLocal", 2,2,"crc","ignoreCrc");
     
@@ -231,7 +302,12 @@
    * @exception IOException If some IO failed
    */
   private void copyToLocal(final FileSystem srcFS, final Path src,
-                           final File dst, final boolean copyCrc)
+		  final File dst, final boolean copyCrc)
+  throws IOException {
+	  copyToLocal(srcFS, src, dst,copyCrc, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  private void copyToLocal(final FileSystem srcFS, final Path src,
+                           final File dst, final boolean copyCrc, LabelSet secrecy, LabelSet integrity)
     throws IOException {
     /* Keep the structure similar to ChecksumFileSystem.copyToLocal(). 
      * Ideal these two should just invoke FileUtil.copy() and not repeat
@@ -248,15 +324,15 @@
       // use absolute name so that tmp file is always created under dest dir
       File tmp = FileUtil.createLocalTempFile(dst.getAbsoluteFile(),
                                               COPYTOLOCAL_PREFIX, true);
-      if (!FileUtil.copy(srcFS, src, tmp, false, srcFS.getConf())) {
+      if (!FileUtil.copy(srcFS, src, tmp, false, srcFS.getConf(), secrecy, integrity)) {
         throw new IOException("Failed to copy " + src + " to " + dst); 
       }
-      
+   
       if (!tmp.renameTo(dst)) {
         throw new IOException("Failed to rename tmp file " + tmp + 
                               " to local destination \"" + dst + "\".");
       }
-
+      /*DIFC:TODO fix this for labeled case*/
       if (copyCrc) {
         ChecksumFileSystem csfs = (ChecksumFileSystem) srcFS;
         File dstcs = FileSystem.getLocal(srcFS.getConf())
@@ -269,7 +345,7 @@
       dst.mkdirs();
       for(FileStatus path : srcFS.listStatus(src)) {
         copyToLocal(srcFS, path.getPath(), 
-                    new File(dst, path.getPath().getName()), copyCrc);
+                    new File(dst, path.getPath().getName()), copyCrc, secrecy, integrity);
       }
     }
   }
@@ -333,6 +409,9 @@
    * @see org.apache.hadoop.fs.FileSystem.globStatus 
    */
   void cat(String src, boolean verifyChecksum) throws IOException {
+	  cat(src,verifyChecksum, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  void cat(String src, boolean verifyChecksum, LabelSet secrecy, LabelSet integrity) throws IOException {
     //cat behavior in Linux
     //  [~/1207]$ ls ?.txt
     //  x.txt  z.txt
@@ -342,13 +421,13 @@
     //  zzz
 
     Path srcPattern = new Path(src);
-    new DelayedExceptionThrowing() {
+    new DelayedExceptionThrowing(secrecy, integrity) {
       @Override
       void process(Path p, FileSystem srcFs) throws IOException {
         if (srcFs.getFileStatus(p).isDir()) {
           throw new IOException("Source must be a file.");
         }
-        printToStdout(srcFs.open(p));
+        printToStdout(srcFs.open(p, this.secrecy, this.integrity));
       }
     }.globAndProcess(srcPattern, getSrcFileSystem(srcPattern, verifyChecksum));
   }
@@ -1517,7 +1596,7 @@
         //
         if ("-cat".equals(cmd)) {
           cat(argv[i], true);
-        } else if ("-mkdir".equals(cmd)) {
+        }else if ("-mkdir".equals(cmd)) {
           mkdir(argv[i]);
         } else if ("-rm".equals(cmd)) {
           delete(argv[i], false);
@@ -1575,7 +1654,10 @@
    */
   private static void printUsage(String cmd) {
     String prefix = "Usage: java " + FsShell.class.getSimpleName();
-    if ("-fs".equals(cmd)) {
+    if ("-putLabeled".equals(cmd)) {
+        System.err.println("Usage: java FsShell" + 
+        		 " [" + cmd + " <localsrc> ... <dst> <secrecy-label(int)> <integrity-label(int)>]");
+      } else if ("-fs".equals(cmd)) {
       System.err.println("Usage: java FsShell" + 
                          " [-fs <local | file system URI>]");
     } else if ("-conf".equals(cmd)) {
@@ -1602,15 +1684,25 @@
                          " [" + cmd + " <localsrc> ... <dst>]");
     } else if ("-get".equals(cmd)) {
       System.err.println("Usage: java FsShell [" + GET_SHORT_USAGE + "]"); 
-    } else if ("-copyToLocal".equals(cmd)) {
+    }else if ("-getLabeled".equals(cmd)) {
+      System.err.println("Usage: java FsShell [" + GET_SHORT_USAGE + " <secrecy-label(int)> <integrity-label(int)>]"); 
+    }else if ("-copyToLocal".equals(cmd)) {
       System.err.println("Usage: java FsShell [" + COPYTOLOCAL_SHORT_USAGE+ "]"); 
-    } else if ("-moveToLocal".equals(cmd)) {
+    }else if ("-copyToLocalLabeled".equals(cmd)) {
+      System.err.println("Usage: java FsShell [" + COPYTOLOCAL_SHORT_USAGE+ " <secrecy-label(int)> <integrity-label(int)>]"); 
+    }else if ("-moveToLocal".equals(cmd)) {
       System.err.println("Usage: java FsShell" + 
                          " [" + cmd + " [-crc] <src> <localdst>]");
-    } else if ("-cat".equals(cmd)) {
+    }else if ("-moveToLocalLabeled".equals(cmd)) {
+        System.err.println("Usage: java FsShell" + 
+                " [" + cmd + " [-crc] <src> <localdst> <secrecy-label(int)> <integrity-label(int)>]");
+    }else if ("-cat".equals(cmd)) {
       System.err.println("Usage: java FsShell" + 
                          " [" + cmd + " <src>]");
-    } else if ("-setrep".equals(cmd)) {
+    }else if ("-catLabeled".equals(cmd)) {
+        System.err.println("Usage: java FsShell" + 
+                " [" + cmd + " <src> <secrecy-label(int)> <integrity-label(int)>]");
+    }   else if ("-setrep".equals(cmd)) {
       System.err.println("Usage: java FsShell [" + SETREP_SHORT_USAGE + "]");
     } else if ("-test".equals(cmd)) {
       System.err.println("Usage: java FsShell" +
@@ -1673,19 +1765,31 @@
     //
     // verify that we have enough command line parameters
     //
-    if ("-put".equals(cmd) || "-test".equals(cmd) ||
+    if ("-putLabeled".equals(cmd)) {
+    	if (argv.length < 5) {
+            printUsage(cmd);
+            return exitCode;
+          }
+    	} else if ("-put".equals(cmd) || "-test".equals(cmd) ||
         "-copyFromLocal".equals(cmd) || "-moveFromLocal".equals(cmd)) {
       if (argv.length < 3) {
         printUsage(cmd);
         return exitCode;
       }
+ 
     } else if ("-get".equals(cmd) || 
                "-copyToLocal".equals(cmd) || "-moveToLocal".equals(cmd)) {
       if (argv.length < 3) {
         printUsage(cmd);
         return exitCode;
       }
-    } else if ("-mv".equals(cmd) || "-cp".equals(cmd)) {
+    }else if ("-getLabeled".equals(cmd) || 
+            "-copyToLocalLabeled".equals(cmd) || "-moveToLocalLabeled".equals(cmd)) {
+        if (argv.length < 5) {
+          printUsage(cmd);
+          return exitCode;
+        }
+      }else if ("-mv".equals(cmd) || "-cp".equals(cmd)) {
       if (argv.length < 3) {
         printUsage(cmd);
         return exitCode;
@@ -1698,6 +1802,11 @@
         printUsage(cmd);
         return exitCode;
       }
+    } else if("-catLabeled".equals(cmd)){
+    	if(argv.length<4){
+    		printUsage(cmd);
+    		return exitCode;
+    	}
     }
 
     // initialize FsShell
@@ -1719,22 +1828,31 @@
         for (int j=0 ; i < argv.length-1 ;) 
           srcs[j++] = new Path(argv[i++]);
         copyFromLocal(srcs, argv[i++]);
-      } else if ("-moveFromLocal".equals(cmd)) {
+      }else if ("-putLabeled".equals(cmd)) {
+          Path[] srcs = new Path[argv.length-4];
+          for (int j=0 ; i < argv.length-3 ;) 
+            srcs[j++] = new Path(argv[i++]);
+          copyFromLocalLabeled(srcs, argv[i++],new LabelSet(argv[i++]), new LabelSet(argv[i++]));
+      }else if ("-moveFromLocal".equals(cmd)) {
         Path[] srcs = new Path[argv.length-2];
         for (int j=0 ; i < argv.length-1 ;) 
           srcs[j++] = new Path(argv[i++]);
         moveFromLocal(srcs, argv[i++]);
       } else if ("-get".equals(cmd) || "-copyToLocal".equals(cmd)) {
         copyToLocal(argv, i);
-      } else if ("-getmerge".equals(cmd)) {
+      }else if ("-getLabeled".equals(cmd) || "-copyToLocalLabeled".equals(cmd)) {
+        copyToLocalLabeled(argv, i);
+      }else if ("-getmerge".equals(cmd)) {
         if (argv.length>i+2)
           copyMergeToLocal(argv[i++], new Path(argv[i++]), Boolean.parseBoolean(argv[i++]));
         else
           copyMergeToLocal(argv[i++], new Path(argv[i++]));
       } else if ("-cat".equals(cmd)) {
         exitCode = doall(cmd, argv, i);
-      } else if ("-text".equals(cmd)) {
-        exitCode = doall(cmd, argv, i);
+      }else if ("-catLabeled".equals(cmd)) {
+        cat(argv[i], true, new LabelSet(argv[i+1]), new LabelSet(argv[i+2]));
+      }else if ("-text".equals(cmd)) {
+    	  exitCode = doall(cmd, argv, i);
       } else if ("-moveToLocal".equals(cmd)) {
         moveToLocal(argv[i++], new Path(argv[i++]));
       } else if ("-setrep".equals(cmd)) {
@@ -1863,7 +1981,15 @@
    */
   private abstract class DelayedExceptionThrowing {
     abstract void process(Path p, FileSystem srcFs) throws IOException;
-
+    LabelSet secrecy=LabelSet.EMPTY;
+    LabelSet integrity=LabelSet.EMPTY;
+    
+    DelayedExceptionThrowing(LabelSet secSet, LabelSet intSet){
+    	this.secrecy=secSet;
+    	this.integrity=intSet;
+    }
+    
+    DelayedExceptionThrowing(){}
     final void globAndProcess(Path srcPattern, FileSystem srcFs
         ) throws IOException {
       List<IOException> exceptions = new ArrayList<IOException>();
Index: src/core/org/apache/hadoop/fs/kfs/KosmosFileSystem.java
===================================================================
--- src/core/org/apache/hadoop/fs/kfs/KosmosFileSystem.java	(revision 142)
+++ src/core/org/apache/hadoop/fs/kfs/KosmosFileSystem.java	(working copy)
@@ -33,7 +33,7 @@
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.util.Progressable;
 import org.apache.hadoop.fs.BlockLocation;
-
+import org.apache.hadoop.security.LabelSet;
 /**
  * A FileSystem backed by KFS.
  *
@@ -165,7 +165,7 @@
 
     public FSDataOutputStream create(Path file, FsPermission permission,
                                      boolean overwrite, int bufferSize,
-				     short replication, long blockSize, Progressable progress)
+				     short replication, long blockSize, Progressable progress, LabelSet secrecy, LabelSet integrity)
 	throws IOException {
 
         if (exists(file)) {
@@ -187,7 +187,11 @@
         return kfsImpl.create(srep, replication, bufferSize);
     }
 
-    public FSDataInputStream open(Path path, int bufferSize) throws IOException {
+    public FSDataInputStream open(Path f, int bufferSize) throws IOException {
+  	  return open(f,bufferSize, LabelSet.EMPTY,LabelSet.EMPTY);
+    }
+    
+    public FSDataInputStream open(Path path, int bufferSize, LabelSet secrecy, LabelSet integrity) throws IOException {
         if (!exists(path))
             throw new IOException("File does not exist: " + path);
 
@@ -197,7 +201,7 @@
         return kfsImpl.open(srep, bufferSize);
     }
 
-    public boolean rename(Path src, Path dst) throws IOException {
+    public boolean rename(Path src, Path dst, LabelSet secrecy, LabelSet integrty) throws IOException {
 	Path absoluteS = makeAbsolute(src);
         String srepS = absoluteS.toUri().getPath();
 	Path absoluteD = makeAbsolute(dst);
Index: src/core/org/apache/hadoop/fs/s3/S3FileSystem.java
===================================================================
--- src/core/org/apache/hadoop/fs/s3/S3FileSystem.java	(revision 142)
+++ src/core/org/apache/hadoop/fs/s3/S3FileSystem.java	(working copy)
@@ -39,7 +39,7 @@
 import org.apache.hadoop.io.retry.RetryPolicy;
 import org.apache.hadoop.io.retry.RetryProxy;
 import org.apache.hadoop.util.Progressable;
-
+import org.apache.hadoop.security.LabelSet;
 /**
  * <p>
  * A block-based {@link FileSystem} backed by
@@ -206,7 +206,7 @@
   @Override
   public FSDataOutputStream create(Path file, FsPermission permission,
       boolean overwrite, int bufferSize,
-      short replication, long blockSize, Progressable progress)
+      short replication, long blockSize, Progressable progress, LabelSet secrecy, LabelSet integrity)
     throws IOException {
 
     INode inode = store.retrieveINode(makeAbsolute(file));
@@ -230,15 +230,19 @@
          statistics);
   }
 
+  public FSDataInputStream open(Path f, int bufferSize) throws IOException {
+	  return open(f,bufferSize, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  
   @Override
-  public FSDataInputStream open(Path path, int bufferSize) throws IOException {
+  public FSDataInputStream open(Path path, int bufferSize, LabelSet secrecy, LabelSet integrity) throws IOException {
     INode inode = checkFile(path);
     return new FSDataInputStream(new S3InputStream(getConf(), store, inode,
                                                    statistics));
   }
 
   @Override
-  public boolean rename(Path src, Path dst) throws IOException {
+  public boolean rename(Path src, Path dst, LabelSet secrecy, LabelSet integrty) throws IOException {
     Path absoluteSrc = makeAbsolute(src);
     INode srcINode = store.retrieveINode(absoluteSrc);
     if (srcINode == null) {
Index: src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java
===================================================================
--- src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java	(revision 142)
+++ src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java	(working copy)
@@ -36,7 +36,7 @@
 import org.apache.hadoop.fs.permission.FsAction;
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.util.Progressable;
-
+import org.apache.hadoop.security.LabelSet;
 /**
  * <p>
  * A {@link FileSystem} backed by an FTP client provided by <a
@@ -154,8 +154,11 @@
     return new Path(workDir, path);
   }
 
+  public FSDataInputStream open(Path f, int bufferSize) throws IOException {
+	  return open(f,bufferSize, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
   @Override
-  public FSDataInputStream open(Path file, int bufferSize) throws IOException {
+  public FSDataInputStream open(Path file, int bufferSize, LabelSet secrecy, LabelSet integrity) throws IOException {
     FTPClient client = connect();
     Path workDir = new Path(client.printWorkingDirectory());
     Path absolute = makeAbsolute(workDir, file);
@@ -193,7 +196,7 @@
   @Override
   public FSDataOutputStream create(Path file, FsPermission permission,
       boolean overwrite, int bufferSize, short replication, long blockSize,
-      Progressable progress) throws IOException {
+      Progressable progress, LabelSet secrecy, LabelSet integrity) throws IOException {
     final FTPClient client = connect();
     Path workDir = new Path(client.printWorkingDirectory());
     Path absolute = makeAbsolute(workDir, file);
@@ -505,7 +508,7 @@
    * assumption correct or it is suppose to work like 'move' ?
    */
   @Override
-  public boolean rename(Path src, Path dst) throws IOException {
+  public boolean rename(Path src, Path dst, LabelSet secrecy, LabelSet integrty) throws IOException {
     FTPClient client = connect();
     try {
       boolean success = rename(client, src, dst);
Index: src/core/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java
===================================================================
--- src/core/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java	(revision 142)
+++ src/core/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java	(working copy)
@@ -53,7 +53,7 @@
 import org.apache.hadoop.io.retry.RetryPolicy;
 import org.apache.hadoop.io.retry.RetryProxy;
 import org.apache.hadoop.util.Progressable;
-
+import org.apache.hadoop.security.LabelSet;
 /**
  * <p>
  * A {@link FileSystem} for reading and writing files stored on
@@ -268,7 +268,7 @@
   @Override
   public FSDataOutputStream create(Path f, FsPermission permission,
       boolean overwrite, int bufferSize, short replication, long blockSize,
-      Progressable progress) throws IOException {
+      Progressable progress, LabelSet secrecy, LabelSet integrity) throws IOException {
 
     if (exists(f) && !overwrite) {
       throw new IOException("File already exists:"+f);
@@ -441,8 +441,12 @@
     return true;
   }
 
+  public FSDataInputStream open(Path f, int bufferSize) throws IOException {
+	  return open(f,bufferSize, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  
   @Override
-  public FSDataInputStream open(Path f, int bufferSize) throws IOException {
+  public FSDataInputStream open(Path f, int bufferSize, LabelSet secrecy, LabelSet integrity) throws IOException {
     if (!exists(f)) {
       throw new FileNotFoundException(f.toString());
     }
@@ -497,7 +501,7 @@
 
 
   @Override
-  public boolean rename(Path src, Path dst) throws IOException {
+  public boolean rename(Path src, Path dst, LabelSet secrecy, LabelSet integrty) throws IOException {
 
     String srcKey = pathToKey(makeAbsolute(src));
 
Index: src/core/org/apache/hadoop/fs/FileSystem.java
===================================================================
--- src/core/org/apache/hadoop/fs/FileSystem.java	(revision 142)
+++ src/core/org/apache/hadoop/fs/FileSystem.java	(working copy)
@@ -33,8 +33,8 @@
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.hdfs.DistributedFileSystem;
 import org.apache.hadoop.io.MultipleIOException;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.security.UserGroupInformation;
-
 /****************************************************************
  * An abstract base class for a fairly generic filesystem.  It
  * may be implemented as a distributed filesystem, or as a "local"
@@ -257,9 +257,20 @@
    * @throws IOException
    */
   public static FSDataOutputStream create(FileSystem fs,
-      Path file, FsPermission permission) throws IOException {
+	      Path file, FsPermission permission) throws IOException {
+	return createLabeled(fs,file,permission,LabelSet.EMPTY,LabelSet.EMPTY);  
+  }
+  
+  public static FSDataOutputStream create(FileSystem fs,
+	      Path file, FsPermission permission, LabelSet secrecy, LabelSet integrity) throws IOException {
+	return createLabeled(fs,file,permission,secrecy,integrity);  
+  }
+  
+  /*DIFC: pass labels while creating files*/
+  public static FSDataOutputStream createLabeled(FileSystem fs,
+      Path file, FsPermission permission, LabelSet secrecy, LabelSet integrity) throws IOException {
     // create the file with default permission
-    FSDataOutputStream out = fs.create(file);
+    FSDataOutputStream out = fs.create(file, secrecy, integrity);
     // set its permission to the supplied one
     fs.setPermission(file, permission);
     return out;
@@ -334,7 +345,11 @@
    * The FileSystem will simply return an elt containing 'localhost'.
    */
   public BlockLocation[] getFileBlockLocations(FileStatus file, 
-      long start, long len) throws IOException {
+		  long start, long len) throws IOException {
+	  return getFileBlockLocations(file,start,len,LabelSet.EMPTY,LabelSet.EMPTY);
+	  }
+  public BlockLocation[] getFileBlockLocations(FileStatus file, 
+      long start, long len,LabelSet secrecy, LabelSet integrity) throws IOException {
     if (file == null) {
       return null;
     }
@@ -348,34 +363,52 @@
    * @param f the file name to open
    * @param bufferSize the size of the buffer to be used.
    */
-  public abstract FSDataInputStream open(Path f, int bufferSize)
-    throws IOException;
+  //public abstract FSDataInputStream open(Path f, int bufferSize)
+    //throws IOException;
     
+  public FSDataInputStream open(Path f, int bufferSize)
+  throws IOException{
+	  return open(f, bufferSize, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  public abstract FSDataInputStream open(Path f, int bufferSize, LabelSet secrecy, LabelSet integrity)
+  throws IOException;
+  
   /**
    * Opens an FSDataInputStream at the indicated Path.
    * @param f the file to open
    */
   public FSDataInputStream open(Path f) throws IOException {
-    return open(f, getConf().getInt("io.file.buffer.size", 4096));
+	    return open(f,LabelSet.EMPTY,LabelSet.EMPTY);
   }
+  public FSDataInputStream open(Path f, LabelSet secrecy, LabelSet integrity) throws IOException {
+    return open(f, getConf().getInt("io.file.buffer.size", 4096), secrecy, integrity);
+  }
 
   /**
    * Opens an FSDataOutputStream at the indicated Path.
    * Files are overwritten by default.
    */
   public FSDataOutputStream create(Path f) throws IOException {
-    return create(f, true);
+    return create(f, true, LabelSet.EMPTY, LabelSet.EMPTY);
   }
-
+  
+  /*DIFC:labeled version*/
+  public FSDataOutputStream create(Path f, LabelSet secrecy, LabelSet integrity) throws IOException {
+	    return create(f, true, secrecy, integrity);
+  }
   /**
    * Opens an FSDataOutputStream at the indicated Path.
    */
-  public FSDataOutputStream create(Path f, boolean overwrite)
+  public FSDataOutputStream create(Path f, boolean overwrite)  throws IOException {
+	return create(f, overwrite, LabelSet.EMPTY, LabelSet.EMPTY);  
+  }
+  
+  public FSDataOutputStream create(Path f, boolean overwrite, LabelSet secrecy, LabelSet integrity)
     throws IOException {
     return create(f, overwrite, 
                   getConf().getInt("io.file.buffer.size", 4096),
                   getDefaultReplication(),
-                  getDefaultBlockSize());
+                  getDefaultBlockSize(), secrecy,integrity);
   }
 
   /**
@@ -384,10 +417,14 @@
    * Files are overwritten by default.
    */
   public FSDataOutputStream create(Path f, Progressable progress) throws IOException {
+	  return create(f, progress, LabelSet.EMPTY, LabelSet.EMPTY);
+  }
+  /*DIFC: changed*/
+  public FSDataOutputStream create(Path f, Progressable progress, LabelSet secrecy, LabelSet integrity) throws IOException {
     return create(f, true, 
                   getConf().getInt("io.file.buffer.size", 4096),
                   getDefaultReplication(),
-                  getDefaultBlockSize(), progress);
+                  getDefaultBlockSize(), progress, secrecy, integrity);
   }
 
   /**
@@ -395,11 +432,16 @@
    * Files are overwritten by default.
    */
   public FSDataOutputStream create(Path f, short replication)
+  throws IOException {
+	  return create(f, replication, LabelSet.EMPTY, LabelSet.EMPTY);
+  }
+  
+  public FSDataOutputStream create(Path f, short replication, LabelSet secrecy, LabelSet integrity)
     throws IOException {
     return create(f, true, 
                   getConf().getInt("io.file.buffer.size", 4096),
                   replication,
-                  getDefaultBlockSize());
+                  getDefaultBlockSize(), secrecy, integrity);
   }
 
   /**
@@ -408,11 +450,16 @@
    * Files are overwritten by default.
    */
   public FSDataOutputStream create(Path f, short replication, Progressable progress)
+  throws IOException {
+	  return createLabeled(f, replication, progress, LabelSet.EMPTY, LabelSet.EMPTY);
+  }
+  
+  public FSDataOutputStream createLabeled(Path f, short replication, Progressable progress, LabelSet secrecy, LabelSet integrity)
     throws IOException {
     return create(f, true, 
                   getConf().getInt("io.file.buffer.size", 4096),
                   replication,
-                  getDefaultBlockSize(), progress);
+                  getDefaultBlockSize(), progress, secrecy, integrity);
   }
 
     
@@ -424,12 +471,18 @@
    * @param bufferSize the size of the buffer to be used.
    */
   public FSDataOutputStream create(Path f, 
+          boolean overwrite,
+          int bufferSize
+          ) throws IOException {
+	  return create(f,overwrite,bufferSize,LabelSet.EMPTY, LabelSet.EMPTY);
+  }
+  public FSDataOutputStream create(Path f, 
                                    boolean overwrite,
-                                   int bufferSize
+                                   int bufferSize, LabelSet secrecy, LabelSet integrity
                                    ) throws IOException {
     return create(f, overwrite, bufferSize, 
                   getDefaultReplication(),
-                  getDefaultBlockSize());
+                  getDefaultBlockSize(), secrecy, integrity);
   }
     
   /**
@@ -441,13 +494,22 @@
    * @param bufferSize the size of the buffer to be used.
    */
   public FSDataOutputStream create(Path f, 
+          boolean overwrite,
+          int bufferSize,
+          Progressable progress
+          ) throws IOException {
+	  return create(f, overwrite, bufferSize, progress, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  
+  
+  public FSDataOutputStream create(Path f, 
                                    boolean overwrite,
                                    int bufferSize,
-                                   Progressable progress
+                                   Progressable progress, LabelSet secrecy, LabelSet integrity
                                    ) throws IOException {
     return create(f, overwrite, bufferSize, 
                   getDefaultReplication(),
-                  getDefaultBlockSize(), progress);
+                  getDefaultBlockSize(), progress, secrecy, integrity);
   }
     
     
@@ -460,12 +522,21 @@
    * @param replication required block replication for the file. 
    */
   public FSDataOutputStream create(Path f, 
+          boolean overwrite,
+          int bufferSize,
+          short replication,
+          long blockSize
+          ) throws IOException {
+	  return create(f,overwrite,bufferSize, replication,blockSize, LabelSet.EMPTY, LabelSet.EMPTY);
+  }
+  
+  public FSDataOutputStream create(Path f, 
                                    boolean overwrite,
                                    int bufferSize,
                                    short replication,
-                                   long blockSize
+                                   long blockSize, LabelSet secrecy, LabelSet integrity
                                    ) throws IOException {
-    return create(f, overwrite, bufferSize, replication, blockSize, null);
+    return create(f, overwrite, bufferSize, replication, blockSize, null, secrecy, integrity);
   }
 
   /**
@@ -478,14 +549,24 @@
    * @param replication required block replication for the file. 
    */
   public FSDataOutputStream create(Path f,
+          boolean overwrite,
+          int bufferSize,
+          short replication,
+          long blockSize,
+          Progressable progress
+          ) throws IOException {
+	  return create(f,overwrite,bufferSize,replication,blockSize,progress,LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  
+   public FSDataOutputStream create(Path f,
                                             boolean overwrite,
                                             int bufferSize,
                                             short replication,
                                             long blockSize,
-                                            Progressable progress
+                                            Progressable progress, LabelSet secrecy, LabelSet integrity
                                             ) throws IOException {
     return this.create(f, FsPermission.getDefault(),
-        overwrite, bufferSize, replication, blockSize, progress);
+        overwrite, bufferSize, replication, blockSize, progress, secrecy, integrity);
   }
 
   /**
@@ -502,13 +583,24 @@
    * @throws IOException
    * @see #setPermission(Path, FsPermission)
    */
+  public FSDataOutputStream create(Path f,
+	      FsPermission permission,
+	      boolean overwrite,
+	      int bufferSize,
+	      short replication,
+	      long blockSize,
+	      Progressable progress) throws IOException{
+	  
+	  return this.create(f,permission,overwrite,bufferSize,replication,blockSize,progress,LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  
   public abstract FSDataOutputStream create(Path f,
       FsPermission permission,
       boolean overwrite,
       int bufferSize,
       short replication,
       long blockSize,
-      Progressable progress) throws IOException;
+      Progressable progress, LabelSet secrecy, LabelSet integrity) throws IOException;
 
   /**
    * Creates the given Path as a brand-new zero-length file.  If
@@ -584,7 +676,10 @@
    * Renames Path src to Path dst.  Can take place on local fs
    * or remote DFS.
    */
-  public abstract boolean rename(Path src, Path dst) throws IOException;
+  public abstract boolean rename(Path src, Path dst, LabelSet secrecy, LabelSet integrty) throws IOException;
+  public boolean rename(Path src, Path dst) throws IOException{
+	  return rename(src,dst,LabelSet.EMPTY,LabelSet.EMPTY);
+  }
     
   /** Delete a file. */
   /** @deprecated Use delete(Path, boolean) instead */ @Deprecated 
@@ -1172,9 +1267,16 @@
                                 Path[] srcs, Path dst)
     throws IOException {
     Configuration conf = getConf();
-    FileUtil.copy(getLocal(conf), srcs, this, dst, delSrc, overwrite, conf);
+    FileUtil.copyLabeled(getLocal(conf), srcs, this, dst, delSrc, overwrite, conf,LabelSet.EMPTY,LabelSet.EMPTY);
   }
   
+  public void copyFromLocalFileLabeled(boolean delSrc, boolean overwrite, 
+          Path[] srcs, Path dst, LabelSet secrecy, LabelSet integrity)
+  throws IOException {
+	  Configuration conf = getConf();
+	  FileUtil.copyLabeled(getLocal(conf), srcs, this, dst, delSrc, overwrite, conf,secrecy, integrity);
+  }
+  
   /**
    * The src file is on the local disk.  Add it to FS at
    * the given dst name.
@@ -1184,7 +1286,7 @@
                                 Path src, Path dst)
     throws IOException {
     Configuration conf = getConf();
-    FileUtil.copy(getLocal(conf), src, this, dst, delSrc, overwrite, conf);
+    FileUtil.copyLabeled(getLocal(conf), src, this, dst, delSrc, overwrite, conf, LabelSet.EMPTY,LabelSet.EMPTY);
   }
     
   /**
Index: src/core/org/apache/hadoop/fs/FilterFileSystem.java
===================================================================
--- src/core/org/apache/hadoop/fs/FilterFileSystem.java	(revision 142)
+++ src/core/org/apache/hadoop/fs/FilterFileSystem.java	(working copy)
@@ -24,7 +24,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.util.Progressable;
-
+import org.apache.hadoop.security.LabelSet;
 /****************************************************************
  * A <code>FilterFileSystem</code> contains
  * some other file system, which it uses as
@@ -98,8 +98,11 @@
    * @param bufferSize the size of the buffer to be used.
    */
   public FSDataInputStream open(Path f, int bufferSize) throws IOException {
-    return fs.open(f, bufferSize);
+	  return open(f,bufferSize, LabelSet.EMPTY,LabelSet.EMPTY);
   }
+  public FSDataInputStream open(Path f, int bufferSize, LabelSet secrecy, LabelSet integrity) throws IOException {
+    return fs.open(f, bufferSize, secrecy, integrity);
+  }
 
   /** {@inheritDoc} */
   public FSDataOutputStream append(Path f, int bufferSize,
@@ -111,9 +114,9 @@
   @Override
   public FSDataOutputStream create(Path f, FsPermission permission,
       boolean overwrite, int bufferSize, short replication, long blockSize,
-      Progressable progress) throws IOException {
+      Progressable progress, LabelSet secrecy, LabelSet integrity) throws IOException {
     return fs.create(f, permission,
-        overwrite, bufferSize, replication, blockSize, progress);
+        overwrite, bufferSize, replication, blockSize, progress, secrecy, integrity);
   }
 
   /**
@@ -133,8 +136,8 @@
    * Renames Path src to Path dst.  Can take place on local fs
    * or remote DFS.
    */
-  public boolean rename(Path src, Path dst) throws IOException {
-    return fs.rename(src, dst);
+  public boolean rename(Path src, Path dst, LabelSet secrecy, LabelSet integrity) throws IOException {
+    return fs.rename(src, dst,secrecy,integrity);
   }
   
   /** Delete a file */@Deprecated
Index: src/core/org/apache/hadoop/fs/InMemoryFileSystem.java
===================================================================
--- src/core/org/apache/hadoop/fs/InMemoryFileSystem.java	(revision 142)
+++ src/core/org/apache/hadoop/fs/InMemoryFileSystem.java	(working copy)
@@ -27,7 +27,7 @@
 import org.apache.hadoop.io.DataInputBuffer;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.util.Progressable;
-
+import org.apache.hadoop.security.LabelSet;
 /** An implementation of the in-memory filesystem. This implementation assumes
  * that the file lengths are known ahead of time and the total lengths of all
  * the files is below a certain number (like 100 MB, configurable). Use the API
@@ -126,7 +126,7 @@
       public long skip(long n) throws IOException { return din.skip(n); }
     }
 
-    public FSDataInputStream open(Path f, int bufferSize) throws IOException {
+    public FSDataInputStream open(Path f, int bufferSize, LabelSet secrecy, LabelSet integrity) throws IOException {
       return new FSDataInputStream(new InMemoryInputStream(f));
     }
 
@@ -187,7 +187,7 @@
      */
     public FSDataOutputStream create(Path f, FsPermission permission,
                                      boolean overwrite, int bufferSize,
-                                     short replication, long blockSize, Progressable progress)
+                                     short replication, long blockSize, Progressable progress, LabelSet secrecy, LabelSet integrity)
       throws IOException {
       synchronized (this) {
         if (exists(f) && !overwrite) {
@@ -229,7 +229,7 @@
       return true;
     }
 
-    public boolean rename(Path src, Path dst) throws IOException {
+    public boolean rename(Path src, Path dst, LabelSet secrecy, LabelSet integrty) throws IOException {
       synchronized (this) {
         if (exists(dst)) {
           throw new IOException ("Path " + dst + " already exists");
Index: src/core/org/apache/hadoop/fs/ChecksumFileSystem.java
===================================================================
--- src/core/org/apache/hadoop/fs/ChecksumFileSystem.java	(revision 142)
+++ src/core/org/apache/hadoop/fs/ChecksumFileSystem.java	(working copy)
@@ -28,7 +28,7 @@
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.util.Progressable;
 import org.apache.hadoop.util.StringUtils;
-
+import org.apache.hadoop.security.LabelSet;
 /****************************************************************
  * Abstract Checksumed FileSystem.
  * It provide a basice implementation of a Checksumed FileSystem,
@@ -270,8 +270,11 @@
    * @param f the file name to open
    * @param bufferSize the size of the buffer to be used.
    */
+  public FSDataInputStream open(Path f, int bufferSize) throws IOException {
+	  return open(f,bufferSize, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
   @Override
-  public FSDataInputStream open(Path f, int bufferSize) throws IOException {
+  public FSDataInputStream open(Path f, int bufferSize, LabelSet secrecy, LabelSet integrity) throws IOException {
     return new FSDataInputStream(
         new ChecksumFSInputChecker(this, f, bufferSize));
   }
@@ -348,11 +351,12 @@
     }
   }
 
+  /*DIFC: Todo complete the labeled implementation*/
   /** {@inheritDoc} */
   @Override
   public FSDataOutputStream create(Path f, FsPermission permission,
       boolean overwrite, int bufferSize, short replication, long blockSize,
-      Progressable progress) throws IOException {
+      Progressable progress, LabelSet secrecy, LabelSet integrity) throws IOException {
     Path parent = f.getParent();
     if (parent != null && !mkdirs(parent)) {
       throw new IOException("Mkdirs failed to create " + parent);
@@ -390,21 +394,21 @@
   /**
    * Rename files/dirs
    */
-  public boolean rename(Path src, Path dst) throws IOException {
+  public boolean rename(Path src, Path dst, LabelSet secrecy, LabelSet integrity) throws IOException {
     if (fs.isDirectory(src)) {
-      return fs.rename(src, dst);
+      return fs.rename(src, dst, secrecy, integrity);
     } else {
 
-      boolean value = fs.rename(src, dst);
+      boolean value = fs.rename(src, dst,secrecy,integrity);
       if (!value)
         return false;
 
       Path checkFile = getChecksumFile(src);
       if (fs.exists(checkFile)) { //try to rename checksum
         if (fs.isDirectory(dst)) {
-          value = fs.rename(checkFile, dst);
+          value = fs.rename(checkFile, dst, secrecy,integrity);
         } else {
-          value = fs.rename(checkFile, getChecksumFile(dst));
+          value = fs.rename(checkFile, getChecksumFile(dst), secrecy,integrity);
         }
       }
 
Index: src/core/org/apache/hadoop/fs/RawLocalFileSystem.java
===================================================================
--- src/core/org/apache/hadoop/fs/RawLocalFileSystem.java	(revision 142)
+++ src/core/org/apache/hadoop/fs/RawLocalFileSystem.java	(working copy)
@@ -29,7 +29,7 @@
 import org.apache.hadoop.util.Progressable;
 import org.apache.hadoop.util.StringUtils;
 import org.apache.hadoop.util.Shell;
-
+import org.apache.hadoop.security.LabelSet;
 /****************************************************************
  * Implement the FileSystem API for the raw local filesystem.
  *
@@ -170,6 +170,10 @@
   }
   
   public FSDataInputStream open(Path f, int bufferSize) throws IOException {
+	  return open(f,bufferSize, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  
+  public FSDataInputStream open(Path f, int bufferSize, LabelSet secrecy, LabelSet integrity) throws IOException {
     if (!exists(f)) {
       throw new FileNotFoundException(f.toString());
     }
@@ -246,14 +250,14 @@
   @Override
   public FSDataOutputStream create(Path f, FsPermission permission,
       boolean overwrite, int bufferSize, short replication, long blockSize,
-      Progressable progress) throws IOException {
+      Progressable progress, LabelSet secrecy, LabelSet integrity) throws IOException {
     FSDataOutputStream out = create(f,
         overwrite, bufferSize, replication, blockSize, progress);
     setPermission(f, permission);
     return out;
   }
   
-  public boolean rename(Path src, Path dst) throws IOException {
+  public boolean rename(Path src, Path dst, LabelSet secrecy, LabelSet integrty) throws IOException {
     if (pathToFile(src).renameTo(pathToFile(dst))) {
       return true;
     }
Index: src/core/org/apache/hadoop/fs/FileUtil.java
===================================================================
--- src/core/org/apache/hadoop/fs/FileUtil.java	(revision 142)
+++ src/core/org/apache/hadoop/fs/FileUtil.java	(working copy)
@@ -24,6 +24,7 @@
 import java.util.zip.ZipFile;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.StringUtils;
 import org.apache.hadoop.util.Shell;
 import org.apache.hadoop.util.Shell.ShellCommandExecutor;
@@ -133,26 +134,39 @@
       }
     }
   }
+  
+  public static boolean copy(FileSystem srcFS, Path src, 
+          FileSystem dstFS, Path dst, 
+          boolean deleteSource,
+          Configuration conf) throws IOException {
+	  return copyLabeled(srcFS, src, dstFS, dst, deleteSource, conf, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
 
   /** Copy files between FileSystems. */
-  public static boolean copy(FileSystem srcFS, Path src, 
+  public static boolean copyLabeled(FileSystem srcFS, Path src, 
                              FileSystem dstFS, Path dst, 
                              boolean deleteSource,
-                             Configuration conf) throws IOException {
-    return copy(srcFS, src, dstFS, dst, deleteSource, true, conf);
+                             Configuration conf, LabelSet secrecy, LabelSet integrity) throws IOException {
+    return copyLabeled(srcFS, src, dstFS, dst, deleteSource, true, conf, secrecy, integrity);
   }
 
   public static boolean copy(FileSystem srcFS, Path[] srcs, 
+          FileSystem dstFS, Path dst,
+          boolean deleteSource, 
+          boolean overwrite, Configuration conf)
+          throws IOException {
+	  return copyLabeled(srcFS,srcs,dstFS,dst,deleteSource,overwrite,conf,LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  public static boolean copyLabeled(FileSystem srcFS, Path[] srcs, 
                              FileSystem dstFS, Path dst,
                              boolean deleteSource, 
-                             boolean overwrite, Configuration conf)
+                             boolean overwrite, Configuration conf, LabelSet secrecy, LabelSet integrity)
                              throws IOException {
     boolean gotException = false;
     boolean returnVal = true;
     StringBuffer exceptions = new StringBuffer();
-
     if (srcs.length == 1)
-      return copy(srcFS, srcs[0], dstFS, dst, deleteSource, overwrite, conf);
+      return copyLabeled(srcFS, srcs[0], dstFS, dst, deleteSource, overwrite, conf, secrecy, integrity);
 
     // Check if dest is directory
     if (!dstFS.exists(dst)) {
@@ -167,7 +181,7 @@
 
     for (Path src : srcs) {
       try {
-        if (!copy(srcFS, src, dstFS, dst, deleteSource, overwrite, conf))
+        if (!copyLabeled(srcFS, src, dstFS, dst, deleteSource, overwrite, conf, secrecy, integrity))
           returnVal = false;
       } catch (IOException e) {
         gotException = true;
@@ -181,14 +195,20 @@
     return returnVal;
   }
 
+  public static boolean copy(FileSystem srcFS, Path src, 
+          FileSystem dstFS, Path dst, 
+          boolean deleteSource,
+          boolean overwrite,
+          Configuration conf) throws IOException {
+	  return copyLabeled(srcFS, src, dstFS, dst, deleteSource, overwrite, conf, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
   /** Copy files between FileSystems. */
-  public static boolean copy(FileSystem srcFS, Path src, 
+  public static boolean copyLabeled(FileSystem srcFS, Path src, 
                              FileSystem dstFS, Path dst, 
                              boolean deleteSource,
                              boolean overwrite,
-                             Configuration conf) throws IOException {
+                             Configuration conf, LabelSet secrecy, LabelSet integrity) throws IOException {
     dst = checkDest(src.getName(), dstFS, dst, overwrite);
-
     if (srcFS.getFileStatus(src).isDir()) {
       checkDependencies(srcFS, src, dstFS, dst);
       if (!dstFS.mkdirs(dst)) {
@@ -196,16 +216,16 @@
       }
       FileStatus contents[] = srcFS.listStatus(src);
       for (int i = 0; i < contents.length; i++) {
-        copy(srcFS, contents[i].getPath(), dstFS, 
+        copyLabeled(srcFS, contents[i].getPath(), dstFS, 
              new Path(dst, contents[i].getPath().getName()),
-             deleteSource, overwrite, conf);
+             deleteSource, overwrite, conf, secrecy, integrity);
       }
     } else if (srcFS.isFile(src)) {
       InputStream in=null;
       OutputStream out = null;
       try {
         in = srcFS.open(src);
-        out = dstFS.create(dst, overwrite);
+        out = dstFS.create(dst, overwrite, secrecy, integrity);
         IOUtils.copyBytes(in, out, conf, true);
       } catch (IOException e) {
         IOUtils.closeStream(out);
@@ -262,11 +282,17 @@
     }
   }  
   
+  public static boolean copy(File src,
+          FileSystem dstFS, Path dst,
+          boolean deleteSource,
+          Configuration conf) throws IOException {
+	  return copyLabeled(src, dstFS, dst, deleteSource,conf, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
   /** Copy local files to a FileSystem. */
-  public static boolean copy(File src,
+  public static boolean copyLabeled(File src,
                              FileSystem dstFS, Path dst,
                              boolean deleteSource,
-                             Configuration conf) throws IOException {
+                             Configuration conf, LabelSet secrecy, LabelSet integrity) throws IOException {
     dst = checkDest(src.getName(), dstFS, dst, false);
 
     if (src.isDirectory()) {
@@ -275,15 +301,15 @@
       }
       File contents[] = src.listFiles();
       for (int i = 0; i < contents.length; i++) {
-        copy(contents[i], dstFS, new Path(dst, contents[i].getName()),
-             deleteSource, conf);
+        copyLabeled(contents[i], dstFS, new Path(dst, contents[i].getName()),
+             deleteSource, conf, secrecy, integrity);
       }
     } else if (src.isFile()) {
       InputStream in = null;
       OutputStream out =null;
       try {
         in = new FileInputStream(src);
-        out = dstFS.create(dst);
+        out = dstFS.create(dst, secrecy, integrity);
         IOUtils.copyBytes(in, out, conf);
       } catch (IOException e) {
         IOUtils.closeStream( out );
@@ -303,8 +329,14 @@
 
   /** Copy FileSystem files to local files. */
   public static boolean copy(FileSystem srcFS, Path src, 
+          File dst, boolean deleteSource,
+          Configuration conf) throws IOException {
+	  return copy(srcFS,src,dst,deleteSource,conf,LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  public static boolean copy(FileSystem srcFS, Path src, 
                              File dst, boolean deleteSource,
-                             Configuration conf) throws IOException {
+                             Configuration conf, LabelSet secrecy, LabelSet integrity) throws IOException {
+	/*DIFC: TODO access to status should also be restricted*/
     if (srcFS.getFileStatus(src).isDir()) {
       if (!dst.mkdirs()) {
         return false;
@@ -313,16 +345,17 @@
       for (int i = 0; i < contents.length; i++) {
         copy(srcFS, contents[i].getPath(), 
              new File(dst, contents[i].getPath().getName()),
-             deleteSource, conf);
+             deleteSource, conf, secrecy, integrity);
       }
     } else if (srcFS.isFile(src)) {
-      InputStream in = srcFS.open(src);
+      InputStream in = srcFS.open(src, secrecy, integrity);
       IOUtils.copyBytes(in, new FileOutputStream(dst), conf);
     } else {
       throw new IOException(src.toString() + 
                             ": No such file or directory");
     }
     if (deleteSource) {
+    	/*DIFC: TODO make this also labeled*/
       return srcFS.delete(src, true);
     } else {
       return true;
Index: src/core/org/apache/hadoop/fs/HarFileSystem.java
===================================================================
--- src/core/org/apache/hadoop/fs/HarFileSystem.java	(revision 142)
+++ src/core/org/apache/hadoop/fs/HarFileSystem.java	(working copy)
@@ -27,6 +27,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.io.Text;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.LineReader;
 import org.apache.hadoop.util.Progressable;
 
@@ -545,8 +546,11 @@
    * file. It reads the index files to get the part 
    * file name and the size and start of the file.
    */
+  public FSDataInputStream open(Path f, int bufferSize) throws IOException {
+	  return open(f,bufferSize,LabelSet.EMPTY,LabelSet.EMPTY);
+  }
   @Override
-  public FSDataInputStream open(Path f, int bufferSize) throws IOException {
+  public FSDataInputStream open(Path f, int bufferSize, LabelSet secrecy, LabelSet integrity) throws IOException {
     // get the fs DataInputStream for the the underlying file
     // look up the index.
     Path p = makeQualified(f);
Index: src/core/org/apache/hadoop/ipc/RPC.java
===================================================================
--- src/core/org/apache/hadoop/ipc/RPC.java	(revision 142)
+++ src/core/org/apache/hadoop/ipc/RPC.java	(working copy)
@@ -443,7 +443,7 @@
       try {
         Invocation call = (Invocation)param;
         if (verbose) log("Call: " + call);
-        
+        //LOG.info("###RPC:method="+call.getMethodName());
         Method method =
           implementation.getMethod(call.getMethodName(),
                                    call.getParameterClasses());
Index: src/core/org/apache/hadoop/ipc/Server.java
===================================================================
--- src/core/org/apache/hadoop/ipc/Server.java	(revision 142)
+++ src/core/org/apache/hadoop/ipc/Server.java	(working copy)
@@ -456,6 +456,7 @@
           writeSelector.select(PURGE_INTERVAL);
           Iterator<SelectionKey> iter = writeSelector.selectedKeys().iterator();
           while (iter.hasNext()) {
+        	
             SelectionKey key = iter.next();
             iter.remove();
             try {
@@ -837,13 +838,10 @@
       DataInputStream dis =
         new DataInputStream(new ByteArrayInputStream(data.array()));
       int id = dis.readInt();                    // try to read an id
-        
       if (LOG.isDebugEnabled())
         LOG.debug(" got #" + id);
-            
       Writable param = ReflectionUtils.newInstance(paramClass, conf);           // read param
       param.readFields(dis);        
-        
       Call call = new Call(id, param, this);
       callQueue.put(call);              // queue the call; maybe blocked here
     }
Index: src/core/org/apache/hadoop/ipc/Client.java
===================================================================
--- src/core/org/apache/hadoop/ipc/Client.java	(revision 142)
+++ src/core/org/apache/hadoop/ipc/Client.java	(working copy)
@@ -681,7 +681,7 @@
   public Writable call(Writable param, InetSocketAddress addr, 
                        UserGroupInformation ticket)  
                        throws InterruptedException, IOException {
-    Call call = new Call(param);
+ 	Call call = new Call(param);
     Connection connection = getConnection(addr, ticket, call);
     connection.sendParam(call);                 // send the parameter
     synchronized (call) {
Index: src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java
===================================================================
--- src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java	(revision 142)
+++ src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java	(working copy)
@@ -71,6 +71,7 @@
 import org.apache.hadoop.util.GenericOptionsParser;
 import org.apache.hadoop.util.StringUtils;
 import org.apache.hadoop.util.Tool;
+import org.apache.hadoop.security.LabelSet;
 
 /** All the client-side work happens here.
  * (Jar packaging, MapRed job submission and monitoring)
@@ -127,7 +128,7 @@
       preProcessArgs();
       parseArgv();
       postProcessArgs();
-  
+     
       setJobConf();
       return submitAndMonitorJob();
     }catch (IllegalArgumentException ex) {
@@ -244,6 +245,13 @@
       inputSpecs_.addAll(cmdLine.getValues("-input"));
       output_ = (String) cmdLine.getValue("-output"); 
       
+      /*DIFC: parse the labels*/
+      secrecyLabel_.addAll(cmdLine.getValues("-secrecyLabel"));
+      integrityLabel_.addAll(cmdLine.getValues("-integrityLabel"));
+      
+      /*Airavat: get the hadoop path*/
+      hadoopPath_=(String) cmdLine.getValue("-hadoopPath");
+      tempDirPath_=(String) cmdLine.getValue("-tempDirPath");
       mapCmd_ = (String)cmdLine.getValue("-mapper"); 
       comCmd_ = (String)cmdLine.getValue("-combiner"); 
       redCmd_ = (String)cmdLine.getValue("-reducer"); 
@@ -396,7 +404,26 @@
                                   "path", 
                                   Integer.MAX_VALUE, 
                                   true);  
-    
+    Option hadoopPath   = createOption("hadoopPath", 
+            "Path to bin/hadoop", 
+            "path", 
+            1, 
+            false);  
+    Option tempDirPath   = createOption("tempDirPath", 
+            "Path to any temp dir. in the local m/c", 
+            "path", 
+            1, 
+            false);  
+    Option secrecyLabel   = createOption("secrecyLabel", 
+            "Secrecy label(s) for the Map reduce", 
+            "int", 
+            Integer.MAX_VALUE, 
+            false);  
+    Option integrityLabel   = createOption("integrityLabel", 
+            "Integrity label(s) for the Map reduce", 
+            "int", 
+            Integer.MAX_VALUE, 
+            false);  
     Option output  = createOption("output", 
                                   "DFS output directory for the Reduce step", 
                                   "path", 1, true); 
@@ -445,6 +472,10 @@
     
     allOptions = new GroupBuilder().
       withOption(input).
+      withOption(hadoopPath).
+      withOption(tempDirPath).
+      withOption(secrecyLabel).
+      withOption(integrityLabel).
       withOption(output).
       withOption(mapper).
       withOption(combiner).
@@ -498,6 +529,10 @@
     System.out.println("  -reducedebug <path>  Optional." +
     " To run this script when a reduce task fails ");
     System.out.println("  -verbose");
+    System.out.println("  -secrecyLabel    <integer>     Secrecy label(s) for the MapReduce steps");
+    System.out.println("  -integrityLabel    <integer>     Integrity label(s) for the MapReduce steps");
+    System.out.println("  -hadoopPath    <path>     Path to bin/hadoop in local machine");
+    System.out.println("  -tempDirPath    <path>     Path to temporary directory in local machine");
     System.out.println();
     GenericOptionsParser.printGenericCommandUsage(System.out);
 
@@ -716,7 +751,24 @@
     if (fmt == null) {
       fmt = StreamInputFormat.class;
     }
+    
+    /*Set hadoop path*/
+    if(hadoopPath_!=null)
+    	jobConf_.setLocalHadoopPath(hadoopPath_);
+    if(tempDirPath_!=null)
+    	jobConf_.setTempDirPath(tempDirPath_);
+    /*DIFC: set the labels*/
+    int len=secrecyLabel_.size();
+    long[] labels=new long[len];
+    for(int i=0;i<len;i++)
+    	labels[i]=Long.parseLong(secrecyLabel_.get(i));
+    jobConf_.setSecrecyLabel(new LabelSet(labels,len));
 
+    len=integrityLabel_.size();
+    labels=new long[len];
+    for(int i=0;i<len;i++)
+    	labels[i]=Long.parseLong(integrityLabel_.get(i));
+    jobConf_.setIntegrityLabel(new LabelSet(labels, len));
     jobConf_.setInputFormat(fmt);
 
     jobConf_.setOutputKeyClass(Text.class);
@@ -1054,4 +1106,11 @@
   protected static String LINK_URI = "You need to specify the uris as hdfs://host:port/#linkname," +
     "Please specify a different link name for all of your caching URIs";
 
+  /*DIFC: option to read the secrecy and integrity labels*/
+  protected ArrayList<String> secrecyLabel_ = new ArrayList<String>(); 
+  protected ArrayList<String> integrityLabel_ = new ArrayList<String>();
+  /*Location of bin/hadoop so that we can start it off*/
+  protected String hadoopPath_="";
+  /*Temp directory on local machine where the mapper can write the sensitivity*/
+  protected String tempDirPath_="";
 }
Index: src/contrib/hive/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
===================================================================
--- src/contrib/hive/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java	(revision 142)
+++ src/contrib/hive/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java	(working copy)
@@ -43,6 +43,7 @@
 import org.apache.hadoop.mapred.Reporter;
 
 import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.ReflectionUtils;
 
 /**
@@ -163,8 +164,13 @@
     return inputFormats.get(inputFormatClass);
   }
 
+
   public RecordReader getRecordReader(InputSplit split, JobConf job,
       Reporter reporter) throws IOException {
+	  return getRecordReader(split, job, reporter, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  public RecordReader getRecordReader(InputSplit split, JobConf job,
+      Reporter reporter, LabelSet secrecy, LabelSet integrity) throws IOException {
 
     HiveInputSplit hsplit = (HiveInputSplit)split;
 
Index: src/contrib/index/src/java/org/apache/hadoop/contrib/index/mapred/IndexUpdateOutputFormat.java
===================================================================
--- src/contrib/index/src/java/org/apache/hadoop/contrib/index/mapred/IndexUpdateOutputFormat.java	(revision 142)
+++ src/contrib/index/src/java/org/apache/hadoop/contrib/index/mapred/IndexUpdateOutputFormat.java	(working copy)
@@ -28,7 +28,7 @@
 import org.apache.hadoop.mapred.RecordWriter;
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.util.Progressable;
-
+import org.apache.hadoop.security.LabelSet;
 /**
  * The record writer of this output format simply puts a message in an output
  * path when a shard update is done.
@@ -39,7 +39,7 @@
    * @see FileOutputFormat#getRecordWriter(FileSystem, JobConf, String, Progressable)
    */
   public RecordWriter<Shard, Text> getRecordWriter(final FileSystem fs,
-      JobConf job, String name, final Progressable progress)
+      JobConf job, String name, final Progressable progress, LabelSet secrecy, LabelSet integrity)
       throws IOException {
 
     final Path perm = new Path(getWorkOutputPath(job), name);
Index: src/contrib/index/src/java/org/apache/hadoop/contrib/index/example/LineDocRecordReader.java
===================================================================
--- src/contrib/index/src/java/org/apache/hadoop/contrib/index/example/LineDocRecordReader.java	(revision 142)
+++ src/contrib/index/src/java/org/apache/hadoop/contrib/index/example/LineDocRecordReader.java	(working copy)
@@ -33,6 +33,7 @@
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.mapred.FileSplit;
 import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.security.LabelSet;
 
 /**
  * A simple RecordReader for LineDoc for plain text files where each line is a
@@ -76,6 +77,10 @@
    * @throws IOException
    */
   public LineDocRecordReader(Configuration job, FileSplit split)
+  throws IOException {
+	  this(job, split, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  public LineDocRecordReader(Configuration job, FileSplit split, LabelSet secrecy, LabelSet integrity)
       throws IOException {
     long start = split.getStart();
     long end = start + split.getLength();
@@ -83,7 +88,7 @@
 
     // open the file and seek to the start of the split
     FileSystem fs = file.getFileSystem(job);
-    FSDataInputStream fileIn = fs.open(split.getPath());
+    FSDataInputStream fileIn = fs.open(split.getPath(), secrecy , integrity);
     InputStream in = fileIn;
     boolean skipFirstLine = false;
     if (start != 0) {
Index: src/contrib/index/src/java/org/apache/hadoop/contrib/index/example/LineDocInputFormat.java
===================================================================
--- src/contrib/index/src/java/org/apache/hadoop/contrib/index/example/LineDocInputFormat.java	(revision 142)
+++ src/contrib/index/src/java/org/apache/hadoop/contrib/index/example/LineDocInputFormat.java	(working copy)
@@ -27,6 +27,7 @@
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.RecordReader;
 import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.security.LabelSet;
 
 /**
  * An InputFormat for LineDoc for plain text files where each line is a doc.
@@ -37,10 +38,15 @@
   /* (non-Javadoc)
    * @see org.apache.hadoop.mapred.FileInputFormat#getRecordReader(org.apache.hadoop.mapred.InputSplit, org.apache.hadoop.mapred.JobConf, org.apache.hadoop.mapred.Reporter)
    */
+	public RecordReader<DocumentID, LineDocTextAndOp> getRecordReader(
+		      InputSplit split, JobConf job, Reporter reporter) throws IOException {
+		return getRecordReader(split, job, reporter, LabelSet.EMPTY,LabelSet.EMPTY);
+	}
+	
   public RecordReader<DocumentID, LineDocTextAndOp> getRecordReader(
-      InputSplit split, JobConf job, Reporter reporter) throws IOException {
+      InputSplit split, JobConf job, Reporter reporter, LabelSet secrecy, LabelSet integrity) throws IOException {
     reporter.setStatus(split.toString());
-    return new LineDocRecordReader(job, (FileSplit) split);
+    return new LineDocRecordReader(job, (FileSplit) split, secrecy , integrity);
   }
 
 }
Index: src/tools/org/apache/hadoop/tools/HadoopArchives.java
===================================================================
--- src/tools/org/apache/hadoop/tools/HadoopArchives.java	(revision 142)
+++ src/tools/org/apache/hadoop/tools/HadoopArchives.java	(working copy)
@@ -56,6 +56,7 @@
 import org.apache.hadoop.mapred.SequenceFileRecordReader;
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.mapred.lib.NullOutputFormat;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.Tool;
 import org.apache.hadoop.util.ToolRunner;
 
@@ -203,8 +204,14 @@
     public RecordReader<LongWritable, Text> getRecordReader(InputSplit split,
         JobConf job, Reporter reporter) throws IOException {
       return new SequenceFileRecordReader<LongWritable, Text>(job,
-                 (FileSplit)split);
+                 (FileSplit)split, LabelSet.EMPTY,LabelSet.EMPTY);
     }
+    
+    public RecordReader<LongWritable, Text> getRecordReader(InputSplit split,
+            JobConf job, Reporter reporter, LabelSet secrecy, LabelSet integrity) throws IOException {
+          return new SequenceFileRecordReader<LongWritable, Text>(job,
+                     (FileSplit)split, secrecy, integrity);
+        }
   }
 
   private boolean checkValidName(String name) {
Index: src/tools/org/apache/hadoop/tools/DistCp.java
===================================================================
--- src/tools/org/apache/hadoop/tools/DistCp.java	(revision 142)
+++ src/tools/org/apache/hadoop/tools/DistCp.java	(working copy)
@@ -63,6 +63,7 @@
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.mapred.SequenceFileRecordReader;
 import org.apache.hadoop.security.AccessControlException;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.StringUtils;
 import org.apache.hadoop.util.Tool;
 import org.apache.hadoop.util.ToolRunner;
@@ -294,9 +295,13 @@
      * Returns a reader for this split of the src file list.
      */
     public RecordReader<Text, Text> getRecordReader(InputSplit split,
-        JobConf job, Reporter reporter) throws IOException {
-      return new SequenceFileRecordReader<Text, Text>(job, (FileSplit)split);
+            JobConf job, Reporter reporter) throws IOException {
+          return new SequenceFileRecordReader<Text, Text>(job, (FileSplit)split, LabelSet.EMPTY, LabelSet.EMPTY);
     }
+    public RecordReader<Text, Text> getRecordReader(InputSplit split,
+        JobConf job, Reporter reporter, LabelSet secrecy, LabelSet integrity) throws IOException {
+      return new SequenceFileRecordReader<Text, Text>(job, (FileSplit)split, secrecy, integrity);
+    }
   }
 
   /**
Index: src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java	(revision 142)
+++ src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java	(working copy)
@@ -26,7 +26,7 @@
 import org.apache.hadoop.fs.permission.*;
 import org.apache.hadoop.fs.ContentSummary;
 import org.apache.hadoop.fs.FileStatus;
-
+import org.apache.hadoop.security.LabelSet;
 /**********************************************************************
  * ClientProtocol is used by user code via 
  * {@link org.apache.hadoop.hdfs.DistributedFileSystem} class to communicate 
@@ -69,6 +69,9 @@
   public LocatedBlocks  getBlockLocations(String src,
                                           long offset,
                                           long length) throws IOException;
+  public LocatedBlocks  getBlockLocations(String src,
+          long offset,
+          long length, LabelSet secrecy, LabelSet integrity) throws IOException;
 
   /**
    * Create a new file entry in the namespace.
@@ -108,6 +111,14 @@
                              long blockSize
                              ) throws IOException;
 
+  public void createLabeled(String src, 
+		  		  FsPermission masked,
+                  String clientName, 
+                  boolean overwrite, 
+                  short replication,
+                  long blockSize, LabelSet secrecy, LabelSet integrity
+                  ) throws IOException;
+
   /**
    * Append to the end of the file. 
    * @param src path of the file being created.
@@ -211,7 +222,7 @@
    *                                any quota restriction
    */
   public boolean rename(String src, String dst) throws IOException;
-
+  public boolean rename(String src, String dst, LabelSet secrecy, LabelSet integrity) throws IOException;
   /**
    * Delete the given file or directory from the file system.
    * <p>
Index: src/hdfs/org/apache/hadoop/hdfs/HftpFileSystem.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/HftpFileSystem.java	(revision 142)
+++ src/hdfs/org/apache/hadoop/hdfs/HftpFileSystem.java	(working copy)
@@ -57,7 +57,7 @@
 import org.apache.hadoop.security.*;
 import org.apache.hadoop.util.Progressable;
 import org.apache.hadoop.util.StringUtils;
-
+import org.apache.hadoop.security.LabelSet;
 /** An implementation of a protocol for accessing filesystems over HTTP.
  * The following implementation provides a limited, read-only interface
  * to a filesystem over HTTP.
@@ -116,7 +116,7 @@
   }
 
   @Override
-  public FSDataInputStream open(Path f, int buffersize) throws IOException {
+  public FSDataInputStream open(Path f, int buffersize, LabelSet secrecy, LabelSet integrity) throws IOException {
     HttpURLConnection connection = null;
     connection = openConnection("/data" + f.toUri().getPath(), "ugi=" + ugi);
     connection.setRequestMethod("GET");
@@ -302,12 +302,12 @@
   public FSDataOutputStream create(Path f, FsPermission permission,
                                    boolean overwrite, int bufferSize,
                                    short replication, long blockSize,
-                                   Progressable progress) throws IOException {
+                                   Progressable progress, LabelSet secrecy, LabelSet integrity) throws IOException {
     throw new IOException("Not supported");
   }
 
   @Override
-  public boolean rename(Path src, Path dst) throws IOException {
+  public boolean rename(Path src, Path dst, LabelSet secrecy, LabelSet integrty) throws IOException {
     throw new IOException("Not supported");
   }
 
Index: src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	(revision 142)
+++ src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	(working copy)
@@ -38,6 +38,7 @@
 import org.apache.hadoop.hdfs.protocol.FSConstants;
 import org.apache.hadoop.hdfs.protocol.LocatedBlock;
 import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.Daemon;
 import org.apache.hadoop.util.DataChecksum;
 import org.apache.hadoop.util.StringUtils;
@@ -76,9 +77,18 @@
   private Checksum partialCrc = null;
   private DataNode datanode = null;
 
+  /*DIFC: added*/
+  LabelSet secrecy=LabelSet.EMPTY;
+  LabelSet integrity=LabelSet.EMPTY;
+  
   BlockReceiver(Block block, DataInputStream in, String inAddr,
+          String myAddr, boolean isRecovery, String clientName, 
+          DatanodeInfo srcDataNode, DataNode datanode) throws IOException {
+	  this(block,in,inAddr,myAddr,isRecovery,clientName,srcDataNode,datanode,LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  BlockReceiver(Block block, DataInputStream in, String inAddr,
                 String myAddr, boolean isRecovery, String clientName, 
-                DatanodeInfo srcDataNode, DataNode datanode) throws IOException {
+                DatanodeInfo srcDataNode, DataNode datanode, LabelSet secrecy, LabelSet integrity) throws IOException {
     try{
       this.block = block;
       this.in = in;
@@ -92,10 +102,12 @@
       this.checksumSize = checksum.getChecksumSize();
       this.srcDataNode = srcDataNode;
       this.datanode = datanode;
+      this.secrecy=secrecy;
+      this.integrity=integrity;
       //
       // Open local disk out
       //
-      streams = datanode.data.writeToBlock(block, isRecovery);
+      streams = datanode.data.writeToBlock(block, isRecovery, secrecy, integrity);
       this.finalized = datanode.data.isValidBlock(block);
       if (streams != null) {
         this.out = streams.dataOut;
@@ -499,7 +511,8 @@
     try {
       // write data chunk header
       if (!finalized) {
-        BlockMetadataHeader.writeHeader(checksumOut, checksum);
+    	/*DIFC: modified to add labels to the header file*/
+        BlockMetadataHeader.writeHeader(checksumOut, checksum, this.secrecy, this.integrity);
       }
       if (clientName.length() > 0) {
         responder = new Daemon(datanode.threadGroup, 
Index: src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java	(revision 142)
+++ src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java	(working copy)
@@ -253,7 +253,7 @@
     storage = new DataStorage();
     // construct registration
     this.dnRegistration = new DatanodeRegistration(machineName + ":" + tmpPort);
-
+    LOG.info("##datanode: my addr="+address);
     // connect to name node
     this.namenode = (DatanodeProtocol) 
       RPC.waitForProxy(DatanodeProtocol.class,
Index: src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	(revision 142)
+++ src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	(working copy)
@@ -38,6 +38,7 @@
 import org.apache.hadoop.io.MD5Hash;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.net.NetUtils;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.DataChecksum;
 import org.apache.hadoop.util.StringUtils;
 import static org.apache.hadoop.hdfs.server.datanode.DataNode.DN_CLIENTTRACE_FORMAT;
@@ -54,6 +55,9 @@
   final String localAddress;  // local address of this daemon
   DataNode datanode;
   DataXceiverServer dataXceiverServer;
+  /*DIFC: added*/
+  LabelSet secrecy=LabelSet.EMPTY;
+  LabelSet integrity=LabelSet.EMPTY;
   
   public DataXceiver(Socket s, DataNode datanode, 
       DataXceiverServer dataXceiverServer) {
@@ -152,6 +156,13 @@
     long startOffset = in.readLong();
     long length = in.readLong();
     String clientName = Text.readString(in);
+    /* DIFC : changed, according to the new protocol, we send 
+     * labels after the name of the client. So lets
+     * read the labels here.
+     */
+    this.secrecy=LabelSet.readFields(in);
+    this.integrity=LabelSet.readFields(in);
+    
     // send the block
     OutputStream baseStream = NetUtils.getOutputStream(s, 
         datanode.socketWriteTimeout);
@@ -169,7 +180,7 @@
     try {
       try {
         blockSender = new BlockSender(block, startOffset, length,
-            true, true, false, datanode, clientTraceFmt);
+            true, true, false, datanode, clientTraceFmt, this.secrecy, this.integrity);
       } catch(IOException e) {
         out.writeShort(DataTransferProtocol.OP_STATUS_ERROR);
         throw e;
@@ -245,7 +256,13 @@
       tmp.readFields(in);
       targets[i] = tmp;
     }
-
+    /* DIFC : changed, according to the new protocol, we send 
+     * labels after the targets but before the checksum. So lets
+     * read the labels here.
+     */
+    this.secrecy=LabelSet.readFields(in);
+    this.integrity=LabelSet.readFields(in);
+  
     DataOutputStream mirrorOut = null;  // stream to next target
     DataInputStream mirrorIn = null;    // reply from next target
     DataOutputStream replyOut = null;   // stream to prev target
@@ -255,10 +272,11 @@
     String firstBadLink = "";           // first datanode that failed in connection setup
     try {
       // open a block receiver and check if the block does not exist
+    	/*DIFC: changed to pass labels*/
       blockReceiver = new BlockReceiver(block, in, 
           s.getRemoteSocketAddress().toString(),
           s.getLocalSocketAddress().toString(),
-          isRecovery, client, srcDataNode, datanode);
+          isRecovery, client, srcDataNode, datanode, this.secrecy, this.integrity);
 
       // get a connection back to the previous target
       replyOut = new DataOutputStream(
@@ -303,7 +321,12 @@
           for ( int i = 1; i < targets.length; i++ ) {
             targets[i].write( mirrorOut );
           }
-
+          /*DIFC: protocol changed to allow passing of labels
+           * Note: we send labels before the checksum!!
+           * */
+          LabelSet.writeToStream(mirrorOut, this.secrecy);
+          LabelSet.writeToStream(mirrorOut, this.integrity);
+          
           blockReceiver.writeChecksumHeader(mirrorOut);
           mirrorOut.flush();
 
Index: src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java	(revision 142)
+++ src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java	(working copy)
@@ -23,10 +23,10 @@
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.OutputStream;
+import org.apache.hadoop.security.LabelSet;
 
 
 
-
 import org.apache.hadoop.hdfs.server.datanode.metrics.FSDatasetMBean;
 import org.apache.hadoop.hdfs.protocol.Block;
 import org.apache.hadoop.io.IOUtils;
@@ -175,7 +175,8 @@
    * @throws IOException
    */
   public BlockWriteStreams writeToBlock(Block b, boolean isRecovery) throws IOException;
-
+  /*DIFC: changed to pass labels*/
+  public BlockWriteStreams writeToBlock(Block b, boolean isRecovery, LabelSet secrecy, LabelSet integrity) throws IOException;
   /**
    * Update the block to the new generation stamp and length.  
    */
Index: src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockMetadataHeader.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockMetadataHeader.java	(revision 142)
+++ src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockMetadataHeader.java	(working copy)
@@ -25,6 +25,7 @@
 import java.io.IOException;
 
 import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.DataChecksum;
 
 
@@ -33,6 +34,9 @@
  * This is not related to the Block related functionality in Namenode.
  * The biggest part of data block metadata is CRC for the block.
  */
+/* DIFC: the header is now changed to include the secrecy and integrity
+ * labels at the end.
+ */
 class BlockMetadataHeader {
 
   static final short METADATA_VERSION = FSDataset.METADATA_VERSION;
@@ -44,12 +48,23 @@
    */
   private short version;
   private DataChecksum checksum = null;
-    
+  
+  /*DIFC:store the Laminar labels for this block*/
+  LabelSet secrecy=LabelSet.EMPTY;
+  LabelSet integrity=LabelSet.EMPTY;
+  
   BlockMetadataHeader(short version, DataChecksum checksum) {
     this.checksum = checksum;
     this.version = version;
   }
-    
+  
+  BlockMetadataHeader(short version, DataChecksum checksum, LabelSet secLabel, LabelSet intLabel) {
+	    this.checksum = checksum;
+	    this.version = version;
+	    this.secrecy=secLabel;
+	    this.integrity=intLabel;
+  }
+  
   short getVersion() {
     return version;
   }
@@ -58,6 +73,9 @@
     return checksum;
   }
 
+  /*DIFC: helper functions to access*/
+  LabelSet getSecrecyLabel(){return this.secrecy;}
+  LabelSet getIntegrityLabel(){return this.integrity;}
  
   /**
    * This reads all the fields till the beginning of checksum.
@@ -92,9 +110,18 @@
   private static BlockMetadataHeader readHeader(short version, DataInputStream in) 
                                    throws IOException {
     DataChecksum checksum = DataChecksum.newDataChecksum(in);
-    return new BlockMetadataHeader(version, checksum);
+    //return new BlockMetadataHeader(version, checksum);
+    return readHeader(version, checksum, in);
   }
   
+  /*DIFC: read the labels*/
+  private static BlockMetadataHeader readHeader(short version, DataChecksum checksum, DataInputStream in) 
+  throws IOException {
+	  LabelSet secSet=LabelSet.readFields(in);
+	  LabelSet intSet=LabelSet.readFields(in);
+	  return new BlockMetadataHeader(version, checksum, secSet, intSet);
+  }
+  
   /**
    * This writes all the fields till the beginning of checksum.
    * @param out DataOutputStream
@@ -107,6 +134,9 @@
                                   throws IOException {
     out.writeShort(header.getVersion());
     header.getChecksum().writeHeader(out);
+    /*DIFC:added*/
+    LabelSet.writeToStream(out, header.secrecy);
+    LabelSet.writeToStream(out, header.integrity);
   }
   
   /**
@@ -117,8 +147,14 @@
    */
   static void writeHeader(DataOutputStream out, DataChecksum checksum)
                          throws IOException {
-    writeHeader(out, new BlockMetadataHeader(METADATA_VERSION, checksum));
+   //DIFC TODO: this will end up with empty labels
+   writeHeader(out, new BlockMetadataHeader(METADATA_VERSION, checksum));
   }
+  /*DIFC: modified to take labels*/
+  static void writeHeader(DataOutputStream out, DataChecksum checksum, LabelSet secrecy, LabelSet integrity)
+  throws IOException {
+	  writeHeader(out, new BlockMetadataHeader(METADATA_VERSION, checksum, secrecy, integrity));
+  }
 
   /**
    * Returns the size of the header
Index: src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockSender.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockSender.java	(revision 142)
+++ src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockSender.java	(working copy)
@@ -37,7 +37,9 @@
 import org.apache.hadoop.net.SocketOutputStream;
 import org.apache.hadoop.util.DataChecksum;
 import org.apache.hadoop.util.StringUtils;
-
+import org.apache.hadoop.security.DIFC;
+import org.apache.hadoop.security.DIFCException;
+import org.apache.hadoop.security.LabelSet;
 /**
  * Reads a block from the disk and sends it to a recipient.
  */
@@ -81,8 +83,14 @@
   }
 
   BlockSender(Block block, long startOffset, long length,
+          boolean corruptChecksumOk, boolean chunkOffsetOK,
+          boolean verifyChecksum, DataNode datanode, String clientTraceFmt)
+  throws IOException {
+	  this(block,startOffset,length,corruptChecksumOk,chunkOffsetOK,verifyChecksum,datanode,clientTraceFmt, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  BlockSender(Block block, long startOffset, long length,
               boolean corruptChecksumOk, boolean chunkOffsetOK,
-              boolean verifyChecksum, DataNode datanode, String clientTraceFmt)
+              boolean verifyChecksum, DataNode datanode, String clientTraceFmt, LabelSet secrecy, LabelSet integrity)
       throws IOException {
     try {
       this.block = block;
@@ -101,7 +109,11 @@
         // read and handle the common header here. For now just a version
        BlockMetadataHeader header = BlockMetadataHeader.readHeader(checksumIn);
        short version = header.getVersion();
-
+       /*DIFC: check if client has read permission given the labels of the block*/
+       if(!DIFC.checkFileReadPermission(secrecy, integrity, header.getSecrecyLabel(), header.getIntegrityLabel())){
+    	   throw new DIFCException("Client does not have permission to read file from datanode: Principal=("+secrecy+","+integrity+"): File=("+header.getSecrecyLabel()+","+header.getIntegrityLabel()+")");
+       }
+       
         if (version != FSDataset.METADATA_VERSION) {
           LOG.warn("Wrong version (" + version + ") for metadata file for "
               + block + " ignoring ...");
Index: src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDataset.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDataset.java	(revision 142)
+++ src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDataset.java	(working copy)
@@ -28,6 +28,7 @@
 import org.apache.hadoop.hdfs.protocol.Block;
 import org.apache.hadoop.hdfs.protocol.FSConstants;
 import org.apache.hadoop.metrics.util.MBeanUtil;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.DataChecksum;
 import org.apache.hadoop.util.DiskChecker;
 import org.apache.hadoop.util.DiskChecker.DiskErrorException;
@@ -99,6 +100,7 @@
         File dest = new File(dir, b.getBlockName());
         File metaData = getMetaFile( src, b );
         File newmeta = getMetaFile(dest, b);
+        DataNode.LOG.info("## FSDataset: in add block: dest="+dest +":dir="+dir+":maxblocks="+maxBlocksPerDir);
         if ( ! metaData.renameTo( newmeta ) ||
             ! src.renameTo( dest ) ) {
           throw new IOException( "could not move files for " + b +
@@ -142,7 +144,7 @@
           children[idx] = new FSDir(new File(dir, DataStorage.BLOCK_SUBDIR_PREFIX+idx));
         }
       }
-            
+      DataNode.LOG.info("## FSDataset: add block: #children="+children.length);
       //now pick a child randomly for creating a new set of subdirs.
       lastChildIdx = random.nextInt(children.length);
       return children[ lastChildIdx ].addBlock(b, src, true, false); 
@@ -950,6 +952,9 @@
    * other threads that might be writing to this block, and then reopen the file.
    */
   public BlockWriteStreams writeToBlock(Block b, boolean isRecovery) throws IOException {
+	  return writeToBlock(b, isRecovery, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  public BlockWriteStreams writeToBlock(Block b, boolean isRecovery, LabelSet secrecy, LabelSet integrity) throws IOException {
     //
     // Make sure the block isn't a valid one - we're still creating it!
     //
@@ -996,6 +1001,7 @@
         // create temporary file to hold block in the designated volume
         f = createTmpFile(v, b);
         volumeMap.put(b, new DatanodeBlockInfo(v));
+        DataNode.LOG.info("## FSDataset: file="+f+":sec="+secrecy);
       } else if (f != null) {
         DataNode.LOG.info("Reopen already-open Block for append " + b);
         // create or reuse temporary file to hold block in the designated volume
@@ -1009,7 +1015,7 @@
         File blkfile = getBlockFile(b);
         File oldmeta = getMetaFile(b);
         File newmeta = getMetaFile(f, b);
-
+        DataNode.LOG.info("## fsdataset blkfile="+blkfile+":oldmeta="+oldmeta+":newmeta="+newmeta);
         // rename meta file to tmp directory
         DataNode.LOG.debug("Renaming " + oldmeta + " to " + newmeta);
         if (!oldmeta.renameTo(newmeta)) {
Index: src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeFileUnderConstruction.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeFileUnderConstruction.java	(revision 142)
+++ src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeFileUnderConstruction.java	(working copy)
@@ -22,8 +22,8 @@
 import org.apache.hadoop.fs.permission.PermissionStatus;
 import org.apache.hadoop.hdfs.protocol.Block;
 import org.apache.hadoop.hdfs.server.namenode.BlocksMap.BlockInfo;
+import org.apache.hadoop.security.LabelSet;
 
-
 public class INodeFileUnderConstruction extends INodeFile {
   StringBytesWritable clientName = null;         // lease holder
   StringBytesWritable clientMachine = null;
@@ -68,6 +68,23 @@
     this.clientNode = clientNode;
   }
 
+  public INodeFileUnderConstruction(byte[] name,
+		  short blockReplication,
+		  long modificationTime,
+		  long preferredBlockSize,
+		  BlockInfo[] blocks,
+		  PermissionStatus perm,
+		  String clientName,
+		  String clientMachine,
+		  DatanodeDescriptor clientNode, LabelSet secrecy, LabelSet integrity)
+  throws IOException {
+	  super(perm, blocks, blockReplication, modificationTime, modificationTime,
+			  preferredBlockSize,secrecy, integrity);
+	  setLocalName(name);
+	  this.clientName = new StringBytesWritable(clientName);
+	  this.clientMachine = new StringBytesWritable(clientMachine);
+	  this.clientNode = clientNode;
+  }
   String getClientName() throws IOException {
     return clientName.getString();
   }
@@ -102,12 +119,13 @@
   // use the modification time as the access time
   //
   INodeFile convertToInodeFile() {
+	NameNode.LOG.info("inodefileunderconst ##called convert inode sec="+getSecrecyLabel());
     INodeFile obj = new INodeFile(getPermissionStatus(),
                                   getBlocks(),
                                   getReplication(),
                                   getModificationTime(),
                                   getModificationTime(),
-                                  getPreferredBlockSize());
+                                  getPreferredBlockSize(), getSecrecyLabel(), getIntegrityLabel());
     return obj;
     
   }
Index: src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java	(revision 142)
+++ src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java	(working copy)
@@ -43,6 +43,7 @@
 import org.apache.hadoop.util.StringUtils;
 import org.apache.hadoop.net.NetUtils;
 import org.apache.hadoop.net.NetworkTopology;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.security.UserGroupInformation;
 
 import java.io.*;
@@ -266,11 +267,17 @@
   /////////////////////////////////////////////////////
   /** {@inheritDoc} */
   public LocatedBlocks   getBlockLocations(String src, 
+          long offset, 
+          long length) throws IOException {
+	  return getBlockLocations(src,offset,length, LabelSet.EMPTY,LabelSet.EMPTY);
+}
+  
+  public LocatedBlocks   getBlockLocations(String src, 
                                           long offset, 
-                                          long length) throws IOException {
+                                          long length, LabelSet secrecy, LabelSet integrity) throws IOException {
     myMetrics.numGetBlockLocations.inc();
     return namesystem.getBlockLocations(getClientMachine(), 
-                                        src, offset, length);
+                                        src, offset, length, secrecy,integrity);
   }
   
   private static String getClientMachine() {
@@ -282,14 +289,25 @@
   }
 
   /** {@inheritDoc} */
+  
   public void create(String src, 
+          FsPermission masked,
+                  String clientName, 
+                  boolean overwrite,
+                  short replication,
+                  long blockSize
+                  ) throws IOException {
+	  createLabeled(src, masked,clientName,overwrite,replication,blockSize,LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  
+  public void createLabeled(String src, 
                      FsPermission masked,
                              String clientName, 
                              boolean overwrite,
                              short replication,
-                             long blockSize
+                             long blockSize, LabelSet secrecy, LabelSet integrity
                              ) throws IOException {
-    String clientMachine = getClientMachine();
+	String clientMachine = getClientMachine();
     if (stateChangeLog.isDebugEnabled()) {
       stateChangeLog.debug("*DIR* NameNode.create: file "
                          +src+" for "+clientName+" at "+clientMachine);
@@ -298,10 +316,10 @@
       throw new IOException("create: Pathname too long.  Limit " 
                             + MAX_PATH_LENGTH + " characters, " + MAX_PATH_DEPTH + " levels.");
     }
-    namesystem.startFile(src,
+    namesystem.startFileLabeled(src,
         new PermissionStatus(UserGroupInformation.getCurrentUGI().getUserName(),
             null, masked),
-        clientName, clientMachine, overwrite, replication, blockSize);
+        clientName, clientMachine, overwrite, replication, blockSize, secrecy, integrity);
     myMetrics.numFilesCreated.inc();
     myMetrics.numCreateFileOps.inc();
   }
@@ -417,13 +435,16 @@
     
   /**
    */
-  public boolean rename(String src, String dst) throws IOException {
+  public boolean rename(String src, String dst) throws IOException{
+	  return rename(src,dst,LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  public boolean rename(String src, String dst, LabelSet secrecy, LabelSet integrity) throws IOException {
     stateChangeLog.debug("*DIR* NameNode.rename: " + src + " to " + dst);
     if (!checkPathLength(dst)) {
       throw new IOException("rename: Pathname too long.  Limit " 
                             + MAX_PATH_LENGTH + " characters, " + MAX_PATH_DEPTH + " levels.");
     }
-    boolean ret = namesystem.renameTo(src, dst);
+    boolean ret = namesystem.renameTo(src, dst, secrecy, integrity);
     if (ret) {
       myMetrics.numFilesRenamed.inc();
     }
@@ -668,7 +689,7 @@
   }
     
   public NamespaceInfo versionRequest() throws IOException {
-    return namesystem.getNamespaceInfo();
+	  return namesystem.getNamespaceInfo();
   }
 
   public UpgradeCommand processUpgradeCommand(UpgradeCommand comm) throws IOException {
Index: src/hdfs/org/apache/hadoop/hdfs/server/namenode/BlocksMap.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/server/namenode/BlocksMap.java	(revision 142)
+++ src/hdfs/org/apache/hadoop/hdfs/server/namenode/BlocksMap.java	(working copy)
@@ -20,7 +20,7 @@
 import java.util.*;
 
 import org.apache.hadoop.hdfs.protocol.Block;
-
+import org.apache.hadoop.security.LabelSet;
 /**
  * This class maintains the map from a block to its metadata.
  * block's metadata currently includes INode it belongs to and
@@ -43,13 +43,32 @@
      * list of blocks belonging to this data-node.
      */
     private Object[] triplets;
-
+    /*DIFC: store Laminar specific label(i.e. local OS label) of each block
+     * on each datanode (a block may be replicated on multiple nodes)
+     *  We store both secrecy([0]) and integrity label([1]), hence 2D array
+     */
+    private LabelSet[][] blockNodeLabel; 
+    private final int SECRECY=0,INTEGRITY=1;
+    
     public BlockInfo(Block blk, int replication) {
       super(blk);
       this.triplets = new Object[3*replication];
       this.inode = null;
+      this.blockNodeLabel=new LabelSet[replication][2];
     }
 
+    /*DIFC: functions to set and access local OS labels for the blocks
+     * Index determines the datanode where we replicate.
+     * */
+    
+    public LabelSet[] getBlockLabels(int index){return blockNodeLabel[index];}
+    public void setBlockLabels(int index, LabelSet secrecy, LabelSet integrity){
+    	blockNodeLabel[index][SECRECY]=secrecy;
+    	blockNodeLabel[index][INTEGRITY]=integrity;
+    }
+    public LabelSet getSecrecyLabelOfBlock(int index){return blockNodeLabel[index][SECRECY];} 
+    public LabelSet getIntegrityLabelOfBlock(int index){return blockNodeLabel[index][INTEGRITY];}
+    
     INodeFile getINode() {
       return inode;
     }
@@ -124,6 +143,15 @@
       for(int i=0; i < last*3; i++) {
         triplets[i] = old[i];
       }
+      
+      /*DIFC: perform expansion for labels as well*/
+      LabelSet[][] oldLabel=blockNodeLabel;
+      blockNodeLabel=new LabelSet[last+num][2];
+      for(int i=0;i<last;i++){
+    	  blockNodeLabel[i][SECRECY]=oldLabel[i][SECRECY];
+    	  blockNodeLabel[i][INTEGRITY]=oldLabel[i][INTEGRITY];
+      }
+    	  
       return last;
     }
 
@@ -153,7 +181,34 @@
       setPrevious(lastNode, null);
       return true;
     }
+    
+    /*DIFC: version where node and its label are added simultaneously*/
+    boolean addNodeWithLabel(DatanodeDescriptor node, LabelSet secrecy, LabelSet integrity ){
+    	if(findDatanode(node) >= 0) // the node is already there
+            return false;
+          // find the last null node
+          int lastNode = ensureCapacity(1);
+          setDatanode(lastNode, node);
+          setNext(lastNode, null);
+          setPrevious(lastNode, null);
+          blockNodeLabel[lastNode][SECRECY]=secrecy;
+          blockNodeLabel[lastNode][INTEGRITY]=integrity;
+          return true;
+    }
 
+    /*DIFC: sets the label corresponding to the datanode.
+     * Avoid using this function, use the above one.
+     * Beware: this assumes that the datanode exists, also it may overwrite
+     * the previous labels!
+     */
+    boolean addBlockLabel(LabelSet secrecy, LabelSet integrity, DatanodeDescriptor node){
+    	int index=findDatanode(node);
+    	if(index<0) return false;
+    	blockNodeLabel[index][SECRECY]=secrecy;
+        blockNodeLabel[index][INTEGRITY]=integrity;
+        return true;
+    }
+    
     /**
      * Remove data-node from the block.
      */
@@ -173,6 +228,13 @@
       setDatanode(lastNode, null);
       setNext(lastNode, null); 
       setPrevious(lastNode, null); 
+      
+      /*DIFC: make corresponding changes to the label*/
+      blockNodeLabel[dnIndex][SECRECY]=blockNodeLabel[lastNode][SECRECY];
+      blockNodeLabel[dnIndex][INTEGRITY]=blockNodeLabel[lastNode][INTEGRITY];
+      blockNodeLabel[lastNode][SECRECY]=LabelSet.EMPTY;
+      blockNodeLabel[lastNode][INTEGRITY]=LabelSet.EMPTY;
+      
       return true;
     }
 
Index: src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	(revision 142)
+++ src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	(working copy)
@@ -30,6 +30,8 @@
 import org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean;
 import org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMetrics;
 import org.apache.hadoop.security.AccessControlException;
+import org.apache.hadoop.security.DIFC;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.security.UnixUserGroupInformation;
 import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.hadoop.util.*;
@@ -71,7 +73,7 @@
 import javax.management.ObjectName;
 import javax.management.StandardMBean;
 import javax.security.auth.login.LoginException;
-
+import org.apache.hadoop.security.DIFCException;;
 /***************************************************
  * FSNamesystem does the actual bookkeeping work for the
  * DataNode.
@@ -740,12 +742,18 @@
    * @see #getBlockLocations(String, long, long)
    */
   LocatedBlocks getBlockLocations(String clientMachine, String src,
-      long offset, long length) throws IOException {
+	      long offset, long length) throws IOException {
+	return getBlockLocations(clientMachine,src,offset,length, LabelSet.EMPTY,LabelSet.EMPTY);  
+  }
+	  
+  
+  LocatedBlocks getBlockLocations(String clientMachine, String src,
+      long offset, long length, LabelSet secrecy, LabelSet integrity) throws IOException {
     if (isPermissionEnabled) {
       checkPathAccess(src, FsAction.READ);
     }
 
-    LocatedBlocks blocks = getBlockLocations(src, offset, length, true);
+    LocatedBlocks blocks = getBlockLocations(src, offset, length, true, secrecy, integrity);
     if (blocks != null) {
       //sort the blocks
       DatanodeDescriptor client = host2DataNodeMap.getDatanodeByHost(
@@ -762,8 +770,12 @@
    * @see ClientProtocol#getBlockLocations(String, long, long)
    */
   public LocatedBlocks getBlockLocations(String src, long offset, long length
+  ) throws IOException {
+	  return getBlockLocations(src,offset,length,LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  public LocatedBlocks getBlockLocations(String src, long offset, long length, LabelSet secrecy, LabelSet integrity
       ) throws IOException {
-    return getBlockLocations(src, offset, length, false);
+    return getBlockLocations(src, offset, length, false, secrecy,integrity);
   }
 
   /**
@@ -771,7 +783,7 @@
    * @see ClientProtocol#getBlockLocations(String, long, long)
    */
   public LocatedBlocks getBlockLocations(String src, long offset, long length,
-      boolean doAccessTime) throws IOException {
+      boolean doAccessTime, LabelSet secrecy, LabelSet integrity) throws IOException {
     if (offset < 0) {
       throw new IOException("Negative offset is not supported. File: " + src );
     }
@@ -779,7 +791,7 @@
       throw new IOException("Negative length is not supported. File: " + src );
     }
     final LocatedBlocks ret = getBlockLocationsInternal(src, dir.getFileINode(src),
-        offset, length, Integer.MAX_VALUE, doAccessTime);  
+        offset, length, Integer.MAX_VALUE, doAccessTime, secrecy , integrity);  
     if (auditLog.isInfoEnabled()) {
       logAuditEvent(UserGroupInformation.getCurrentUGI(),
                     Server.getRemoteIp(),
@@ -793,11 +805,15 @@
                                                        long offset, 
                                                        long length,
                                                        int nrBlocksToReturn,
-                                                       boolean doAccessTime) 
+                                                       boolean doAccessTime, LabelSet curSecrecy, LabelSet curIntegrity) 
                                                        throws IOException {
     if(inode == null) {
       return null;
     }
+    LOG.info("###fsnamesystem: principal=("+curSecrecy+","+curIntegrity+"): File=("+inode.getSecrecyLabel()+","+inode.getIntegrityLabel()+"): inode="+inode+":parent="+inode.getParent()+":parent sec="+inode.getParent().getSecrecyLabel());
+    /*DIFC: make check on whether access is allowed given the inode permissions*/
+    if(!DIFC.checkFileReadPermission(curSecrecy, curIntegrity, inode.getSecrecyLabel(),inode.getIntegrityLabel()))
+    	throw new DIFCException("Principal does not have the permission to read file. Principal=("+curSecrecy+","+curIntegrity+"): File=("+inode.getSecrecyLabel()+","+inode.getIntegrityLabel()+")");
     if (doAccessTime & isAccessTimeSupported()) {
       dir.setTimes(src, inode, -1, now(), false);
     }
@@ -989,11 +1005,17 @@
    *         {@link FSDirectory#isValidToCreate(String)}.
    */
   void startFile(String src, PermissionStatus permissions,
+          String holder, String clientMachine,
+          boolean overwrite, short replication, long blockSize
+         ) throws IOException {
+	  startFileLabeled(src,permissions,holder,clientMachine,overwrite,replication,blockSize,LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  void startFileLabeled(String src, PermissionStatus permissions,
                  String holder, String clientMachine,
-                 boolean overwrite, short replication, long blockSize
+                 boolean overwrite, short replication, long blockSize, LabelSet secrecy, LabelSet integrity
                 ) throws IOException {
-    startFileInternal(src, permissions, holder, clientMachine, overwrite, false,
-                      replication, blockSize);
+	startFileInternalLabeled(src, permissions, holder, clientMachine, overwrite, false,
+                      replication, blockSize, secrecy, integrity);
     getEditLog().logSync();
     if (auditLog.isInfoEnabled()) {
       final FileStatus stat = dir.getFileInfo(src);
@@ -1004,13 +1026,24 @@
   }
 
   private synchronized void startFileInternal(String src,
+          PermissionStatus permissions,
+          String holder, 
+          String clientMachine, 
+          boolean overwrite,
+          boolean append,
+          short replication,
+          long blockSize
+          ) throws IOException {
+	startFileInternalLabeled(src,permissions, holder,clientMachine,overwrite,append,replication,blockSize,LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  private synchronized void startFileInternalLabeled(String src,
                                               PermissionStatus permissions,
                                               String holder, 
                                               String clientMachine, 
                                               boolean overwrite,
                                               boolean append,
                                               short replication,
-                                              long blockSize
+                                              long blockSize, LabelSet secrecy , LabelSet integrity
                                               ) throws IOException {
     if (NameNode.stateChangeLog.isDebugEnabled()) {
       NameNode.stateChangeLog.debug("DIR* NameSystem.startFile: src=" + src
@@ -1034,7 +1067,7 @@
         checkAncestorAccess(src, FsAction.WRITE);
       }
     }
-
+    System.out.println("FSNamesystem  file system internal");
     try {
       INode myFile = dir.getFileINode(src);
       if (myFile != null && myFile.isUnderConstruction()) {
@@ -1092,6 +1125,7 @@
           throw new IOException("failed to append to directory " + src 
                                 +" on client " + clientMachine);
         }
+        /*DIFC : TODO: perform label checks in isValidToCreate() as well?*/
       } else if (!dir.isValidToCreate(src)) {
         if (overwrite) {
           delete(src, true);
@@ -1129,11 +1163,10 @@
        // blocks associated with it.
        //
        checkFsObjectLimit();
-
         // increment global generation stamp
         long genstamp = nextGenerationStamp();
-        INodeFileUnderConstruction newNode = dir.addFile(src, permissions,
-            replication, blockSize, holder, clientMachine, clientNode, genstamp);
+        INodeFileUnderConstruction newNode = dir.addFileLabeled(src, permissions,
+            replication, blockSize, holder, clientMachine, clientNode, genstamp, secrecy, integrity);
         if (newNode == null) {
           throw new IOException("DIR* NameSystem.startFile: " +
                                 "Unable to add file to namespace.");
@@ -1248,7 +1281,6 @@
       checkFsObjectLimit();
 
       INodeFileUnderConstruction pendingFile  = checkLease(src, clientName);
-
       //
       // If we fail this, bad things happen!
       //
@@ -1622,7 +1654,10 @@
 
   /** Change the indicated filename. */
   public boolean renameTo(String src, String dst) throws IOException {
-    boolean status = renameToInternal(src, dst);
+	  return renameTo(src,dst,LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  public boolean renameTo(String src, String dst, LabelSet secrecy, LabelSet integrity) throws IOException {
+    boolean status = renameToInternal(src, dst, secrecy, integrity);
     getEditLog().logSync();
     if (status && auditLog.isInfoEnabled()) {
       final FileStatus stat = dir.getFileInfo(dst);
@@ -1633,7 +1668,7 @@
     return status;
   }
 
-  private synchronized boolean renameToInternal(String src, String dst
+  private synchronized boolean renameToInternal(String src, String dst, LabelSet secrecy, LabelSet integrity
       ) throws IOException {
     NameNode.stateChangeLog.debug("DIR* NameSystem.renameTo: " + src + " to " + dst);
     if (isInSafeMode())
@@ -1641,7 +1676,7 @@
     if (!DFSUtil.isValidName(dst)) {
       throw new IOException("Invalid name: " + dst);
     }
-
+   
     if (isPermissionEnabled) {
       //We should not be doing this.  This is move() not renameTo().
       //but for now,
@@ -1652,7 +1687,7 @@
     }
 
     FileStatus dinfo = dir.getFileInfo(dst);
-    if (dir.renameTo(src, dst)) {
+    if (dir.renameTo(src, dst, secrecy, integrity)) {
       changeLease(src, dst, dinfo);     // update lease with new filename
       return true;
     }
@@ -1878,7 +1913,6 @@
     // Create permanent INode, update blockmap
     INodeFile newFile = pendingFile.convertToInodeFile();
     dir.replaceNode(src, pendingFile, newFile);
-
     // close file and persist block allocations for this file
     dir.closeFile(src, newFile);
 
Index: src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java	(revision 142)
+++ src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java	(working copy)
@@ -996,6 +996,7 @@
       FSEditLog.toLogLong(newNode.getModificationTime()),
       FSEditLog.toLogLong(newNode.getAccessTime()),
       FSEditLog.toLogLong(newNode.getPreferredBlockSize())};
+    /*DIFC: TODO need to write out the labels as well*/
     logEdit(OP_CLOSE,
             new ArrayWritable(UTF8.class, nameReplicationPair),
             new ArrayWritable(Block.class, newNode.getBlocks()),
Index: src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java	(revision 142)
+++ src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java	(working copy)
@@ -26,7 +26,9 @@
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.fs.permission.PermissionStatus;
 import org.apache.hadoop.hdfs.protocol.Block;
-
+import org.apache.hadoop.security.LabelSet;
+import org.apache.hadoop.security.DIFC;
+import org.apache.hadoop.security.DIFCException;
 /**
  * Directory INode class.
  */
@@ -211,7 +213,17 @@
    *          node, otherwise
    */
   <T extends INode> T addChild(final T node, boolean inheritPermission) {
-    if (inheritPermission) {
+	  return addChildLabeled(node, inheritPermission, LabelSet.EMPTY, LabelSet.EMPTY);
+  }
+  <T extends INode> T addChildLabeled(final T node, boolean inheritPermission, LabelSet secrecy, LabelSet integrity) {
+	  /*DIFC: check that the labels of the new file conform to those of directory*/
+	  if(!DIFC.checkFileCreation(this.secLabel, this.intLabel, secrecy,integrity)){
+		  throw new DIFCException("Labels of file do not conform to labels of parent directory: Dir Label=("+this.secLabel+","+this.intLabel+"):File label=("+secrecy+","+integrity+")");
+	  }
+	  node.setSecrecyLabel(secrecy);
+	  node.setIntegrityLabel(integrity);
+	  
+	  if (inheritPermission) {
       FsPermission p = getFsPermission();
       //make sure the  permission has wx for the user
       if (!p.getUserAction().implies(FsAction.WRITE_EXECUTE)) {
@@ -296,8 +308,9 @@
       parent = (INodeDirectory)inode;
     }
     // insert into the parent children list
-    newNode.name = pathComponents[pathLen-1];
-    if(parent.addChild(newNode, inheritPermission) == null)
+    newNode.name = pathComponents[pathLen-1]; 
+    
+    if(parent.addChildLabeled(newNode, inheritPermission, newNode.getSecrecyLabel(),newNode.getIntegrityLabel()) == null)
       return null;
     return parent;
   }
Index: src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java	(revision 142)
+++ src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java	(working copy)
@@ -28,12 +28,14 @@
 import org.apache.hadoop.metrics.MetricsRecord;
 import org.apache.hadoop.metrics.MetricsUtil;
 import org.apache.hadoop.metrics.MetricsContext;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.hdfs.protocol.FSConstants;
 import org.apache.hadoop.hdfs.protocol.Block;
 import org.apache.hadoop.hdfs.protocol.QuotaExceededException;
 import org.apache.hadoop.hdfs.server.common.HdfsConstants.StartupOption;
 import org.apache.hadoop.hdfs.server.namenode.BlocksMap.BlockInfo;
-
+import org.apache.hadoop.security.DIFC;
+import org.apache.hadoop.security.DIFCException;
 /*************************************************
  * FSDirectory stores the filesystem directory state.
  * It handles writing/loading values to disk, and logging
@@ -134,13 +136,24 @@
    * Add the given filename to the fs.
    */
   INodeFileUnderConstruction addFile(String path, 
+          PermissionStatus permissions,
+          short replication,
+          long preferredBlockSize,
+          String clientName,
+          String clientMachine,
+          DatanodeDescriptor clientNode,
+          long generationStamp) 
+          throws IOException {
+	  return addFileLabeled(path,permissions, replication,preferredBlockSize, clientName, clientMachine,clientNode,generationStamp, LabelSet.EMPTY, LabelSet.EMPTY);
+  }
+  INodeFileUnderConstruction addFileLabeled(String path, 
                 PermissionStatus permissions,
                 short replication,
                 long preferredBlockSize,
                 String clientName,
                 String clientMachine,
                 DatanodeDescriptor clientNode,
-                long generationStamp) 
+                long generationStamp, LabelSet secrecy, LabelSet integrity) 
                 throws IOException {
     waitForReady();
 
@@ -155,7 +168,7 @@
                                  preferredBlockSize, modTime, clientName, 
                                  clientMachine, clientNode);
     synchronized (rootDir) {
-      newNode = addNode(path, newNode, -1, false);
+      newNode = addNodeLabeled(path, newNode, -1, false, secrecy, integrity);
     }
     if (newNode == null) {
       NameNode.stateChangeLog.info("DIR* FSDirectory.addFile: "
@@ -336,13 +349,17 @@
    * @see #unprotectedRenameTo(String, String, long)
    */
   boolean renameTo(String src, String dst) throws QuotaExceededException {
+	  return renameTo(src,dst,LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+	  
+  boolean renameTo(String src, String dst, LabelSet secrecy, LabelSet integrity) throws QuotaExceededException {
     if (NameNode.stateChangeLog.isDebugEnabled()) {
       NameNode.stateChangeLog.debug("DIR* FSDirectory.renameTo: "
                                   +src+" to "+dst);
     }
     waitForReady();
     long now = FSNamesystem.now();
-    if (!unprotectedRenameTo(src, dst, now))
+    if (!unprotectedRenameTo(src, dst, now, secrecy, integrity))
       return false;
     fsImage.getEditLog().logRename(src, dst, now);
     return true;
@@ -357,7 +374,14 @@
    */
   boolean unprotectedRenameTo(String src, String dst, long timestamp) 
   throws QuotaExceededException {
+	  return unprotectedRenameTo(src,dst,timestamp,LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  boolean unprotectedRenameTo(String src, String dst, long timestamp, LabelSet userSecrecy, LabelSet userIntegrity) 
+  throws QuotaExceededException {
     synchronized (rootDir) {
+    	/*DIFC: check if the rename is allowed*/
+    	if(!DIFC.checkRename(src, dst, userSecrecy, userIntegrity))
+    		throw new DIFCException("Relabeling not allowed of file from ="+src+" to dest. ="+dst);
       INode[] srcInodes = rootDir.getExistingPathINodes(src);
 
       // check the validation of the source
@@ -410,7 +434,8 @@
         srcChild.setLocalName(dstComponents[dstInodes.length-1]);
         try {
           // add it to the namespace
-          dstChild = addChild(dstInodes, dstInodes.length-1, srcChild, false);
+        	/*DIFC: use the labels of the source.*/
+          dstChild = addChild(dstInodes, dstInodes.length-1, srcChild, false, srcChild.getSecrecyLabel(),srcChild.getIntegrityLabel());
         } catch (QuotaExceededException qe) {
           failureByQuota = qe;
         }
@@ -653,7 +678,6 @@
                            boolean updateDiskspace) throws IOException {    
     synchronized (rootDir) {
       long dsOld = oldnode.diskspaceConsumed();
-      
       //
       // Remove the node from the namespace 
       //
@@ -972,7 +996,13 @@
    * childDiskspace should be -1, if unknown. 
    * QuotaExceededException is thrown if it violates quota limit */
   private <T extends INode> T addNode(String src, T child, 
-        long childDiskspace, boolean inheritPermission) 
+	        long childDiskspace, boolean inheritPermission) 
+	  throws QuotaExceededException {
+	  return addNodeLabeled(src,child,childDiskspace,inheritPermission,LabelSet.EMPTY, LabelSet.EMPTY);
+  }
+  /*DIFC: changed*/
+  private <T extends INode> T addNodeLabeled(String src, T child, 
+        long childDiskspace, boolean inheritPermission, LabelSet secrecy, LabelSet integrity) 
   throws QuotaExceededException {
     byte[][] components = INode.getPathComponents(src);
     child.setLocalName(components[components.length-1]);
@@ -980,7 +1010,7 @@
     synchronized (rootDir) {
       rootDir.getExistingPathINodes(components, inodes);
       return addChild(inodes, inodes.length-1, child, childDiskspace,
-                      inheritPermission);
+                      inheritPermission, secrecy, integrity);
     }
   }
   
@@ -988,23 +1018,31 @@
    * Its ancestors are stored at [0, pos-1]. 
    * QuotaExceededException is thrown if it violates quota limit */
   private <T extends INode> T addChild(INode[] pathComponents, int pos, T child,
-      boolean inheritPermission) throws QuotaExceededException {
-    return addChild(pathComponents, pos, child, -1, inheritPermission);
+	      boolean inheritPermission) throws QuotaExceededException {
+   return addChild(pathComponents,pos,child,inheritPermission, LabelSet.EMPTY, LabelSet.EMPTY);
   }
+  private <T extends INode> T addChild(INode[] pathComponents, int pos, T child,
+      boolean inheritPermission, LabelSet secrecy, LabelSet integrity) throws QuotaExceededException {
+    return addChild(pathComponents, pos, child, -1, inheritPermission, secrecy, integrity);
+  }
   
   /** Add a node child to the inodes at index pos. 
    * Its ancestors are stored at [0, pos-1]. 
    * QuotaExceededException is thrown if it violates quota limit */
   private <T extends INode> T addChild(INode[] pathComponents, int pos, T child,
-       long childDiskspace, boolean inheritPermission) throws QuotaExceededException {
+	       long childDiskspace, boolean inheritPermission) throws QuotaExceededException {
+	  return addChild(pathComponents, pos, child, childDiskspace,inheritPermission, LabelSet.EMPTY, LabelSet.EMPTY);
+  }
+  private <T extends INode> T addChild(INode[] pathComponents, int pos, T child,
+       long childDiskspace, boolean inheritPermission, LabelSet secrecy, LabelSet integrity) throws QuotaExceededException {
     INode.DirCounts counts = new INode.DirCounts();
     child.spaceConsumedInTree(counts);
     if (childDiskspace < 0) {
       childDiskspace = counts.getDsCount();
     }
     updateCount(pathComponents, pos, counts.getNsCount(), childDiskspace);
-    T addedNode = ((INodeDirectory)pathComponents[pos-1]).addChild(
-        child, inheritPermission);
+    T addedNode = ((INodeDirectory)pathComponents[pos-1]).addChildLabeled(
+        child, inheritPermission, secrecy, integrity);
     if (addedNode == null) {
       updateCount(pathComponents, pos, 
                   -counts.getNsCount(), -childDiskspace);
Index: src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeFile.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeFile.java	(revision 142)
+++ src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeFile.java	(working copy)
@@ -25,7 +25,7 @@
 import org.apache.hadoop.fs.permission.PermissionStatus;
 import org.apache.hadoop.hdfs.protocol.Block;
 import org.apache.hadoop.hdfs.server.namenode.BlocksMap.BlockInfo;
-
+import org.apache.hadoop.security.LabelSet;
 public class INodeFile extends INode {
   static final FsPermission UMASK = FsPermission.createImmutable((short)0111);
 
@@ -55,6 +55,15 @@
     blocks = blklist;
   }
 
+  protected INodeFile(PermissionStatus permissions, BlockInfo[] blklist,
+		  short replication, long modificationTime,
+		  long atime, long preferredBlockSize, LabelSet secSet, LabelSet intSet) {
+	  super(permissions, modificationTime, atime, secSet, intSet);
+	  this.blockReplication = replication;
+	  this.preferredBlockSize = preferredBlockSize;
+	  blocks = blklist;
+  }
+  
   /**
    * Set the {@link FsPermission} of this {@link INodeFile}.
    * Since this is a file,
@@ -191,6 +200,6 @@
     return new INodeFileUnderConstruction(name,
         blockReplication, modificationTime, preferredBlockSize,
         blocks, getPermissionStatus(),
-        clientName, clientMachine, clientNode);
+        clientName, clientMachine, clientNode, getSecrecyLabel(),getIntegrityLabel());
   }
 }
Index: src/hdfs/org/apache/hadoop/hdfs/server/namenode/INode.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/server/namenode/INode.java	(revision 142)
+++ src/hdfs/org/apache/hadoop/hdfs/server/namenode/INode.java	(working copy)
@@ -27,7 +27,7 @@
 import org.apache.hadoop.hdfs.protocol.Block;
 import org.apache.hadoop.hdfs.protocol.LocatedBlock;
 import org.apache.hadoop.hdfs.protocol.LocatedBlocks;
-
+import org.apache.hadoop.security.LabelSet;
 /**
  * We keep an in-memory representation of the file/block hierarchy.
  * This is a base INode class containing common fields for file and 
@@ -39,6 +39,15 @@
   protected long modificationTime;
   protected long accessTime;
 
+  /*DIFC: secrecy and integrity labels associated with inodes*/
+  LabelSet secLabel=LabelSet.EMPTY;
+  LabelSet intLabel=LabelSet.EMPTY;
+  
+  public void setSecrecyLabel(LabelSet l){secLabel=l;}
+  public void setIntegrityLabel(LabelSet l){intLabel=l;}
+  public LabelSet getSecrecyLabel(){return secLabel;}
+  public LabelSet getIntegrityLabel(){return intLabel;}
+  
   /** Simple wrapper for two counters : 
    *  nsCount (namespace consumed) and dsCount (diskspace consumed).
    */
@@ -99,6 +108,17 @@
     setPermissionStatus(permissions);
   }
 
+  INode(PermissionStatus permissions, long mTime, long atime, LabelSet secSet, LabelSet intSet) {
+	  this.name = null;
+	  this.parent = null;
+	  this.modificationTime = mTime;
+	  setAccessTime(atime);
+	  setPermissionStatus(permissions);
+	  this.secLabel=secSet;
+	  this.intLabel=intSet;
+	  NameNode.LOG.info("## Inode: set sec="+secSet+": now seclabel="+this.secLabel);
+  }
+
   protected INode(String name, PermissionStatus permissions) {
     this(permissions, 0L, 0L);
     setLocalName(name);
@@ -114,6 +134,8 @@
     setPermissionStatus(other.getPermissionStatus());
     setModificationTime(other.getModificationTime());
     setAccessTime(other.getAccessTime());
+    setSecrecyLabel(other.getSecrecyLabel());
+    setIntegrityLabel(other.getIntegrityLabel());
   }
 
   /**
Index: src/hdfs/org/apache/hadoop/hdfs/DistributedFileSystem.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/DistributedFileSystem.java	(revision 142)
+++ src/hdfs/org/apache/hadoop/hdfs/DistributedFileSystem.java	(working copy)
@@ -32,6 +32,7 @@
 import org.apache.hadoop.hdfs.protocol.FSConstants.UpgradeAction;
 import org.apache.hadoop.hdfs.server.common.UpgradeStatusReport;
 import org.apache.hadoop.hdfs.server.namenode.NameNode;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.*;
 
 
@@ -135,13 +136,16 @@
     return result;
   }
   
-
+  public BlockLocation[] getFileBlockLocations(FileStatus file, 
+		  long start, long len) throws IOException {
+	  return getFileBlockLocations(file,start,len,LabelSet.EMPTY,LabelSet.EMPTY);
+	  }
   public BlockLocation[] getFileBlockLocations(FileStatus file, long start,
-      long len) throws IOException {
+      long len, LabelSet secrecy, LabelSet integrity) throws IOException {
     if (file == null) {
       return null;
     }
-    return dfs.getBlockLocations(getPathName(file.getPath()), start, len);
+    return dfs.getBlockLocations(getPathName(file.getPath()), start, len, secrecy, integrity);
   }
 
   public void setVerifyChecksum(boolean verifyChecksum) {
@@ -149,8 +153,11 @@
   }
 
   public FSDataInputStream open(Path f, int bufferSize) throws IOException {
+	  return open(f,bufferSize, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  public FSDataInputStream open(Path f, int bufferSize, LabelSet secrecy, LabelSet integrity) throws IOException {
     return new DFSClient.DFSDataInputStream(
-          dfs.open(getPathName(f), bufferSize, verifyChecksum, statistics));
+          dfs.open(getPathName(f), bufferSize, verifyChecksum, statistics,secrecy,integrity));
   }
 
   /** This optional operation is not yet supported. */
@@ -162,13 +169,20 @@
   }
 
   public FSDataOutputStream create(Path f, FsPermission permission,
+		    boolean overwrite,
+		    int bufferSize, short replication, long blockSize,
+		    Progressable progress) throws IOException {
+	  return create(f, permission, overwrite,bufferSize,replication, blockSize, progress, LabelSet.EMPTY, LabelSet.EMPTY);
+  }
+	  /*DIFC: modified*/
+  public FSDataOutputStream create(Path f, FsPermission permission,
     boolean overwrite,
     int bufferSize, short replication, long blockSize,
-    Progressable progress) throws IOException {
-
+    Progressable progress, LabelSet secrecy, LabelSet integrity) throws IOException {
+	  
     return new FSDataOutputStream
-       (dfs.create(getPathName(f), permission,
-                   overwrite, replication, blockSize, progress, bufferSize),
+       (dfs.createLabeled(getPathName(f), permission,
+                   overwrite, replication, blockSize, progress, bufferSize, secrecy, integrity),
         statistics);
   }
 
@@ -181,10 +195,10 @@
   /**
    * Rename files/dirs
    */
-  public boolean rename(Path src, Path dst) throws IOException {
-    return dfs.rename(getPathName(src), getPathName(dst));
+  public boolean rename(Path src, Path dst, LabelSet secrecy, LabelSet integrity) throws IOException {
+    return dfs.rename(getPathName(src), getPathName(dst), secrecy, integrity);
   }
-
+  
   /**
    * Get rid of Path f, whether a true file or dir.
    */
Index: src/hdfs/org/apache/hadoop/hdfs/DFSClient.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/DFSClient.java	(revision 142)
+++ src/hdfs/org/apache/hadoop/hdfs/DFSClient.java	(working copy)
@@ -34,6 +34,7 @@
 import org.apache.hadoop.hdfs.server.namenode.NameNode;
 import org.apache.hadoop.hdfs.server.namenode.NotReplicatedYetException;
 import org.apache.hadoop.security.AccessControlException;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.security.UnixUserGroupInformation;
 import org.apache.hadoop.util.*;
 
@@ -125,7 +126,8 @@
     Map<String,RetryPolicy> methodNameToPolicyMap = new HashMap<String,RetryPolicy>();
     
     methodNameToPolicyMap.put("create", methodPolicy);
-
+    /*DIFC: added*/
+    methodNameToPolicyMap.put("createLabeled", methodPolicy);
     return (ClientProtocol) RetryProxy.create(ClientProtocol.class,
         rpcNamenode, methodNameToPolicyMap);
   }
@@ -272,11 +274,15 @@
     }
     return hints;
   }
-
   private static LocatedBlocks callGetBlockLocations(ClientProtocol namenode,
-      String src, long start, long length) throws IOException {
+	      String src, long start, long length) throws IOException {
+	  return callGetBlockLocations(namenode,src,start,length,LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  
+  private static LocatedBlocks callGetBlockLocations(ClientProtocol namenode,
+      String src, long start, long length, LabelSet secrecy, LabelSet integrity) throws IOException {
     try {
-      return namenode.getBlockLocations(src, start, length);
+      return namenode.getBlockLocations(src, start, length, secrecy, integrity);
     } catch(RemoteException re) {
       throw re.unwrapRemoteException(AccessControlException.class,
                                     FileNotFoundException.class);
@@ -295,9 +301,14 @@
    * MapReduce system tries to schedule tasks on the same machines
    * as the data-block the task processes. 
    */
+  public BlockLocation[] getBlockLocations(String src, 
+		  long start, long len) throws IOException {
+	  return getBlockLocations(src,start,len,LabelSet.EMPTY,LabelSet.EMPTY);
+	  }
+  
   public BlockLocation[] getBlockLocations(String src, long start, 
-    long length) throws IOException {
-    LocatedBlocks blocks = callGetBlockLocations(namenode, src, start, length);
+    long length, LabelSet secrecy, LabelSet integrity) throws IOException {
+    LocatedBlocks blocks = callGetBlockLocations(namenode, src, start, length, secrecy, integrity);
     if (blocks == null) {
       return new BlockLocation[0];
     }
@@ -321,8 +332,12 @@
   }
 
   public DFSInputStream open(String src) throws IOException {
-    return open(src, conf.getInt("io.file.buffer.size", 4096), true, null);
+	  return open(src,LabelSet.EMPTY,LabelSet.EMPTY);
   }
+  
+  public DFSInputStream open(String src, LabelSet secrecy, LabelSet integrity) throws IOException {
+    return open(src, conf.getInt("io.file.buffer.size", 4096), true, null, secrecy, integrity);
+  }
 
   /**
    * Create an input stream that obtains a nodelist from the
@@ -331,11 +346,11 @@
    * work.
    */
   DFSInputStream open(String src, int buffersize, boolean verifyChecksum,
-                      FileSystem.Statistics stats
+                      FileSystem.Statistics stats, LabelSet secrecy, LabelSet integrity
       ) throws IOException {
     checkOpen();
     //    Get block info from namenode
-    return new DFSInputStream(src, buffersize, verifyChecksum);
+    return new DFSInputStream(src, buffersize, verifyChecksum, secrecy, integrity);
   }
 
   /**
@@ -347,9 +362,15 @@
    * @throws IOException
    */
   public OutputStream create(String src, 
-                             boolean overwrite
+          boolean overwrite
+          ) throws IOException {
+	  return createLabeled(src, overwrite,LabelSet.EMPTY, LabelSet.EMPTY);
+  }
+  
+  public OutputStream createLabeled(String src, 
+                             boolean overwrite, LabelSet secrecy, LabelSet integrity
                              ) throws IOException {
-    return create(src, overwrite, defaultReplication, defaultBlockSize, null);
+    return createLabeled(src, overwrite, defaultReplication, defaultBlockSize, null, secrecy, integrity);
   }
     
   /**
@@ -362,10 +383,16 @@
    * @throws IOException
    */
   public OutputStream create(String src, 
+          boolean overwrite,
+          Progressable progress
+          ) throws IOException {
+	  return createLabeled(src, overwrite, progress, LabelSet.EMPTY, LabelSet.EMPTY);
+  }
+  public OutputStream createLabeled(String src, 
                              boolean overwrite,
-                             Progressable progress
+                             Progressable progress, LabelSet secrecy, LabelSet integrity
                              ) throws IOException {
-    return create(src, overwrite, defaultReplication, defaultBlockSize, null);
+    return createLabeled(src, overwrite, defaultReplication, defaultBlockSize, null, secrecy, integrity);
   }
     
   /**
@@ -378,12 +405,20 @@
    * @return output stream
    * @throws IOException
    */
+  
   public OutputStream create(String src, 
+          boolean overwrite, 
+          short replication,
+          long blockSize
+          ) throws IOException {
+	  return createLabeled(src, overwrite, replication, blockSize, LabelSet.EMPTY, LabelSet.EMPTY);
+  }
+  public OutputStream createLabeled(String src, 
                              boolean overwrite, 
                              short replication,
-                             long blockSize
+                             long blockSize, LabelSet secrecy, LabelSet integrity
                              ) throws IOException {
-    return create(src, overwrite, replication, blockSize, null);
+    return createLabeled(src, overwrite, replication, blockSize, null, secrecy, integrity);
   }
 
   
@@ -399,13 +434,21 @@
    * @throws IOException
    */
   public OutputStream create(String src, 
+          boolean overwrite, 
+          short replication,
+          long blockSize,
+          Progressable progress
+          ) throws IOException {
+	  return createLabeled(src, overwrite,replication, blockSize,progress, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  public OutputStream createLabeled(String src, 
                              boolean overwrite, 
                              short replication,
                              long blockSize,
-                             Progressable progress
+                             Progressable progress, LabelSet secrecy, LabelSet integrity
                              ) throws IOException {
-    return create(src, overwrite, replication, blockSize, progress,
-        conf.getInt("io.file.buffer.size", 4096));
+    return createLabeled(src, overwrite, replication, blockSize, progress,
+        conf.getInt("io.file.buffer.size", 4096), secrecy, integrity);
   }
   /**
    * Call
@@ -413,15 +456,25 @@
    * with default permission.
    * @see FsPermission#getDefault()
    */
+  
   public OutputStream create(String src,
+	      boolean overwrite,
+	      short replication,
+	      long blockSize,
+	      Progressable progress,
+	      int buffersize
+	      ) throws IOException {
+	return createLabeled(src, overwrite, replication, blockSize, progress, buffersize, LabelSet.EMPTY, LabelSet.EMPTY);  
+  }
+  public OutputStream createLabeled(String src,
       boolean overwrite,
       short replication,
       long blockSize,
       Progressable progress,
-      int buffersize
+      int buffersize, LabelSet secrecy, LabelSet integrity
       ) throws IOException {
-    return create(src, FsPermission.getDefault(),
-        overwrite, replication, blockSize, progress, buffersize);
+    return createLabeled(src, FsPermission.getDefault(),
+        overwrite, replication, blockSize, progress, buffersize, secrecy, integrity);
   }
   /**
    * Create a new dfs file with the specified block replication 
@@ -437,15 +490,26 @@
    * @throws IOException
    * @see ClientProtocol#create(String, FsPermission, String, boolean, short, long)
    */
+  
   public OutputStream create(String src, 
+          FsPermission permission,
+          boolean overwrite, 
+          short replication,
+          long blockSize,
+          Progressable progress,
+          int buffersize
+          ) throws IOException {
+	  return createLabeled(src,permission, overwrite,replication,blockSize,progress,buffersize, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  public OutputStream createLabeled(String src, 
                              FsPermission permission,
                              boolean overwrite, 
                              short replication,
                              long blockSize,
                              Progressable progress,
-                             int buffersize
+                             int buffersize, LabelSet secrecy, LabelSet integrity
                              ) throws IOException {
-    checkOpen();
+	checkOpen();
     if (permission == null) {
       permission = FsPermission.getDefault();
     }
@@ -453,7 +517,7 @@
     LOG.debug(src + ": masked=" + masked);
     OutputStream result = new DFSOutputStream(src, masked,
         overwrite, replication, blockSize, progress, buffersize,
-        conf.getInt("io.bytes.per.checksum", 512));
+        conf.getInt("io.bytes.per.checksum", 512), secrecy, integrity);
     leasechecker.put(src, result);
     return result;
   }
@@ -511,9 +575,12 @@
    * See {@link ClientProtocol#rename(String, String)}. 
    */
   public boolean rename(String src, String dst) throws IOException {
+	  return rename(src,dst,LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  public boolean rename(String src, String dst, LabelSet secrecy,LabelSet integrity) throws IOException {
     checkOpen();
     try {
-      return namenode.rename(src, dst);
+      return namenode.rename(src, dst, secrecy,integrity);
     } catch(RemoteException re) {
       throw re.unwrapRemoteException(AccessControlException.class,
                                      QuotaExceededException.class);
@@ -1263,11 +1330,20 @@
     }
 
     public static BlockReader newBlockReader( Socket sock, String file,
+            long blockId, 
+            long genStamp,
+            long startOffset, long len,
+            int bufferSize, boolean verifyChecksum,
+            String clientName)
+            throws IOException {
+    	return newBlockReader(sock,file,blockId,genStamp,startOffset,len,bufferSize,verifyChecksum,clientName,LabelSet.EMPTY,LabelSet.EMPTY);
+    }
+    public static BlockReader newBlockReader( Socket sock, String file,
                                        long blockId, 
                                        long genStamp,
                                        long startOffset, long len,
                                        int bufferSize, boolean verifyChecksum,
-                                       String clientName)
+                                       String clientName, LabelSet secrecy, LabelSet integrity)
                                        throws IOException {
       // in and out will be closed when sock is closed (by the caller)
       DataOutputStream out = new DataOutputStream(
@@ -1281,6 +1357,11 @@
       out.writeLong( startOffset );
       out.writeLong( len );
       Text.writeString(out, clientName);
+      /*DIFC: protocol changed to allow passing of labels
+       * Note: we send labels before the checksum!!
+       * */
+      LabelSet.writeToStream(out, secrecy);
+      LabelSet.writeToStream(out, integrity);
       out.flush();
       
       //
@@ -1364,6 +1445,9 @@
     private long pos = 0;
     private long blockEnd = -1;
     private int failures = 0;
+    /*DIFC: add secrecy and integrity labels*/
+    LabelSet secrecy=LabelSet.EMPTY;
+    LabelSet integrity=LabelSet.EMPTY;
 
     /* XXX Use of CocurrentHashMap is temp fix. Need to fix 
      * parallel accesses to DFSInputStream (through ptreads) properly */
@@ -1377,12 +1461,14 @@
       deadNodes.put(dnInfo, dnInfo);
     }
     
-    DFSInputStream(String src, int buffersize, boolean verifyChecksum
+    DFSInputStream(String src, int buffersize, boolean verifyChecksum, LabelSet secSet, LabelSet intSet
                    ) throws IOException {
       this.verifyChecksum = verifyChecksum;
       this.buffersize = buffersize;
       this.src = src;
       prefetchSize = conf.getLong("dfs.read.prefetch.size", prefetchSize);
+      this.secrecy=secSet;
+      this.integrity=intSet;
       openInfo();
     }
 
@@ -1390,7 +1476,7 @@
      * Grab the open-file info from namenode
      */
     synchronized void openInfo() throws IOException {
-      LocatedBlocks newInfo = callGetBlockLocations(namenode, src, 0, prefetchSize);
+      LocatedBlocks newInfo = callGetBlockLocations(namenode, src, 0, prefetchSize, this.secrecy, this.integrity);
       if (newInfo == null) {
         throw new IOException("Cannot open filename " + src);
       }
@@ -1449,7 +1535,7 @@
         targetBlockIdx = LocatedBlocks.getInsertIndex(targetBlockIdx);
         // fetch more blocks
         LocatedBlocks newBlocks;
-        newBlocks = callGetBlockLocations(namenode, src, offset, prefetchSize);
+        newBlocks = callGetBlockLocations(namenode, src, offset, prefetchSize,this.secrecy, this.integrity);
         assert (newBlocks != null) : "Could not find target position " + offset;
         locatedBlocks.insertRange(targetBlockIdx, newBlocks.getLocatedBlocks());
       }
@@ -1488,7 +1574,7 @@
           blk = locatedBlocks.get(blockIdx);
         if (blk == null || curOff < blk.getStartOffset()) {
           LocatedBlocks newBlocks;
-          newBlocks = callGetBlockLocations(namenode, src, curOff, remaining);
+          newBlocks = callGetBlockLocations(namenode, src, curOff, remaining, this.secrecy, this.integrity);
           locatedBlocks.insertRange(blockIdx, newBlocks.getLocatedBlocks());
           continue;
         }
@@ -1546,7 +1632,7 @@
           blockReader = BlockReader.newBlockReader(s, src, blk.getBlockId(), 
               blk.getGenerationStamp(),
               offsetIntoBlock, blk.getNumBytes() - offsetIntoBlock,
-              buffersize, verifyChecksum, clientName);
+              buffersize, verifyChecksum, clientName, this.secrecy, this.integrity);
           return chosenNode;
         } catch (IOException ex) {
           // Put chosen node into dead list, continue
@@ -1750,7 +1836,7 @@
                                               block.getBlock().getBlockId(),
                                               block.getBlock().getGenerationStamp(),
                                               start, len, buffersize, 
-                                              verifyChecksum, clientName);
+                                              verifyChecksum, clientName, this.secrecy, this.integrity);
           int nread = reader.readAll(buf, offset, len);
           if (nread != len) {
             throw new IOException("truncated return from reader.read(): " +
@@ -2138,7 +2224,14 @@
     private class DataStreamer extends Daemon {
 
       private volatile boolean closed = false;
-  
+      LabelSet secrecy=LabelSet.EMPTY;
+      LabelSet integrity=LabelSet.EMPTY;
+      
+      public void setLabels(LabelSet secSet, LabelSet intSet){
+    	  this.secrecy=secSet;
+    	  this.integrity=intSet;
+      }
+      
       public void run() {
 
         while (!closed && clientRunning) {
@@ -2180,7 +2273,8 @@
               // get new block from namenode.
               if (blockStream == null) {
                 LOG.debug("Allocating new block");
-                nodes = nextBlockOutputStream(src); 
+                /*DIFC: changed*/
+                nodes = nextBlockOutputStream(src, this.secrecy, this.integrity); 
                 this.setName("DataStreamer for file " + src +
                              " block " + block);
                 response = new ResponseProcessor(nodes);
@@ -2577,19 +2671,28 @@
      * @see ClientProtocol#create(String, FsPermission, String, boolean, short, long)
      */
     DFSOutputStream(String src, FsPermission masked, boolean overwrite,
+            short replication, long blockSize, Progressable progress,
+            int buffersize, int bytesPerChecksum) throws IOException {
+    	this(src,masked,overwrite,replication,blockSize,progress,buffersize,bytesPerChecksum,LabelSet.EMPTY,LabelSet.EMPTY);
+    }
+   
+    DFSOutputStream(String src, FsPermission masked, boolean overwrite,
         short replication, long blockSize, Progressable progress,
-        int buffersize, int bytesPerChecksum) throws IOException {
+        int buffersize, int bytesPerChecksum, LabelSet secrecy, LabelSet integrity) throws IOException {
       this(src, blockSize, progress, bytesPerChecksum);
 
       computePacketChunkSize(writePacketSize, bytesPerChecksum);
-
+      System.out.println("DFSCliet: src="+src+":secrecy="+secrecy+":int="+integrity);
       try {
-        namenode.create(
-            src, masked, clientName, overwrite, replication, blockSize);
+    	  //namenode.create(
+    	    //        src, masked, clientName, overwrite, replication, blockSize);
+        namenode.createLabeled(
+            src, masked, clientName, overwrite, replication, blockSize, secrecy, integrity);
       } catch(RemoteException re) {
-        throw re.unwrapRemoteException(AccessControlException.class,
+    	  throw re.unwrapRemoteException(AccessControlException.class,
                                        QuotaExceededException.class);
       }
+      streamer.setLabels(secrecy, integrity);
       streamer.start();
     }
   
@@ -2680,6 +2783,10 @@
      * Returns the list of target datanodes.
      */
     private DatanodeInfo[] nextBlockOutputStream(String client) throws IOException {
+    	return nextBlockOutputStream(client, LabelSet.EMPTY,LabelSet.EMPTY);
+    }
+    /*DIFC: changed to add labels*/
+    private DatanodeInfo[] nextBlockOutputStream(String client, LabelSet secrecy, LabelSet integrity) throws IOException {
       LocatedBlock lb = null;
       boolean retry = false;
       DatanodeInfo[] nodes;
@@ -2701,7 +2808,7 @@
         //
         // Connect to first DataNode in the list.
         //
-        success = createBlockOutputStream(nodes, clientName, false);
+        success = createBlockOutputStream(nodes, clientName, false, secrecy, integrity);
 
         if (!success) {
           LOG.info("Abandoning block " + block);
@@ -2728,8 +2835,14 @@
     // connects to the first datanode in the pipeline
     // Returns true if success, otherwise return failure.
     //
+    
     private boolean createBlockOutputStream(DatanodeInfo[] nodes, String client,
-                    boolean recoveryFlag) {
+            boolean recoveryFlag) {
+    	return createBlockOutputStream(nodes,client,recoveryFlag,LabelSet.EMPTY,LabelSet.EMPTY);
+    }
+    /*DIFC: pass labels*/
+    private boolean createBlockOutputStream(DatanodeInfo[] nodes, String client,
+                    boolean recoveryFlag, LabelSet secrecy, LabelSet integrity) {
       String firstBadLink = "";
       if (LOG.isDebugEnabled()) {
         for (int i = 0; i < nodes.length; i++) {
@@ -2743,6 +2856,7 @@
       try {
         LOG.debug("Connecting to " + nodes[0].getName());
         InetSocketAddress target = NetUtils.createSocketAddr(nodes[0].getName());
+        //LOG.info("## dfsclient: connecting to target:="+target);
         s = socketFactory.createSocket();
         int timeoutValue = 3000 * nodes.length + socketTimeout;
         s.connect(target, timeoutValue);
@@ -2772,7 +2886,14 @@
         for (int i = 1; i < nodes.length; i++) {
           nodes[i].write(out);
         }
+        /*DIFC: protocol changed to allow passing of labels
+         * Note: we send labels before the checksum!!
+         * */
+        LabelSet.writeToStream(out, secrecy);
+        LabelSet.writeToStream(out, integrity);
+        
         checksum.writeHeader( out );
+      
         out.flush();
 
         // receive ack for connect
Index: src/mapred/org/apache/hadoop/mapred/SequenceFileAsTextRecordReader.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/SequenceFileAsTextRecordReader.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/SequenceFileAsTextRecordReader.java	(working copy)
@@ -24,6 +24,7 @@
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableComparable;
+import org.apache.hadoop.security.LabelSet;
 
 /**
  * This class converts the input keys and values to their String forms by calling toString()
@@ -40,6 +41,10 @@
   private Writable innerValue;
 
   public SequenceFileAsTextRecordReader(Configuration conf, FileSplit split)
+  throws IOException {
+	  this(conf, split, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  public SequenceFileAsTextRecordReader(Configuration conf, FileSplit split, LabelSet secrecy, LabelSet integrity)
     throws IOException {
     sequenceFileRecordReader =
       new SequenceFileRecordReader<WritableComparable, Writable>(conf, split);
Index: src/mapred/org/apache/hadoop/mapred/pipes/PipesNonJavaInputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/pipes/PipesNonJavaInputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/pipes/PipesNonJavaInputFormat.java	(working copy)
@@ -28,6 +28,7 @@
 import org.apache.hadoop.mapred.RecordReader;
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.ReflectionUtils;
 
 /**
@@ -45,9 +46,15 @@
   public RecordReader<FloatWritable, NullWritable> getRecordReader(
       InputSplit genericSplit, JobConf job, Reporter reporter)
       throws IOException {
-    return new PipesDummyRecordReader(job, genericSplit);
+    return new PipesDummyRecordReader(job, genericSplit, LabelSet.EMPTY, LabelSet.EMPTY);
   }
   
+  public RecordReader<FloatWritable, NullWritable> getRecordReader(
+	      InputSplit genericSplit, JobConf job, Reporter reporter, LabelSet secrecy, LabelSet integrity)
+	      throws IOException {
+	    return new PipesDummyRecordReader(job, genericSplit, secrecy, integrity);
+	  }
+  
   public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
     // Delegate the generation of input splits to the 'original' InputFormat
     return ReflectionUtils.newInstance(
@@ -73,6 +80,9 @@
     throws IOException{
     }
 
+    public PipesDummyRecordReader(Configuration job, InputSplit split, LabelSet secrecy, LabelSet integrity)
+    throws IOException{
+    }
     
     public FloatWritable createKey() {
       return null;
Index: src/mapred/org/apache/hadoop/mapred/MapRunner.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/MapRunner.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/MapRunner.java	(working copy)
@@ -18,7 +18,10 @@
 
 package org.apache.hadoop.mapred;
 
+import java.io.*;
+import java.io.FileWriter;
 import java.io.IOException;
+import java.util.Random;
 
 import org.apache.hadoop.util.ReflectionUtils;
 
@@ -44,7 +47,7 @@
       // allocate key & value instances that are re-used for all entries
       K1 key = input.createKey();
       V1 value = input.createValue();
-      
+     
       while (input.next(key, value)) {
         // map pair to output
         mapper.map(key, value, output, reporter);
Index: src/mapred/org/apache/hadoop/mapred/JobClient.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/JobClient.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/JobClient.java	(working copy)
@@ -781,7 +781,7 @@
     LOG.debug("Creating splits at " + fs.makeQualified(submitSplitFile));
     InputSplit[] splits = 
       job.getInputFormat().getSplits(job, job.getNumMapTasks());
-    // sort the splits into order based on size, so that the biggest
+   // sort the splits into order based on size, so that the biggest
     // go first
     Arrays.sort(splits, new Comparator<InputSplit>() {
       public int compare(InputSplit a, InputSplit b) {
Index: src/mapred/org/apache/hadoop/mapred/KeyValueLineRecordReader.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/KeyValueLineRecordReader.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/KeyValueLineRecordReader.java	(working copy)
@@ -23,7 +23,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.Text;
-
+import org.apache.hadoop.security.LabelSet;
 /**
  * This class treats a line in the input as a key/value pair separated by a 
  * separator character. The separator can be specified in config file 
@@ -50,7 +50,11 @@
     return new Text();
   }
 
-  public KeyValueLineRecordReader(Configuration job, FileSplit split)
+  public KeyValueLineRecordReader(Configuration job, FileSplit split) throws IOException{
+	  this(job, split, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  
+  public KeyValueLineRecordReader(Configuration job, FileSplit split, LabelSet secrecy, LabelSet integrity)
     throws IOException {
     
     lineRecordReader = new LineRecordReader(job, split);
Index: src/mapred/org/apache/hadoop/mapred/join/ComposableInputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/join/ComposableInputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/join/ComposableInputFormat.java	(working copy)
@@ -26,7 +26,7 @@
 import org.apache.hadoop.mapred.InputSplit;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.Reporter;
-
+import org.apache.hadoop.security.LabelSet;
 /**
  * Refinement of InputFormat requiring implementors to provide
  * ComposableRecordReader instead of RecordReader.
@@ -37,4 +37,6 @@
 
   ComposableRecordReader<K,V> getRecordReader(InputSplit split,
       JobConf job, Reporter reporter) throws IOException;
+  ComposableRecordReader<K,V> getRecordReader(InputSplit split,
+	      JobConf job, Reporter reporter, LabelSet secrecy, LabelSet integrity) throws IOException;
 }
Index: src/mapred/org/apache/hadoop/mapred/join/CompositeRecordReader.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/join/CompositeRecordReader.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/join/CompositeRecordReader.java	(working copy)
@@ -29,6 +29,7 @@
 import org.apache.hadoop.io.WritableComparator;
 import org.apache.hadoop.io.WritableUtils;
 import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.ReflectionUtils;
 
 /**
@@ -61,9 +62,14 @@
    * The id of a root CompositeRecordReader is -1 by convention, but relying
    * on this is not recommended.
    */
+  public CompositeRecordReader(int id, int capacity,
+	      Class<? extends WritableComparator> cmpcl)
+	      throws IOException {
+	  this(id, capacity, cmpcl, LabelSet.EMPTY, LabelSet.EMPTY);
+  }
   @SuppressWarnings("unchecked") // Generic array assignment
   public CompositeRecordReader(int id, int capacity,
-      Class<? extends WritableComparator> cmpcl)
+      Class<? extends WritableComparator> cmpcl, LabelSet secrecy, LabelSet integrity)
       throws IOException {
     assert capacity > 0 : "Invalid capacity";
     this.id = id;
Index: src/mapred/org/apache/hadoop/mapred/join/CompositeInputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/join/CompositeInputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/join/CompositeInputFormat.java	(working copy)
@@ -30,6 +30,7 @@
 import org.apache.hadoop.mapred.InputSplit;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.security.LabelSet;
 
 /**
  * An InputFormat capable of performing joins over a set of data sources sorted
@@ -123,11 +124,15 @@
    * The outermost join need only be composable, not necessarily a composite.
    * Mandating TupleWritable isn't strictly correct.
    */
+  public ComposableRecordReader<K,TupleWritable> getRecordReader(
+	      InputSplit split, JobConf job, Reporter reporter) throws IOException {
+	  return getRecordReader(split, job, reporter, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
   @SuppressWarnings("unchecked") // child types unknown
   public ComposableRecordReader<K,TupleWritable> getRecordReader(
-      InputSplit split, JobConf job, Reporter reporter) throws IOException {
+      InputSplit split, JobConf job, Reporter reporter, LabelSet secrecy, LabelSet integrity) throws IOException {
     setFormat(job);
-    return root.getRecordReader(split, job, reporter);
+    return root.getRecordReader(split, job, reporter, secrecy, integrity);
   }
 
   /**
Index: src/mapred/org/apache/hadoop/mapred/join/Parser.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/join/Parser.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/join/Parser.java	(working copy)
@@ -40,6 +40,7 @@
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.RecordReader;
 import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.ReflectionUtils;
 
 /**
@@ -304,7 +305,11 @@
     }
 
     public ComposableRecordReader getRecordReader(
-        InputSplit split, JobConf job, Reporter reporter) throws IOException {
+            InputSplit split, JobConf job, Reporter reporter) throws IOException {
+    	return getRecordReader(split, job,reporter, LabelSet.EMPTY,LabelSet.EMPTY);
+    }
+    public ComposableRecordReader getRecordReader(
+        InputSplit split, JobConf job, Reporter reporter, LabelSet secrecy, LabelSet integrity) throws IOException {
       try {
         if (!rrCstrMap.containsKey(ident)) {
           throw new IOException("No RecordReader for " + ident);
@@ -383,8 +388,12 @@
     }
 
     @SuppressWarnings("unchecked") // child types unknowable
+     public ComposableRecordReader getRecordReader(
+        InputSplit split, JobConf job, Reporter reporter) throws IOException {
+    	return getRecordReader(split, job, reporter, LabelSet.EMPTY,LabelSet.EMPTY);
+    }
     public ComposableRecordReader getRecordReader(
-        InputSplit split, JobConf job, Reporter reporter) throws IOException {
+        InputSplit split, JobConf job, Reporter reporter, LabelSet secrecy, LabelSet integrity) throws IOException {
       if (!(split instanceof CompositeInputSplit)) {
         throw new IOException("Invalid split type:" +
                               split.getClass().getName());
Index: src/mapred/org/apache/hadoop/mapred/JobConf.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/JobConf.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/JobConf.java	(working copy)
@@ -43,7 +43,7 @@
 import org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner;
 import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hadoop.util.Tool;
-
+import org.apache.hadoop.security.LabelSet;
 /** 
  * A map/reduce job configuration.
  * 
@@ -1385,6 +1385,33 @@
     set("mapred.job.queue.name", queueName);
   }
   
+  /*DIFC: labels with which the computation should run
+   * TODO: set the labels on a per mapper basis*/
+  public void setSecrecyLabel(LabelSet secrecy){
+	  setLabel("mapred.secrecy", secrecy);
+  }
+  public void setIntegrityLabel(LabelSet integrity){
+	  setLabel("mapred.integrity", integrity);
+  }
+  public LabelSet getSecrecyLabel(){
+	  return getLabel("mapred.secrecy",LabelSet.EMPTY);
+  }
+  public LabelSet getIntegrityLabel(){
+	  return getLabel("mapred.integrity",LabelSet.EMPTY);
+  }
+  
+  public void setLocalHadoopPath(String p){
+	  set("local.hadoop.path",p);
+  }
+  public String getLocalHadoopPath(){
+	  return get("local.hadoop.path");
+  }
+  public void setTempDirPath(String p){
+	  set("temp.dir.path",p);
+  }
+  public String getTempDirPath(){
+	  return get("temp.dir.path");
+  }
   /** 
    * Find a jar that contains a class of the same name, if any.
    * It will return a jar file, even if that is not the first thing
Index: src/mapred/org/apache/hadoop/mapred/MultiFileInputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/MultiFileInputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/MultiFileInputFormat.java	(working copy)
@@ -25,7 +25,7 @@
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.FileUtil;
 import org.apache.hadoop.fs.Path;
-
+import org.apache.hadoop.security.LabelSet;
 /**
  * An abstract {@link InputFormat} that returns {@link MultiFileSplit}'s
  * in {@link #getSplits(JobConf, int)} method. Splits are constructed from 
@@ -98,6 +98,6 @@
   
   @Override
   public abstract RecordReader<K, V> getRecordReader(InputSplit split,
-      JobConf job, Reporter reporter)
+      JobConf job, Reporter reporter, LabelSet secrecy, LabelSet integrity)
       throws IOException;
 }
Index: src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java	(working copy)
@@ -21,6 +21,7 @@
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableComparable;
 import org.apache.hadoop.mapred.*;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.Progressable;
 
 import java.io.IOException;
@@ -543,7 +544,7 @@
 
     @SuppressWarnings({"unchecked"})
     public RecordWriter<Object, Object> getRecordWriter(
-      FileSystem fs, JobConf job, String baseFileName, Progressable progress)
+      FileSystem fs, JobConf job, String baseFileName, Progressable progress, LabelSet secrecy, LabelSet integrity)
       throws IOException {
 
       String nameOutput = job.get(CONFIG_NAMED_OUTPUT, null);
@@ -556,7 +557,7 @@
       outputConf.setOutputKeyClass(getNamedOutputKeyClass(job, nameOutput));
       outputConf.setOutputValueClass(getNamedOutputValueClass(job, nameOutput));
       OutputFormat outputFormat = outputConf.getOutputFormat();
-      return outputFormat.getRecordWriter(fs, outputConf, fileName, progress);
+      return outputFormat.getRecordWriter(fs, outputConf, fileName, progress, secrecy,  integrity);
     }
   }
 
Index: src/mapred/org/apache/hadoop/mapred/lib/NullOutputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/lib/NullOutputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/lib/NullOutputFormat.java	(working copy)
@@ -23,6 +23,7 @@
 import org.apache.hadoop.mapred.OutputFormat;
 import org.apache.hadoop.mapred.RecordWriter;
 import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.Progressable;
 
 /**
@@ -31,12 +32,19 @@
 public class NullOutputFormat<K, V> implements OutputFormat<K, V> {
   
   public RecordWriter<K, V> getRecordWriter(FileSystem ignored, JobConf job, 
-                                      String name, Progressable progress) {
+                                      String name, Progressable progress, LabelSet secrecy, LabelSet integrity) {
     return new RecordWriter<K, V>(){
         public void write(K key, V value) { }
         public void close(Reporter reporter) { }
       };
   }
   
+  public RecordWriter<K, V> getRecordWriter(FileSystem ignored, JobConf job, 
+		  String name, Progressable progress) {
+	  return new RecordWriter<K, V>(){
+		  public void write(K key, V value) { }
+		  public void close(Reporter reporter) { }
+	  };
+  }
   public void checkOutputSpecs(FileSystem ignored, JobConf job) { }
 }
Index: src/mapred/org/apache/hadoop/mapred/lib/db/DBOutputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/lib/db/DBOutputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/lib/db/DBOutputFormat.java	(working copy)
@@ -30,6 +30,7 @@
 import org.apache.hadoop.mapred.OutputFormat;
 import org.apache.hadoop.mapred.RecordWriter;
 import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.Progressable;
 import org.apache.hadoop.util.StringUtils;
 
@@ -121,10 +122,14 @@
   throws IOException {
   }
 
+  public RecordWriter<K, V> getRecordWriter(FileSystem filesystem,
+	      JobConf job, String name, Progressable progress) throws IOException {
+	  return getRecordWriter(filesystem,job,name,progress,LabelSet.EMPTY,LabelSet.EMPTY);
+  }
 
   /** {@inheritDoc} */
   public RecordWriter<K, V> getRecordWriter(FileSystem filesystem,
-      JobConf job, String name, Progressable progress) throws IOException {
+      JobConf job, String name, Progressable progress, LabelSet secrecy, LabelSet integrity) throws IOException {
 
     DBConfiguration dbConf = new DBConfiguration(job);
     String tableName = dbConf.getOutputTableName();
Index: src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java	(working copy)
@@ -35,6 +35,7 @@
 import org.apache.hadoop.mapred.JobConfigurable;
 import org.apache.hadoop.mapred.RecordReader;
 import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.ReflectionUtils;
 
 /**
@@ -280,6 +281,11 @@
   @SuppressWarnings("unchecked")
   public RecordReader<LongWritable, T> getRecordReader(InputSplit split,
       JobConf job, Reporter reporter) throws IOException {
+	  return getRecordReader(split,job,reporter,LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  
+  public RecordReader<LongWritable, T> getRecordReader(InputSplit split,
+      JobConf job, Reporter reporter, LabelSet secrecy, LabelSet integrity) throws IOException {
 
     Class inputClass = dbConf.getInputClass();
     try {
Index: src/mapred/org/apache/hadoop/mapred/lib/MultipleSequenceFileOutputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/lib/MultipleSequenceFileOutputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/lib/MultipleSequenceFileOutputFormat.java	(working copy)
@@ -24,6 +24,7 @@
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.RecordWriter;
 import org.apache.hadoop.mapred.SequenceFileOutputFormat;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.Progressable;
 
 /**
@@ -33,17 +34,28 @@
 public class MultipleSequenceFileOutputFormat <K,V>
 extends MultipleOutputFormat<K, V> {
 
-    private SequenceFileOutputFormat<K,V> theSequenceFileOutputFormat = null;
-  
-  @Override
-  protected RecordWriter<K, V> getBaseRecordWriter(FileSystem fs,
-                                                   JobConf job,
-                                                   String name,
-                                                   Progressable arg3) 
-  throws IOException {
-    if (theSequenceFileOutputFormat == null) {
-      theSequenceFileOutputFormat = new SequenceFileOutputFormat<K,V>();
-    }
-    return theSequenceFileOutputFormat.getRecordWriter(fs, job, name, arg3);
-  }
+	private SequenceFileOutputFormat<K,V> theSequenceFileOutputFormat = null;
+
+	
+	protected RecordWriter<K, V> getBaseRecordWriter(FileSystem fs,
+			JobConf job,
+			String name,
+			Progressable arg3, LabelSet secrecy, LabelSet integrity) 
+			throws IOException {
+		if (theSequenceFileOutputFormat == null) {
+			theSequenceFileOutputFormat = new SequenceFileOutputFormat<K,V>();
+		}
+		return theSequenceFileOutputFormat.getRecordWriter(fs, job, name, arg3, secrecy,integrity);
+	}
+	@Override
+	protected RecordWriter<K, V> getBaseRecordWriter(FileSystem fs,
+			JobConf job,
+			String name,
+			Progressable arg3) 
+			throws IOException {
+		if (theSequenceFileOutputFormat == null) {
+			theSequenceFileOutputFormat = new SequenceFileOutputFormat<K,V>();
+		}
+		return theSequenceFileOutputFormat.getRecordWriter(fs, job, name, arg3);
+	}
 }
Index: src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java	(working copy)
@@ -28,6 +28,7 @@
 import org.apache.hadoop.mapred.FileOutputFormat;
 import org.apache.hadoop.mapred.RecordWriter;
 import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.Progressable;
 
 /**
@@ -67,7 +68,7 @@
    * @throws IOException
    */
   public RecordWriter<K, V> getRecordWriter(FileSystem fs, JobConf job,
-      String name, Progressable arg3) throws IOException {
+      String name, Progressable arg3, LabelSet secrecy, LabelSet integrity) throws IOException {
 
     final FileSystem myFS = fs;
     final String myName = generateLeafFileName(name);
Index: src/mapred/org/apache/hadoop/mapred/lib/MultipleTextOutputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/lib/MultipleTextOutputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/lib/MultipleTextOutputFormat.java	(working copy)
@@ -24,6 +24,7 @@
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.RecordWriter;
 import org.apache.hadoop.mapred.TextOutputFormat;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.Progressable;
 
 /**
@@ -35,12 +36,20 @@
 
   private TextOutputFormat<K, V> theTextOutputFormat = null;
 
-  @Override
+ 
   protected RecordWriter<K, V> getBaseRecordWriter(FileSystem fs, JobConf job,
-      String name, Progressable arg3) throws IOException {
+      String name, Progressable arg3, LabelSet secrecy, LabelSet integrity) throws IOException {
     if (theTextOutputFormat == null) {
       theTextOutputFormat = new TextOutputFormat<K, V>();
     }
-    return theTextOutputFormat.getRecordWriter(fs, job, name, arg3);
+    return theTextOutputFormat.getRecordWriter(fs, job, name, arg3, secrecy,integrity);
   }
+  @Override
+  protected RecordWriter<K, V> getBaseRecordWriter(FileSystem fs, JobConf job,
+	      String name, Progressable arg3) throws IOException {
+	    if (theTextOutputFormat == null) {
+	      theTextOutputFormat = new TextOutputFormat<K, V>();
+	    }
+	    return theTextOutputFormat.getRecordWriter(fs, job, name, arg3);
+	  }
 }
Index: src/mapred/org/apache/hadoop/mapred/lib/DelegatingInputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/lib/DelegatingInputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/lib/DelegatingInputFormat.java	(working copy)
@@ -34,6 +34,7 @@
 import org.apache.hadoop.mapred.Mapper;
 import org.apache.hadoop.mapred.RecordReader;
 import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.ReflectionUtils;
 
 /**
@@ -114,6 +115,10 @@
   @SuppressWarnings("unchecked")
   public RecordReader<K, V> getRecordReader(InputSplit split, JobConf conf,
       Reporter reporter) throws IOException {
+	  return getRecordReader(split, conf, reporter, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  public RecordReader<K, V> getRecordReader(InputSplit split, JobConf conf,
+      Reporter reporter, LabelSet secrecy, LabelSet integrity) throws IOException {
 
     // Find the InputFormat and then the RecordReader from the
     // TaggedInputSplit.
Index: src/mapred/org/apache/hadoop/mapred/lib/NLineInputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/lib/NLineInputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/lib/NLineInputFormat.java	(working copy)
@@ -35,6 +35,7 @@
 import org.apache.hadoop.mapred.LineRecordReader;
 import org.apache.hadoop.mapred.RecordReader;
 import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.LineReader;
 
 /**
@@ -61,12 +62,19 @@
   private int N = 1;
 
   public RecordReader<LongWritable, Text> getRecordReader(
+          InputSplit genericSplit,
+          JobConf job,
+          Reporter reporter) 
+throws IOException {
+	  return getRecordReader(genericSplit, job, reporter, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  public RecordReader<LongWritable, Text> getRecordReader(
                                             InputSplit genericSplit,
                                             JobConf job,
-                                            Reporter reporter) 
+                                            Reporter reporter, LabelSet secrecy, LabelSet integrity) 
   throws IOException {
     reporter.setStatus(genericSplit.toString());
-    return new LineRecordReader(job, (FileSplit) genericSplit);
+    return new LineRecordReader(job, (FileSplit) genericSplit, secrecy, integrity);
   }
 
   /** 
Index: src/mapred/org/apache/hadoop/mapred/ReduceTask.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/ReduceTask.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/ReduceTask.java	(working copy)
@@ -76,6 +76,7 @@
 import org.apache.hadoop.metrics.MetricsRecord;
 import org.apache.hadoop.metrics.MetricsUtil;
 import org.apache.hadoop.metrics.Updater;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.Progress;
 import org.apache.hadoop.util.Progressable;
 import org.apache.hadoop.util.ReflectionUtils;
@@ -361,7 +362,7 @@
 
     boolean isLocal = "local".equals(job.get("mapred.job.tracker", "local"));
     if (!isLocal) {
-      reduceCopier = new ReduceCopier(umbilical, job);
+      reduceCopier = new ReduceCopier(umbilical, job, job.getSecrecyLabel(),job.getIntegrityLabel());
       if (!reduceCopier.fetchOutputs()) {
         if(reduceCopier.mergeThrowable instanceof FSError) {
           LOG.error("Task: " + getTaskID() + " - FSError: " + 
@@ -395,9 +396,9 @@
     String finalName = getOutputName(getPartition());
 
     FileSystem fs = FileSystem.get(job);
-
+    /*Write to a labeled file*/
     final RecordWriter out = 
-      job.getOutputFormat().getRecordWriter(fs, job, finalName, reporter);  
+      job.getOutputFormat().getRecordWriter(fs, job, finalName, reporter, job.getSecrecyLabel(),job.getIntegrityLabel());  
     
     OutputCollector collector = new OutputCollector() {
         public void collect(Object key, Object value)
@@ -409,6 +410,7 @@
         }
       };
     
+
     // apply reduce function
     try {
       Class keyClass = job.getMapOutputKeyClass();
@@ -439,6 +441,7 @@
       //Clean up: repeated in catch block below
       reducer.close();
       out.close(reporter);
+
       //End of clean up.
     } catch (IOException ioe) {
       try {
@@ -668,7 +671,9 @@
     private final List<MapOutput> mapOutputsFilesInMemory =
       Collections.synchronizedList(new LinkedList<MapOutput>());
     
-
+    /*DIFC: add labels*/
+    LabelSet secrecyLabel=LabelSet.EMPTY;
+    LabelSet integrityLabel=LabelSet.EMPTY;
     /**
      * This class contains the methods that should be used for metrics-reporting
      * the specific metrics for shuffle. This class actually reports the
@@ -1220,6 +1225,10 @@
                                      Path filename)
       throws IOException, InterruptedException {
         // Connect
+    	  /*DIFC: TODO: this is connecting directly to the URL, we need
+    	   * to modify it to send the labels first otherwise the Laminar
+    	   * OS may deny connection! 
+    	   */
         URLConnection connection = 
           mapOutputLoc.getOutputLocation().openConnection();
         InputStream input = getInputStream(connection, DEFAULT_READ_TIMEOUT, 
@@ -1551,8 +1560,13 @@
     }
     
     public ReduceCopier(TaskUmbilicalProtocol umbilical, JobConf conf)
+    throws IOException {
+    	this(umbilical, conf, LabelSet.EMPTY,LabelSet.EMPTY);
+    }
+    public ReduceCopier(TaskUmbilicalProtocol umbilical, JobConf conf, LabelSet secrecy, LabelSet integrity)
       throws IOException {
-      
+      this.secrecyLabel=secrecy;
+      this.integrityLabel=integrity;
       configureClasspath(conf);
       this.shuffleClientMetrics = new ShuffleClientMetrics(conf);
       this.umbilical = umbilical;      
Index: src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java	(working copy)
@@ -32,6 +32,7 @@
 import org.apache.hadoop.io.compress.CompressionCodec;
 import org.apache.hadoop.io.compress.DefaultCodec;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hadoop.util.Progressable;
 
@@ -126,8 +127,13 @@
   
   @Override 
   public RecordWriter <BytesWritable, BytesWritable> 
-             getRecordWriter(FileSystem ignored, JobConf job,
-                             String name, Progressable progress)
+  getRecordWriter(FileSystem ignored, JobConf job,
+                  String name, Progressable progress)
+throws IOException {
+	  return getRecordWriter(ignored, job,name,progress, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  public RecordWriter <BytesWritable, BytesWritable> 
+             getRecordWriter(FileSystem ignored, JobConf job, String name, Progressable progress, LabelSet secrecy, LabelSet integrity)
     throws IOException {
     // get the path of the temporary output file 
     Path file = FileOutputFormat.getTaskOutputPath(job, name);
@@ -150,7 +156,7 @@
                     getSequenceFileOutputValueClass(job),
                     compressionType,
                     codec,
-                    progress);
+                    progress, secrecy, integrity);
 
     return new RecordWriter<BytesWritable, BytesWritable>() {
         
Index: src/mapred/org/apache/hadoop/mapred/JobHistory.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/JobHistory.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/JobHistory.java	(working copy)
@@ -44,8 +44,9 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.PathFilter;
 import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.StringUtils;
-
+import org.apache.hadoop.security.LabelSet;
 /**
  * Provides methods for writing to and reading from job history. 
  * Job History works in an append mode, JobHistory and its inner classes provide methods 
@@ -826,7 +827,7 @@
             out = fs.create(logFile, FsPermission.getDefault(), true, 
                             defaultBufferSize, 
                             fs.getDefaultReplication(), 
-                            jobHistoryBlockSize, null);
+                            jobHistoryBlockSize, null, LabelSet.EMPTY, LabelSet.EMPTY);
             writer = new PrintWriter(out);
             writers.add(writer);
           }
Index: src/mapred/org/apache/hadoop/mapred/MapTask.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/MapTask.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/MapTask.java	(working copy)
@@ -56,6 +56,7 @@
 import org.apache.hadoop.mapred.IFile.Reader;
 import org.apache.hadoop.mapred.Merger.Segment;
 import org.apache.hadoop.mapred.SortedRanges.SkipRangeIterator;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.IndexedSortable;
 import org.apache.hadoop.util.IndexedSorter;
 import org.apache.hadoop.util.Progress;
@@ -271,7 +272,8 @@
   @SuppressWarnings("unchecked")
   public void run(final JobConf job, final TaskUmbilicalProtocol umbilical)
     throws IOException {
-
+	  
+	 /*DIFC: TODO reporters should also be labeled*/ 
     final Reporter reporter = getReporter(umbilical);
 
     // start thread that will handle communication with parent
@@ -290,11 +292,14 @@
 
     int numReduceTasks = conf.getNumReduceTasks();
     LOG.info("numReduceTasks: " + numReduceTasks);
+    /*DIFC: we assume that the ouput labels are the same as the input
+     * one. In these experiments input labels=union of labeles of all input files
+     */
     MapOutputCollector collector = null;
     if (numReduceTasks > 0) {
-      collector = new MapOutputBuffer(umbilical, job, reporter);
+      collector = new MapOutputBuffer(umbilical, job, reporter, job.getSecrecyLabel(),job.getIntegrityLabel());
     } else { 
-      collector = new DirectMapOutputCollector(umbilical, job, reporter);
+      collector = new DirectMapOutputCollector(umbilical, job, reporter, job.getSecrecyLabel(),job.getIntegrityLabel());
     }
     // reinstantiate the split
     try {
@@ -317,9 +322,10 @@
       job.setLong("map.input.start", fileSplit.getStart());
       job.setLong("map.input.length", fileSplit.getLength());
     }
-      
+    
+    /*DIFC: use the labels given in the input config*/
     RecordReader rawIn =                  // open input
-      job.getInputFormat().getRecordReader(instantiatedSplit, job, reporter);
+      job.getInputFormat().getRecordReader(instantiatedSplit, job, reporter, job.getSecrecyLabel(),job.getIntegrityLabel());
     RecordReader in = isSkipping() ? 
         new SkippingRecordReader(rawIn, getCounters(), umbilical) :
         new TrackedRecordReader(rawIn, getCounters());
@@ -327,7 +333,7 @@
 
     MapRunnable runner =
       ReflectionUtils.newInstance(job.getMapRunnerClass(), job);
-
+    LOG.info("### Mapper: map phase started");
     try {
       runner.run(in, collector, reporter);      
       collector.flush();
@@ -336,6 +342,7 @@
       in.close();                               // close input
       collector.close();
     }
+    LOG.info("### Mapper: map phase ended");
     done(umbilical);
   }
 
@@ -358,13 +365,17 @@
     private final Counters.Counter mapOutputRecordCounter;
 
     @SuppressWarnings("unchecked")
+     public DirectMapOutputCollector(TaskUmbilicalProtocol umbilical,
+        JobConf job, Reporter reporter) throws IOException {
+    	this(umbilical,job,reporter,LabelSet.EMPTY,LabelSet.EMPTY);
+    }
     public DirectMapOutputCollector(TaskUmbilicalProtocol umbilical,
-        JobConf job, Reporter reporter) throws IOException {
+        JobConf job, Reporter reporter,LabelSet secrecy, LabelSet integrity) throws IOException {
       this.reporter = reporter;
       String finalName = getOutputName(getPartition());
       FileSystem fs = FileSystem.get(job);
 
-      out = job.getOutputFormat().getRecordWriter(fs, job, finalName, reporter);
+      out = job.getOutputFormat().getRecordWriter(fs, job, finalName, reporter, secrecy, integrity);
 
       Counters counters = getCounters();
       mapOutputRecordCounter = counters.findCounter(MAP_OUTPUT_RECORDS);
@@ -452,9 +463,19 @@
     private int totalIndexCacheMemory;
     private static final int INDEX_CACHE_MEMORY_LIMIT = 1024*1024;
     
+    /*DIFC: include labels for this buffer*/
+    LabelSet secrecy=LabelSet.EMPTY;
+    LabelSet integrity=LabelSet.EMPTY;
+    
+    public MapOutputBuffer(TaskUmbilicalProtocol umbilical, JobConf job,
+            Reporter reporter) throws IOException {
+    	this(umbilical,job,reporter,LabelSet.EMPTY,LabelSet.EMPTY);
+    }
     @SuppressWarnings("unchecked")
     public MapOutputBuffer(TaskUmbilicalProtocol umbilical, JobConf job,
-                           Reporter reporter) throws IOException {
+                           Reporter reporter, LabelSet secSet, LabelSet intSet) throws IOException {
+      this.secrecy=secSet;
+      this.integrity=intSet;
       this.job = job;
       this.reporter = reporter;
       localFs = FileSystem.getLocal(job);
@@ -919,7 +940,7 @@
         // create spill file
         Path filename = mapOutputFile.getSpillFileForWrite(getTaskID(),
                                       numSpills, size);
-        out = rfs.create(filename);
+        out = rfs.create(filename, secrecy, integrity);
         // All records (reducers) of a given spill go to 
         // the same destination (memory or file).
         if (totalIndexCacheMemory >= INDEX_CACHE_MEMORY_LIMIT) {
@@ -928,7 +949,7 @@
               getTaskID(), numSpills,
               partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH);
 
-          indexOut = rfs.create(indexFilename);
+          indexOut = rfs.create(indexFilename,secrecy,integrity);
           indexChecksumOut = new IFileOutputStream(indexOut);
         }
         else {
@@ -1024,7 +1045,7 @@
         // create spill file
         Path filename = mapOutputFile.getSpillFileForWrite(getTaskID(),
                                       numSpills, size);
-        out = rfs.create(filename);
+        out = rfs.create(filename, secrecy, integrity);
         
         if (totalIndexCacheMemory >= INDEX_CACHE_MEMORY_LIMIT) {
           // create spill index
@@ -1032,7 +1053,7 @@
               getTaskID(), numSpills,
               partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH);
 
-          indexOut = rfs.create(indexFilename);
+          indexOut = rfs.create(indexFilename, secrecy, integrity);
           indexChecksumOut = new IFileOutputStream(indexOut);
         }
         else {
@@ -1171,7 +1192,7 @@
       long finalOutFileSize = 0;
       long finalIndexFileSize = 0;
       Path [] filename = new Path[numSpills];
-      
+      /*DIFC:TODO later add access control to file status also*/
       for(int i = 0; i < numSpills; i++) {
         filename[i] = mapOutputFile.getSpillFile(getTaskID(), i);
         finalOutFileSize += rfs.getFileStatus(filename[i]).getLen();
@@ -1179,10 +1200,10 @@
       
       if (numSpills == 1) { //the spill is the final output
         rfs.rename(filename[0],
-            new Path(filename[0].getParent(), "file.out"));
+            new Path(filename[0].getParent(), "file.out"), secrecy, integrity);
         if (indexCacheList.size() == 0) {
           rfs.rename(mapOutputFile.getSpillIndexFile(getTaskID(), 0),
-              new Path(filename[0].getParent(),"file.out.index"));
+              new Path(filename[0].getParent(),"file.out.index"), secrecy, integrity);
         } 
         else { 
           writeSingleSpillIndexToFile(getTaskID(),
@@ -1204,11 +1225,11 @@
       //The output stream for the final single output file
 
       FSDataOutputStream finalOut = rfs.create(finalOutputFile, true,
-                                               4096);
+                                               4096, secrecy, integrity);
 
       //The final index file output stream
       FSDataOutputStream finalIndexOut = rfs.create(finalIndexFile, true,
-                                                        4096);
+                                                        4096, secrecy, integrity);
 
       IFileOutputStream finalIndexChecksumOut = 
         new IFileOutputStream(finalIndexOut);
@@ -1241,7 +1262,7 @@
             long rawSegmentLength = indexRecord.rawLength;
             long segmentLength = indexRecord.partLength;
 
-            FSDataInputStream in = rfs.open(filename[i]);
+            FSDataInputStream in = rfs.open(filename[i], secrecy, integrity);
             in.seek(segmentOffset);
 
             Segment<K, V> s = 
@@ -1358,7 +1379,7 @@
             
       irArray = indexCacheList.get(0);
       
-      FSDataOutputStream indexOut = rfs.create(finalName);
+      FSDataOutputStream indexOut = rfs.create(finalName, secrecy, integrity);
       IFileOutputStream indexChecksumOut = new IFileOutputStream (indexOut);
       DataOutputStream wrapper = new DataOutputStream(indexChecksumOut);
       
Index: src/mapred/org/apache/hadoop/mapred/InputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/InputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/InputFormat.java	(working copy)
@@ -21,7 +21,7 @@
 import java.io.IOException;
 
 import org.apache.hadoop.fs.FileSystem;
-
+import org.apache.hadoop.security.LabelSet;
 /** 
  * <code>InputFormat</code> describes the input-specification for a 
  * Map-Reduce job. 
@@ -94,5 +94,8 @@
   RecordReader<K, V> getRecordReader(InputSplit split,
                                      JobConf job, 
                                      Reporter reporter) throws IOException;
+  RecordReader<K, V> getRecordReader(InputSplit split,
+          JobConf job, 
+          Reporter reporter, LabelSet secrecy, LabelSet integrity) throws IOException;
 }
 
Index: src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java	(working copy)
@@ -24,6 +24,7 @@
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.Progressable;
 
 /** A base class for {@link OutputFormat}. */
@@ -88,11 +89,17 @@
     return codecClass;
   }
   
-  public abstract RecordWriter<K, V> getRecordWriter(FileSystem ignored,
+  public RecordWriter<K, V> getRecordWriter(FileSystem ignored,
                                                JobConf job, String name,
                                                Progressable progress)
-    throws IOException;
+    throws IOException{
+	  return getRecordWriter(ignored, job,name,progress, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
 
+  public abstract RecordWriter<K, V> getRecordWriter(FileSystem ignored,
+          JobConf job, String name,
+          Progressable progress, LabelSet secrecy, LabelSet integrity)
+throws IOException;
   public void checkOutputSpecs(FileSystem ignored, JobConf job) 
     throws FileAlreadyExistsException, 
            InvalidJobConfException, IOException {
Index: src/mapred/org/apache/hadoop/mapred/LineRecordReader.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/LineRecordReader.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/LineRecordReader.java	(working copy)
@@ -30,6 +30,7 @@
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.compress.CompressionCodec;
 import org.apache.hadoop.io.compress.CompressionCodecFactory;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.commons.logging.LogFactory;
 import org.apache.commons.logging.Log;
 
@@ -65,7 +66,11 @@
   }
 
   public LineRecordReader(Configuration job, 
-                          FileSplit split) throws IOException {
+          FileSplit split) throws IOException {
+	  this(job,split,LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  public LineRecordReader(Configuration job, 
+                          FileSplit split, LabelSet secrecy, LabelSet integrity) throws IOException {
     this.maxLineLength = job.getInt("mapred.linerecordreader.maxlength",
                                     Integer.MAX_VALUE);
     start = split.getStart();
@@ -76,7 +81,7 @@
 
     // open the file and seek to the start of the split
     FileSystem fs = file.getFileSystem(job);
-    FSDataInputStream fileIn = fs.open(split.getPath());
+    FSDataInputStream fileIn = fs.open(split.getPath(),secrecy,integrity);
     boolean skipFirstLine = false;
     if (codec != null) {
       in = new LineReader(codec.createInputStream(fileIn), job);
Index: src/mapred/org/apache/hadoop/mapred/IFile.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/IFile.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/IFile.java	(working copy)
@@ -38,7 +38,7 @@
 import org.apache.hadoop.io.compress.Decompressor;
 import org.apache.hadoop.io.serializer.SerializationFactory;
 import org.apache.hadoop.io.serializer.Serializer;
-
+import org.apache.hadoop.security.LabelSet;
 /**
  * <code>IFile</code> is the simple <key-len, key, value-len, value> format
  * for the intermediate map-outputs in Map-Reduce.
@@ -76,9 +76,16 @@
     DataOutputBuffer buffer = new DataOutputBuffer();
 
     public Writer(Configuration conf, FileSystem fs, Path file, 
+    		Class<K> keyClass, Class<V> valueClass,
+    		CompressionCodec codec) throws IOException {
+    	this(conf, fs.create(file), keyClass, valueClass, codec);
+    	ownOutputStream = true;
+    }
+    
+    public Writer(Configuration conf, FileSystem fs, Path file, 
                   Class<K> keyClass, Class<V> valueClass,
-                  CompressionCodec codec) throws IOException {
-      this(conf, fs.create(file), keyClass, valueClass, codec);
+                  CompressionCodec codec,LabelSet secrecy,LabelSet integrity) throws IOException {
+      this(conf, fs.create(file,secrecy,integrity), keyClass, valueClass, codec);
       ownOutputStream = true;
     }
     
Index: src/mapred/org/apache/hadoop/mapred/SequenceFileOutputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/SequenceFileOutputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/SequenceFileOutputFormat.java	(working copy)
@@ -30,14 +30,21 @@
 import org.apache.hadoop.io.compress.CompressionCodec;
 import org.apache.hadoop.io.compress.DefaultCodec;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.*;
 
 /** An {@link OutputFormat} that writes {@link SequenceFile}s. */
 public class SequenceFileOutputFormat <K,V> extends FileOutputFormat<K, V> {
 
+	public RecordWriter<K, V> getRecordWriter(
+            FileSystem ignored, JobConf job,
+            String name, Progressable progress)
+throws IOException {
+		return getRecordWriter(ignored, job,name,progress, LabelSet.EMPTY,LabelSet.EMPTY);
+	}
   public RecordWriter<K, V> getRecordWriter(
                                           FileSystem ignored, JobConf job,
-                                          String name, Progressable progress)
+                                          String name, Progressable progress, LabelSet secrecy, LabelSet integrity)
     throws IOException {
     // get the path of the temporary output file 
     Path file = FileOutputFormat.getTaskOutputPath(job, name);
@@ -60,7 +67,7 @@
                                 job.getOutputValueClass(),
                                 compressionType,
                                 codec,
-                                progress);
+                                progress, secrecy, integrity);
 
     return new RecordWriter<K, V>() {
 
Index: src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryInputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryInputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryInputFormat.java	(working copy)
@@ -30,7 +30,7 @@
 import org.apache.hadoop.mapred.RecordReader;
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.mapred.SequenceFileInputFormat;
-
+import org.apache.hadoop.security.LabelSet;
 /**
  * InputFormat reading keys, values from SequenceFiles in binary (raw)
  * format.
@@ -43,9 +43,9 @@
   }
 
   public RecordReader<BytesWritable,BytesWritable> getRecordReader(
-      InputSplit split, JobConf job, Reporter reporter)
+      InputSplit split, JobConf job, Reporter reporter, LabelSet secrecy, LabelSet integrity)
       throws IOException {
-    return new SequenceFileAsBinaryRecordReader(job, (FileSplit)split);
+    return new SequenceFileAsBinaryRecordReader(job, (FileSplit)split, secrecy, integrity);
   }
 
   /**
@@ -61,6 +61,10 @@
     private SequenceFile.ValueBytes vbytes;
 
     public SequenceFileAsBinaryRecordReader(Configuration conf, FileSplit split)
+    throws IOException {
+    	this(conf, split, LabelSet.EMPTY, LabelSet.EMPTY);
+    }
+    public SequenceFileAsBinaryRecordReader(Configuration conf, FileSplit split, LabelSet secrecy, LabelSet integrity)
         throws IOException {
       Path path = split.getPath();
       FileSystem fs = path.getFileSystem(conf);
Index: src/mapred/org/apache/hadoop/mapred/MapFileOutputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/MapFileOutputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/MapFileOutputFormat.java	(working copy)
@@ -32,6 +32,7 @@
 import org.apache.hadoop.io.compress.CompressionCodec;
 import org.apache.hadoop.io.compress.DefaultCodec;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.Progressable;
 import org.apache.hadoop.util.ReflectionUtils;
 
@@ -39,8 +40,14 @@
 public class MapFileOutputFormat 
 extends FileOutputFormat<WritableComparable, Writable> {
 
+	public RecordWriter<WritableComparable, Writable> getRecordWriter(FileSystem ignored, JobConf job,
+            String name, Progressable progress)
+throws IOException {
+		return getRecordWriter(ignored,job,name,progress, LabelSet.EMPTY,LabelSet.EMPTY);
+	}
+	
   public RecordWriter<WritableComparable, Writable> getRecordWriter(FileSystem ignored, JobConf job,
-                                      String name, Progressable progress)
+                                      String name, Progressable progress, LabelSet secrecy, LabelSet integrity)
     throws IOException {
     // get the path of the temporary output file 
     Path file = FileOutputFormat.getTaskOutputPath(job, name);
@@ -64,7 +71,7 @@
                          job.getOutputKeyClass().asSubclass(WritableComparable.class),
                          job.getOutputValueClass().asSubclass(Writable.class),
                          compressionType, codec,
-                         progress);
+                         progress, secrecy,integrity);
 
     return new RecordWriter<WritableComparable, Writable>() {
 
Index: src/mapred/org/apache/hadoop/mapred/TextInputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/TextInputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/TextInputFormat.java	(working copy)
@@ -24,7 +24,7 @@
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.compress.*;
-
+import org.apache.hadoop.security.LabelSet;
 /** An {@link InputFormat} for plain text files.  Files are broken into lines.
  * Either linefeed or carriage-return are used to signal end of line.  Keys are
  * the position in the file, and values are the line of text.. */
@@ -43,10 +43,10 @@
 
   public RecordReader<LongWritable, Text> getRecordReader(
                                           InputSplit genericSplit, JobConf job,
-                                          Reporter reporter)
+                                          Reporter reporter, LabelSet secrecy, LabelSet integrity)
     throws IOException {
     
     reporter.setStatus(genericSplit.toString());
-    return new LineRecordReader(job, (FileSplit) genericSplit);
+    return new LineRecordReader(job, (FileSplit) genericSplit, secrecy, integrity);
   }
 }
Index: src/mapred/org/apache/hadoop/mapred/SequenceFileAsTextInputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/SequenceFileAsTextInputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/SequenceFileAsTextInputFormat.java	(working copy)
@@ -21,7 +21,7 @@
 import java.io.IOException;
 
 import org.apache.hadoop.io.Text;
-
+import org.apache.hadoop.security.LabelSet;
 /**
  * This class is similar to SequenceFileInputFormat, except it generates SequenceFileAsTextRecordReader 
  * which converts the input keys and values to their String forms by calling toString() method. 
@@ -35,11 +35,11 @@
 
   public RecordReader<Text, Text> getRecordReader(InputSplit split,
                                                   JobConf job,
-                                                  Reporter reporter)
+                                                  Reporter reporter, LabelSet secrecy, LabelSet integrity)
     throws IOException {
 
     reporter.setStatus(split.toString());
 
-    return new SequenceFileAsTextRecordReader(job, (FileSplit) split);
+    return new SequenceFileAsTextRecordReader(job, (FileSplit) split, secrecy ,integrity);
   }
 }
Index: src/mapred/org/apache/hadoop/mapred/SequenceFileInputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/SequenceFileInputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/SequenceFileInputFormat.java	(working copy)
@@ -26,7 +26,7 @@
 
 import org.apache.hadoop.io.SequenceFile;
 import org.apache.hadoop.io.MapFile;
-
+import org.apache.hadoop.security.LabelSet;
 /** An {@link InputFormat} for {@link SequenceFile}s. */
 public class SequenceFileInputFormat<K, V> extends FileInputFormat<K, V> {
 
@@ -50,12 +50,12 @@
   }
 
   public RecordReader<K, V> getRecordReader(InputSplit split,
-                                      JobConf job, Reporter reporter)
+                                      JobConf job, Reporter reporter,LabelSet secrecy, LabelSet integrity)
     throws IOException {
 
     reporter.setStatus(split.toString());
 
-    return new SequenceFileRecordReader<K, V>(job, (FileSplit) split);
+    return new SequenceFileRecordReader<K, V>(job, (FileSplit) split, secrecy, integrity);
   }
 
 }
Index: src/mapred/org/apache/hadoop/mapred/FileInputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/FileInputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/FileInputFormat.java	(working copy)
@@ -31,6 +31,7 @@
 import org.apache.hadoop.fs.FileUtil;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.PathFilter;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hadoop.util.StringUtils;
 
@@ -100,9 +101,15 @@
     return true;
   }
   
+  public RecordReader<K, V> getRecordReader(InputSplit split,
+          JobConf job,
+          Reporter reporter) throws IOException{
+	  return getRecordReader(split,job,reporter,LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+          
   public abstract RecordReader<K, V> getRecordReader(InputSplit split,
                                                JobConf job,
-                                               Reporter reporter)
+                                               Reporter reporter, LabelSet secrecy, LabelSet integrity)
     throws IOException;
 
   /**
@@ -188,7 +195,7 @@
   public InputSplit[] getSplits(JobConf job, int numSplits)
     throws IOException {
     FileStatus[] files = listStatus(job);
-    
+
     long totalSize = 0;                           // compute total size
     for (FileStatus file: files) {                // check we have valid files
       if (file.isDir()) {
@@ -200,18 +207,17 @@
     long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits);
     long minSize = Math.max(job.getLong("mapred.min.split.size", 1),
                             minSplitSize);
-
     // generate splits
     ArrayList<FileSplit> splits = new ArrayList<FileSplit>(numSplits);
     for (FileStatus file: files) {
       Path path = file.getPath();
       FileSystem fs = path.getFileSystem(job);
       long length = file.getLen();
-      BlockLocation[] blkLocations = fs.getFileBlockLocations(file, 0, length);
+      /*DIFC: changes to pass labels*/
+      BlockLocation[] blkLocations = fs.getFileBlockLocations(file, 0, length, job.getSecrecyLabel(),job.getIntegrityLabel());
       if ((length != 0) && isSplitable(fs, path)) { 
         long blockSize = file.getBlockSize();
         long splitSize = computeSplitSize(goalSize, minSize, blockSize);
-
         long bytesRemaining = length;
         while (((double) bytesRemaining)/splitSize > SPLIT_SLOP) {
           int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);
@@ -219,7 +225,6 @@
                                    blkLocations[blkIndex].getHosts()));
           bytesRemaining -= splitSize;
         }
-        
         if (bytesRemaining != 0) {
           splits.add(new FileSplit(path, length-bytesRemaining, bytesRemaining, 
                      blkLocations[blkLocations.length-1].getHosts()));
Index: src/mapred/org/apache/hadoop/mapred/OutputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/OutputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/OutputFormat.java	(working copy)
@@ -22,7 +22,7 @@
 
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.util.Progressable;
-
+import org.apache.hadoop.security.LabelSet;
 /** 
  * <code>OutputFormat</code> describes the output-specification for a 
  * Map-Reduce job.
@@ -59,6 +59,9 @@
                                      String name, Progressable progress)
   throws IOException;
 
+  RecordWriter<K, V> getRecordWriter(FileSystem ignored, JobConf job,
+          String name, Progressable progress, LabelSet secrecy, LabelSet integrity)
+          throws IOException;
   /** 
    * Check for validity of the output-specification for the job.
    *  
Index: src/mapred/org/apache/hadoop/mapred/TextOutputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/TextOutputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/TextOutputFormat.java	(working copy)
@@ -30,6 +30,7 @@
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.compress.CompressionCodec;
 import org.apache.hadoop.io.compress.GzipCodec;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.*;
 
 /** An {@link OutputFormat} that writes plain text files. */
@@ -103,10 +104,17 @@
     }
   }
 
+  public RecordWriter<K, V> getRecordWriter(
+          FileSystem ignored, JobConf job,
+          String name, Progressable progress)
+throws IOException {
+		return getRecordWriter(ignored, job,name,progress, LabelSet.EMPTY,LabelSet.EMPTY);
+	}
+  
   public RecordWriter<K, V> getRecordWriter(FileSystem ignored,
                                                   JobConf job,
                                                   String name,
-                                                  Progressable progress)
+                                                  Progressable progress, LabelSet secrecy, LabelSet integrity)
     throws IOException {
     boolean isCompressed = getCompressOutput(job);
     String keyValueSeparator = job.get("mapred.textoutputformat.separator", 
@@ -114,7 +122,7 @@
     if (!isCompressed) {
       Path file = FileOutputFormat.getTaskOutputPath(job, name);
       FileSystem fs = file.getFileSystem(job);
-      FSDataOutputStream fileOut = fs.create(file, progress);
+      FSDataOutputStream fileOut = fs.create(file, progress, secrecy, integrity);
       return new LineRecordWriter<K, V>(fileOut, keyValueSeparator);
     } else {
       Class<? extends CompressionCodec> codecClass =
@@ -126,7 +134,7 @@
         FileOutputFormat.getTaskOutputPath(job, 
                                            name + codec.getDefaultExtension());
       FileSystem fs = file.getFileSystem(job);
-      FSDataOutputStream fileOut = fs.create(file, progress);
+      FSDataOutputStream fileOut = fs.create(file, progress, secrecy, integrity);
       return new LineRecordWriter<K, V>(new DataOutputStream
                                         (codec.createOutputStream(fileOut)),
                                         keyValueSeparator);
Index: src/mapred/org/apache/hadoop/mapred/KeyValueTextInputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/KeyValueTextInputFormat.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/KeyValueTextInputFormat.java	(working copy)
@@ -24,7 +24,7 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.compress.CompressionCodecFactory;
-
+import org.apache.hadoop.security.LabelSet;
 /**
  * An {@link InputFormat} for plain text files. Files are broken into lines.
  * Either linefeed or carriage-return are used to signal end of line. Each line
@@ -46,11 +46,11 @@
   
   public RecordReader<Text, Text> getRecordReader(InputSplit genericSplit,
                                                   JobConf job,
-                                                  Reporter reporter)
+                                                  Reporter reporter, LabelSet secrecy, LabelSet integrity)
     throws IOException {
     
     reporter.setStatus(genericSplit.toString());
-    return new KeyValueLineRecordReader(job, (FileSplit) genericSplit);
+    return new KeyValueLineRecordReader(job, (FileSplit) genericSplit, secrecy, integrity);
   }
 
 }
Index: src/mapred/org/apache/hadoop/mapred/SequenceFileRecordReader.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/SequenceFileRecordReader.java	(revision 142)
+++ src/mapred/org/apache/hadoop/mapred/SequenceFileRecordReader.java	(working copy)
@@ -25,6 +25,7 @@
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.*;
+import org.apache.hadoop.security.LabelSet;
 import org.apache.hadoop.util.ReflectionUtils;
 
 /** An {@link RecordReader} for {@link SequenceFile}s. */
@@ -37,10 +38,15 @@
   protected Configuration conf;
 
   public SequenceFileRecordReader(Configuration conf, FileSplit split)
+  throws IOException {
+	  this(conf, split, LabelSet.EMPTY,LabelSet.EMPTY);
+  }
+  
+  public SequenceFileRecordReader(Configuration conf, FileSplit split, LabelSet secrecy, LabelSet integrity)
     throws IOException {
     Path path = split.getPath();
     FileSystem fs = path.getFileSystem(conf);
-    this.in = new SequenceFile.Reader(fs, path, conf);
+    this.in = new SequenceFile.Reader(fs, path, conf, secrecy , integrity);
     this.end = split.getStart() + split.getLength();
     this.conf = conf;
 
Index: lib/native/Linux-amd64-64/libLaminarCalls.so
===================================================================
Cannot display: file marked as a binary type.
svn:mime-type = application/octet-stream

Property changes on: lib/native/Linux-amd64-64/libLaminarCalls.so
___________________________________________________________________
Name: svn:executable
   + *
Name: svn:mime-type
   + application/octet-stream

Index: lib/native/Linux-i386-32/libLaminarCalls.so
===================================================================
Cannot display: file marked as a binary type.
svn:mime-type = application/octet-stream

Property changes on: lib/native/Linux-i386-32/libLaminarCalls.so
___________________________________________________________________
Name: svn:executable
   + *
Name: svn:mime-type
   + application/octet-stream

Index: build.xml
===================================================================
--- build.xml	(revision 142)
+++ build.xml	(working copy)
@@ -57,6 +57,7 @@
   <property name="build.classes" value="${build.dir}/classes"/>
   <property name="build.src" value="${build.dir}/src"/>
   <property name="build.tools" value="${build.dir}/tools"/>
+  <property name="build.contrib.streaming" value="${build.dir}/contrib/streaming/classes"/>
   <property name="build.webapps" value="${build.dir}/webapps"/>
   <property name="build.examples" value="${build.dir}/examples"/>
   <property name="build.anttasks" value="${build.dir}/ant"/>
@@ -423,7 +424,7 @@
      </subant>  	
   </target>
   
-  <target name="compile" depends="compile-core, compile-contrib, compile-ant-tasks, compile-tools" description="Compile core, contrib">
+  <target name="compile" depends="compile-core, compile-contrib, compile-ant-tasks, compile-tools, streaming-jar" description="Compile core, contrib">
   </target>
 
   <target name="compile-examples" 
@@ -498,6 +499,16 @@
     </jar>
   </target>
 
+ <target name="streaming-jar" depends="jar, compile-contrib"  
+          description="Make the Hadoop streaming jar."> 
+    <jar jarfile="${build.dir}/airavat-streaming.jar"
+         basedir="${build.contrib.streaming}"> 
+      <manifest>           
+        <attribute name="Main-Class"  
+                   value="org/apache/hadoop/streaming/HadoopStreaming"/>  
+      </manifest>                                                                           
+    </jar>  
+  </target>        
   <!-- ================================================================== -->
   <!-- Make the Hadoop metrics jar. (for use outside Hadoop)              -->
   <!-- ================================================================== -->
